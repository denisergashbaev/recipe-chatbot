{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import litellm\n",
    "from typing import Final\n",
    "from dotenv import load_dotenv\n",
    "from textwrap import dedent, wrap\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "\n",
    "class Dimention(BaseModel):\n",
    "    meal_type: str = Field(..., description=\"The type of meal the user is interested in, such as 'Lite breakfast', 'Brunch'\")\n",
    "    dietary_restriction: str = Field(..., description=\"The dietary restrictions the user has, such as 'Not spicy', 'without fish or meat'\")\n",
    "    preparation_time: str = Field(..., description=\"The preparation time the user is interested in, such as '10 minutes', 'something quick', 'under 1 hour'\")\n",
    "\n",
    "    @classmethod\n",
    "    def get_prompt_description(cls) -> str:\n",
    "        description = []\n",
    "        for field_name, field_info in cls.model_fields.items():\n",
    "            description.append(f\"- {field_name}: {field_info.description}\")\n",
    "        return \"\\n\".join(description)\n",
    "\n",
    "\n",
    "num_dimention_instances: Final[int] = 50\n",
    "\n",
    "\n",
    "class DimentionList(BaseModel):\n",
    "    dimention_list: list[Dimention] = Field(\n",
    "        ..., \n",
    "        description=f\"List of dimention instances, there should be {num_dimention_instances} instances in the list\", \n",
    "        default_factory=list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gemini/gemini-2.5-flash'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME: Final[str] = os.environ[\"MODEL_NAME\"]\n",
    "MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('gemini-2.5-flash', 'gemini', None, None)\n",
      "['temperature', 'top_p', 'max_tokens', 'max_completion_tokens', 'stream', 'tools', 'tool_choice', 'functions', 'response_format', 'n', 'stop', 'logprobs', 'frequency_penalty', 'modalities', 'parallel_tool_calls', 'web_search_options', 'reasoning_effort', 'thinking']\n"
     ]
    }
   ],
   "source": [
    "from litellm import get_supported_openai_params\n",
    "\n",
    "params = get_supported_openai_params(model=MODEL_NAME)\n",
    "\n",
    "custom_llm_provider = litellm.get_llm_provider(model=MODEL_NAME)\n",
    "\n",
    "print(custom_llm_provider)\n",
    "\n",
    "assert \"response_format\" in params\n",
    "\n",
    "print(params)\n",
    "\n",
    "from litellm import supports_response_schema\n",
    "\n",
    "assert supports_response_schema(model=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dimention_list(dimention_list: str):\n",
    "    SYSTEM_PROMPT = dedent(f\"\"\"\n",
    "        You are AI assistant that helps generate instances for dimentions of the queries that will be sent to the recipe chatbot. A dimention describes aspects of the query that that the users send to the chatbot. \n",
    "        Given following dimention types, generate tuple combinations of the dimentions\n",
    "\n",
    "        Dimentions:\n",
    "            meal_type: The type of meal the user is interested in, such as 'Lite breakfast', 'Brunch'\n",
    "            dietary_restriction: The dietary restrictions the user has, such as 'Not spicy', 'without fish or meat', 'meaty'\n",
    "            preparation_time: The preparation time the user is interested in, such as '10 minutes', 'something quick', 'under 1 hour', 'I haveall the time until retirement'\n",
    "\n",
    "        Instructions:\n",
    "            - Generate  {num_dimention_instances} tuple combinations of the dimentions.\n",
    "            - Avoid duplicates. \n",
    "            - Make sure that the tuples are diverse. \n",
    "\n",
    "        Example output:\n",
    "        - ('Lite breakfast', 'Not spicy', '10 minutes')\n",
    "        - ('Snack', 'with hummus and carrots', 'instant')\n",
    "    \"\"\")\n",
    "    completion = litellm.completion(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": SYSTEM_PROMPT}\n",
    "        ],\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 50 diverse tuple combinations of the dimensions:\n",
      "\n",
      "1.  ('Breakfast', 'Not spicy', '10 minutes')\n",
      "2.  ('Lunch', 'Vegetarian', '30 minutes')\n",
      "3.  ('Dinner', 'Meaty', 'Under 1 hour')\n",
      "4.  ('Snack', 'Gluten-free', 'Quick')\n",
      "5.  ('Dessert', 'Dairy-free', '45 minutes')\n",
      "6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\n",
      "7.  ('Appetizer', 'Low-carb', '15 minutes')\n",
      "8.  ('Side Dish', 'High-protein', '20 minutes')\n",
      "9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\n",
      "10. ('Holiday meal', 'Pescatarian', 'All day')\n",
      "11. ('Party food', 'Sugar-free', 'About an hour')\n",
      "12. ('Quick bite', 'Nut-free', 'Instant')\n",
      "13. ('Breakfast', 'Keto', '30 minutes')\n",
      "14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\n",
      "15. ('Dinner', 'Spicy', '45 minutes')\n",
      "16. ('Snack', 'without fish or meat', '10 minutes')\n",
      "17. ('Dessert', 'Meaty', 'Long prep')\n",
      "18. ('Brunch', 'Paleo', 'Quick')\n",
      "19. ('Appetizer', 'Low-salt', '15 minutes')\n",
      "20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\n",
      "21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\n",
      "22. ('Holiday meal', 'Lactose-free', 'Overnight')\n",
      "23. ('Party food', 'Comfort food', 'About an hour')\n",
      "24. ('Quick bite', 'with hummus and carrots', 'Instant')\n",
      "25. ('Breakfast', 'Gluten-free', 'Quick and easy')\n",
      "26. ('Lunch', 'Vegan', '30 minutes')\n",
      "27. ('Dinner', 'High-protein', 'Under 1 hour')\n",
      "28. ('Snack', 'Not spicy', '10 minutes')\n",
      "29. ('Dessert', 'Vegetarian', '45 minutes')\n",
      "30. ('Brunch', 'Meaty', 'I have all the time until retirement')\n",
      "31. ('Appetizer', 'Dairy-free', '15 minutes')\n",
      "32. ('Side Dish', 'Low-carb', '20 minutes')\n",
      "33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\n",
      "34. ('Holiday meal', 'Kid-friendly', 'All day')\n",
      "35. ('Party food', 'Nut-free', 'About an hour')\n",
      "36. ('Quick bite', 'Sugar-free', 'Instant')\n",
      "37. ('Breakfast', 'Spicy', 'Quick and easy')\n",
      "38. ('Lunch', 'Keto', '30 minutes')\n",
      "39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\n",
      "40. ('Snack', 'Low-salt', '10 minutes')\n",
      "41. ('Dessert', 'Diabetic-friendly', '45 minutes')\n",
      "42. ('Brunch', 'High-fiber', 'Long prep')\n",
      "43. ('Appetizer', 'Lactose-free', '15 minutes')\n",
      "44. ('Side Dish', 'Comfort food', '20 minutes')\n",
      "45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\n",
      "46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\n",
      "47. ('Party food', 'Not spicy', 'About an hour')\n",
      "48. ('Quick bite', 'Vegetarian', 'Instant')\n",
      "49. ('Dinner', 'Gluten-free', '10 minutes')\n",
      "50. ('Lunch', 'Meaty', 'something quick')\n"
     ]
    }
   ],
   "source": [
    "dimention_list = generate_dimention_list(\"\")\n",
    "\n",
    "print(dimention_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimention_tuples = [\n",
    "    ('Breakfast', 'Not spicy', '10 minutes'),\n",
    "    ('Lunch', 'Vegetarian', '30 minutes'),\n",
    "    ('Dinner', 'Meaty', 'Under 1 hour'),\n",
    "    ('Snack', 'Gluten-free', 'Quick'),\n",
    "    ('Dessert', 'Dairy-free', '45 minutes'),\n",
    "    ('Brunch', 'Vegan', 'I have all the time until retirement'),\n",
    "    ('Appetizer', 'Low-carb', '15 minutes'),\n",
    "    ('Side Dish', 'High-protein', '20 minutes'),\n",
    "    ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes'),\n",
    "    ('Holiday meal', 'Pescatarian', 'All day'),\n",
    "    ('Party food', 'Sugar-free', 'About an hour'),\n",
    "    ('Quick bite', 'Nut-free', 'Instant'),\n",
    "    ('Breakfast', 'Keto', '30 minutes'),\n",
    "    ('Lunch', 'Heart-healthy', 'Under 1 hour'),\n",
    "    ('Dinner', 'Spicy', '45 minutes'),\n",
    "    ('Snack', 'without fish or meat', '10 minutes'),\n",
    "    ('Dessert', 'Meaty', 'Long prep'),\n",
    "    ('Brunch', 'Paleo', 'Quick'),\n",
    "    ('Appetizer', 'Low-salt', '15 minutes'),\n",
    "    ('Side Dish', 'Diabetic-friendly', '20 minutes'),\n",
    "    ('Lite breakfast', 'High-fiber', 'Less than 20 minutes'),\n",
    "    ('Holiday meal', 'Lactose-free', 'Overnight'),\n",
    "    ('Party food', 'Comfort food', 'About an hour'),\n",
    "    ('Quick bite', 'with hummus and carrots', 'Instant'),\n",
    "    ('Breakfast', 'Gluten-free', 'Quick and easy'),\n",
    "    ('Lunch', 'Vegan', '30 minutes'),\n",
    "    ('Dinner', 'High-protein', 'Under 1 hour'),\n",
    "    ('Snack', 'Not spicy', '10 minutes'),\n",
    "    ('Dessert', 'Vegetarian', '45 minutes'),\n",
    "    ('Brunch', 'Meaty', 'I have all the time until retirement'),\n",
    "    ('Appetizer', 'Dairy-free', '15 minutes'),\n",
    "    ('Side Dish', 'Low-carb', '20 minutes'),\n",
    "    ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes'),\n",
    "    ('Holiday meal', 'Kid-friendly', 'All day'),\n",
    "    ('Party food', 'Nut-free', 'About an hour'),\n",
    "    ('Quick bite', 'Sugar-free', 'Instant'),\n",
    "    ('Breakfast', 'Spicy', 'Quick and easy'),\n",
    "    ('Lunch', 'Keto', '30 minutes'),\n",
    "    ('Dinner', 'Heart-healthy', 'Under 1 hour'),\n",
    "    ('Snack', 'Low-salt', '10 minutes'),\n",
    "    ('Dessert', 'Diabetic-friendly', '45 minutes'),\n",
    "    ('Brunch', 'High-fiber', 'Long prep'),\n",
    "    ('Appetizer', 'Lactose-free', '15 minutes'),\n",
    "    ('Side Dish', 'Comfort food', '20 minutes'),\n",
    "    ('Lite breakfast', 'Paleo', 'Less than 20 minutes'),\n",
    "    ('Holiday meal', 'with hummus and carrots', 'Overnight'),\n",
    "    ('Party food', 'Not spicy', 'About an hour'),\n",
    "    ('Quick bite', 'Vegetarian', 'Instant'),\n",
    "    ('Dinner', 'Gluten-free', '10 minutes'),\n",
    "    ('Lunch', 'Meaty', 'something quick'),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tenacity import retry, wait_fixed, wait_random, before_log, before_sleep_log\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stderr, level=logging.DEBUG)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@retry(wait=wait_fixed(3) + wait_random(0, 2), before=before_sleep_log(logger, logging.DEBUG))\n",
    "def call_llm(dimention: str):\n",
    "    prompt = f\"\"\"\n",
    "        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\n",
    "        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \n",
    "\n",
    "        <dimension>\n",
    "        {dimention_list}\n",
    "        </dimension>\n",
    "\n",
    "        <example>\n",
    "        <dimension>\n",
    "        ('Breakfast', 'Not spicy', '10 minutes')\n",
    "        </dimension>\n",
    "\n",
    "        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\n",
    "        - Give me a quick recipe for plain breakfast. \n",
    "        - Breakfast, fast and non-spicy.\n",
    "        - A ten-minute breakfast meal, non-spicy.\n",
    "        - I need breakfast under 10 minutes, it should be mild to taste. \n",
    "        </example>\n",
    "\n",
    "        Just output user queries, no other text.\n",
    "    \"\"\"\n",
    "    completion = litellm.completion(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:32:38 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:32:38 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:32:38 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:32:38 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:32:38 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:32:38 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:32:38 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:32:38 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:32:38 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:32:38 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:32:38 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:32:38 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:32:38 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:32:38 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14149ca40>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x14549fa50> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x141284590>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:32:45 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=6691'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:32:45 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that takes around 30 minutes to prepare.\\n- What are some quick vegetarian lunch options I can make in half an hour?\\n- Give me ideas for a 30-minute vegetarian lunch.\\n- I need a fast and vegetarian lunch, preferably ready in under 30 minutes.\\n- Suggest a lunch that's vegetarian and can be made in 30 minutes or less.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 92,\n",
      "    \"totalTokenCount\": 1670,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 486\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"LRlgaPyiCvyFvdIP3KrjuAg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that takes around 30 minutes to prepare.\\n- What are some quick vegetarian lunch options I can make in half an hour?\\n- Give me ideas for a 30-minute vegetarian lunch.\\n- I need a fast and vegetarian lunch, preferably ready in under 30 minutes.\\n- Suggest a lunch that's vegetarian and can be made in 30 minutes or less.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 92,\n",
      "    \"totalTokenCount\": 1670,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 486\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"LRlgaPyiCvyFvdIP3KrjuAg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:32:45 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:32:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:32:45 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:32:45 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:32:45 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:32:45 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0017726\n",
      "DEBUG:LiteLLM:response_cost: 0.0017726\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:32:45 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:32:45 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:32:45 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:32:45 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:32:45 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:32:45 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:32:45 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:32:45 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:32:45 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:32:45 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:32:45 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:32:45 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:32:45 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:32:45 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1456d0c20>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x145628fd0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x141498ce0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:32:46 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=1242'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:32:46 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in 30 minutes.\\n- What are some quick vegetarian lunch ideas ready in half an hour?\\n- I need a vegetarian lunch that takes about 30 minutes to prepare.\\n- Suggest a 30-minute vegetarian lunch.\\n- Can you find me a recipe for a vegetarian lunch, ready in 30 minutes?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 84,\n",
      "    \"totalTokenCount\": 1250,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 74\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"LhlgaMrqJOWEvdIPz--V2AE\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in 30 minutes.\\n- What are some quick vegetarian lunch ideas ready in half an hour?\\n- I need a vegetarian lunch that takes about 30 minutes to prepare.\\n- Suggest a 30-minute vegetarian lunch.\\n- Can you find me a recipe for a vegetarian lunch, ready in 30 minutes?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 84,\n",
      "    \"totalTokenCount\": 1250,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 74\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"LhlgaMrqJOWEvdIPz--V2AE\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:32:46 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:32:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:32:46 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:32:46 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:32:46 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:32:46 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0007226000000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.0007226000000000001\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:32:46 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:32:46 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:32:46 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:32:46 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:32:46 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:32:46 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:32:46 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:32:46 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:32:46 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:32:46 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:32:46 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:32:46 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:32:46 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:32:46 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14149ac30>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x14562a6d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145702b10>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:32:48 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=2089'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:32:48 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a vegetarian lunch recipe that takes about 30 minutes.\\n- What can I cook for lunch that's vegetarian and ready in half an hour?\\n- Show me quick vegetarian lunch ideas, 30-minute max.\\n- Vegetarian lunch, 30 minutes prep.\\n- Recipe for a 30-minute vegetarian lunch.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 74,\n",
      "    \"totalTokenCount\": 1378,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 212\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"MBlgaNuVMdaynsEPrZ_cwAQ\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a vegetarian lunch recipe that takes about 30 minutes.\\n- What can I cook for lunch that's vegetarian and ready in half an hour?\\n- Show me quick vegetarian lunch ideas, 30-minute max.\\n- Vegetarian lunch, 30 minutes prep.\\n- Recipe for a 30-minute vegetarian lunch.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 74,\n",
      "    \"totalTokenCount\": 1378,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 212\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"MBlgaNuVMdaynsEPrZ_cwAQ\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:32:48 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:32:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:32:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:32:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:32:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:32:48 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0010426\n",
      "DEBUG:LiteLLM:response_cost: 0.0010426\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:32:48 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:32:48 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:32:48 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:32:48 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:32:48 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:32:48 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:32:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:32:48 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:32:48 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:32:48 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:32:48 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:32:48 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:32:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:32:48 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1444d4c50>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x14562a8d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14149ca40>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:32:50 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=2067'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:32:50 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in about 30 minutes.\\n- Can you suggest some quick vegetarian lunch options? I have half an hour.\\n- Vegetarian lunch, 30 minutes, please.\\n- What can I cook for a vegetarian lunch in under 30 minutes?\\n- Need a fast, meat-free lunch recipe, ready in 30 mins.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 86,\n",
      "    \"totalTokenCount\": 1403,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 225\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"MhlgaJD9O5afvdIPwqyKsQg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in about 30 minutes.\\n- Can you suggest some quick vegetarian lunch options? I have half an hour.\\n- Vegetarian lunch, 30 minutes, please.\\n- What can I cook for a vegetarian lunch in under 30 minutes?\\n- Need a fast, meat-free lunch recipe, ready in 30 mins.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 86,\n",
      "    \"totalTokenCount\": 1403,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 225\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"MhlgaJD9O5afvdIPwqyKsQg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:32:50 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:32:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:32:50 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:32:50 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:32:50 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:32:50 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0011051000000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.0011051000000000001\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:32:50 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:32:50 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:32:50 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:32:50 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:32:50 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:32:50 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:32:50 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:32:50 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:32:50 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:32:50 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:32:50 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:32:50 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:32:50 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:32:50 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x141498a70>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x145683a50> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x141498380>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:33:07 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=16245'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:33:07 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a vegetarian lunch recipe that I can make in 30 minutes.\\n- What's a good vegetarian lunch option for someone with only half an hour?\\n- Quick vegetarian lunch, ready in 30 minutes, please.\\n- Can you suggest a vegetarian lunch recipe that takes no more than 30 minutes?\\n- Give me ideas for a 30-minute vegetarian lunch.\\n\\n- I'm looking for a meaty dinner recipe that takes under an hour.\\n- What are some dinner ideas with meat that take less than an hour to prepare?\\n- Meaty dinner, quick, under 60 minutes.\\n- Suggest a dinner with meat that's ready in under an hour.\\n- I need a meaty dinner recipe that won't take more than an hour.\\n\\n- Quick gluten-free snack ideas?\\n- What's a good gluten-free snack I can make quickly?\\n- I need a fast gluten-free snack.\\n- Give me a quick gluten-free snack recipe.\\n- Gluten-free snack, super quick.\\n\\n- Dairy-free dessert, about 45 minutes to make.\\n- What dairy-free desserts can I make in 45 minutes?\\n- I'm looking for a dairy-free dessert that takes around 45 minutes.\\n- Dessert, dairy-free, 45-minute prep time.\\n- Can you suggest a dairy-free dessert recipe that takes about 45 minutes?\\n\\n- I'm making brunch and want a vegan recipe, I have all day to cook.\\n- What are some elaborate vegan brunch recipes? I have unlimited time.\\n- Vegan brunch ideas, no time limit.\\n- Suggest a vegan brunch that takes a long time to prepare.\\n- I need a vegan brunch recipe, I have all the time in the world.\\n\\n- Quick low-carb appetizer for 15 minutes.\\n- What's an easy low-carb appetizer I can make in 15 minutes?\\n- I need a low-carb appetizer recipe that's ready in 15 minutes.\\n- Give me a 15-minute low-carb appetizer.\\n- Appetizer, low-carb, quick, 15 minutes.\\n\\n- High-protein side dish that takes about 20 minutes to make.\\n- What are some high-protein side dishes I can prepare in 20 minutes?\\n- I need a 20-minute high-protein side dish.\\n- Suggest a high-protein side that's ready in under 20 minutes.\\n- Side dish, high-protein, 20 minutes.\\n\\n- Kid-friendly lite breakfast, less than 20 minutes.\\n- What's a quick, light, kid-friendly breakfast recipe, under 20 minutes?\\n- I need a lite breakfast for kids that takes less than 20 minutes.\\n- Suggest a less than 20-minute kid-friendly light breakfast.\\n- Lite breakfast, kid-friendly, fast, under 20 minutes.\\n\\n- Pescatarian holiday meal, I have all day.\\n- What are some full-day pescatarian holiday meal recipes?\\n- I'm planning a holiday meal, pescatarian, with no time constraints.\\n- Give me ideas for an all-day pescatarian holiday meal.\\n- Holiday meal, pescatarian, takes all day to make.\\n\\n- Sugar-free party food that takes about an hour.\\n- What are some sugar-free party food ideas I can make in roughly an hour?\\n- I need party food that's sugar-free and takes about an hour to prepare.\\n- Suggest an hour-long sugar-free party food recipe.\\n- Party food, sugar-free, roughly 60 minutes.\\n\\n- Instant nut-free quick bite.\\n- What's a really quick nut-free bite?\\n- I need an instant nut-free snack.\\n- Give me an immediate nut-free quick bite recipe.\\n- Nut-free quick bite, takes no time.\\n\\n- Keto breakfast in 30 minutes.\\n- What's a quick keto breakfast recipe that takes 30 minutes?\\n- I need a keto-friendly breakfast I can make in half an hour.\\n- Suggest a 30-minute keto breakfast.\\n- Breakfast, keto, 30 minutes.\\n\\n- Heart-healthy lunch under 1 hour.\\n- What heart-healthy lunch can I make in less than an hour?\\n- I need a quick heart-healthy lunch that's ready in under 60 minutes.\\n- Suggest a heart-healthy lunch that doesn't take more than an hour.\\n- Lunch, heart-healthy, under an hour.\\n\\n- Spicy dinner ready in 45 minutes.\\n- What's a good spicy dinner recipe I can make in 45 minutes?\\n- I need a spicy dinner that takes about 45 minutes.\\n- Suggest a 45-minute spicy dinner.\\n- Dinner, spicy, 45 minutes.\\n\\n- Snack without fish or meat, 10 minutes.\\n- What's a quick 10-minute snack that's vegetarian (no fish or meat)?\\n- I need a snack ready in 10 minutes, no fish or meat.\\n- Give me ideas for a 10-minute snack, no animal products.\\n- Snack, plant-based, 10 minutes.\\n\\n- Meaty dessert with long prep.\\n- What are some meaty dessert recipes that take a long time to prepare?\\n- I'm looking for a meaty dessert that requires long prep.\\n- Suggest a meaty dessert that's time-consuming.\\n- Dessert, meaty, long preparation time.\\n\\n- Quick paleo brunch.\\n- What's a fast paleo brunch idea?\\n- I need a paleo brunch that's quick to make.\\n- Suggest a quick paleo-friendly brunch.\\n- Paleo brunch, quick.\\n\\n- Low-salt appetizer in 15 minutes.\\n- What's an easy low-salt appetizer I can make in 15 minutes?\\n- I need a low-salt appetizer recipe that's ready in 15 minutes.\\n- Give me a 15-minute low-salt appetizer.\\n- Appetizer, low-salt, quick, 15 minutes.\\n\\n- Diabetic-friendly side dish in 20 minutes.\\n- What are some diabetic-friendly side dishes I can prepare in 20 minutes?\\n- I need a 20-minute diabetic-friendly side dish.\\n- Suggest a diabetic-friendly side that's ready in under 20 minutes.\\n- Side dish, diabetic-friendly, 20 minutes.\\n\\n- High-fiber lite breakfast, less than 20 minutes.\\n- What's a quick, light, high-fiber breakfast recipe, under 20 minutes?\\n- I need a lite breakfast high in fiber that takes less than 20 minutes.\\n- Suggest a less than 20-minute high-fiber light breakfast.\\n- Lite breakfast, high-fiber, fast, under 20 minutes.\\n\\n- Lactose-free holiday meal, overnight prep.\\n- What are some overnight lactose-free holiday meal recipes?\\n- I'm planning a holiday meal, lactose-free, that can be prepped overnight.\\n- Give me ideas for an overnight lactose-free holiday meal.\\n- Holiday meal, lactose-free, takes overnight to prepare.\\n\\n- Comfort food party food, about an hour.\\n- What are some comfort food party food ideas I can make in roughly an hour?\\n- I need party food that's comfort food and takes about an hour to prepare.\\n- Suggest an hour-long comfort food party recipe.\\n- Party food, comfort food, roughly 60 minutes.\\n\\n- Instant quick bite with hummus and carrots.\\n- What's a really quick bite with hummus and carrots?\\n- I need an instant snack with hummus and carrots.\\n- Give me an immediate quick bite recipe using hummus and carrots.\\n- Hummus and carrots quick bite, takes no time.\\n\\n- Gluten-free breakfast, quick and easy.\\n- What's a fast and easy gluten-free breakfast idea?\\n- I need a gluten-free breakfast that's quick and simple to make.\\n- Suggest a quick and easy gluten-free breakfast.\\n- Breakfast, gluten-free, quick, easy.\\n\\n- Vegan lunch in 30 minutes.\\n- What's a good vegan lunch option for someone with only half an hour?\\n- Quick vegan lunch, ready in 30 minutes, please.\\n- Can you suggest a vegan lunch recipe that takes no more than 30 minutes?\\n- Give me ideas for a 30-minute vegan lunch.\\n\\n- High-protein dinner under 1 hour.\\n- What are some dinner ideas with high protein that take less than an hour to prepare?\\n- High-protein dinner, quick, under 60 minutes.\\n- Suggest a dinner with high protein that's ready in under an hour.\\n- I need a high-protein dinner recipe that won't take more than an hour.\\n\\n- Not spicy snack, 10 minutes.\\n- What's a good non-spicy snack I can make in 10 minutes?\\n- I need a snack ready in 10 minutes, not spicy.\\n- Give me ideas for a 10-minute mild snack.\\n- Snack, non-spicy, 10 minutes.\\n\\n- Vegetarian dessert, 45 minutes.\\n- What vegetarian desserts can I make in 45 minutes?\\n- I'm looking for a vegetarian dessert that takes around 45 minutes.\\n- Dessert, vegetarian, 45-minute prep time.\\n- Can you suggest a vegetarian dessert recipe that takes about 45 minutes?\\n\\n- Meaty brunch, I have all the time until retirement.\\n- What are some elaborate meaty brunch recipes? I have unlimited time.\\n- Meaty brunch ideas, no time limit.\\n- Suggest a meaty brunch that takes a long time to prepare.\\n- I need a meaty brunch recipe, I have all the time in the world.\\n\\n- Dairy-free appetizer in 15 minutes.\\n- What's an easy dairy-free appetizer I can make in 15 minutes?\\n- I need a dairy-free appetizer recipe that's ready in 15 minutes.\\n- Give me a 15-minute dairy-free appetizer.\\n- Appetizer, dairy-free, quick, 15 minutes.\\n\\n- Low-carb side dish in 20 minutes.\\n- What are some low-carb side dishes I can prepare in 20 minutes?\\n- I need a 20-minute low-carb side dish.\\n- Suggest a low-carb side that's ready in under 20 minutes.\\n- Side dish, low-carb, 20 minutes.\\n\\n- Pescatarian lite breakfast, less than 20 minutes.\\n- What's a quick, light, pescatarian breakfast recipe, under 20 minutes?\\n- I need a lite breakfast for pescatarians that takes less than 20 minutes.\\n- Suggest a less than 20-minute pescatarian light breakfast.\\n- Lite breakfast, pescatarian, fast, under 20 minutes.\\n\\n- Kid-friendly holiday meal, all day.\\n- What are some full-day kid-friendly holiday meal recipes?\\n- I'm planning a holiday meal, kid-friendly, with no time constraints.\\n- Give me ideas for an all-day kid-friendly holiday meal.\\n- Holiday meal, kid-friendly, takes all day to make.\\n\\n- Nut-free party food, about an hour.\\n- What are some nut-free party food ideas I can make in roughly an hour?\\n- I need party food that's nut-free and takes about an hour to prepare.\\n- Suggest an hour-long nut-free party recipe.\\n- Party food, nut-free, roughly 60 minutes.\\n\\n- Sugar-free quick bite, instant.\\n- What's a really quick sugar-free bite?\\n- I need an instant sugar-free snack.\\n- Give me an immediate sugar-free quick bite recipe.\\n- Sugar-free quick bite, takes no time.\\n\\n- Spicy breakfast, quick and easy.\\n- What's a fast and easy spicy breakfast idea?\\n- I need a spicy breakfast that's quick and simple to make.\\n- Suggest a quick and easy spicy breakfast.\\n- Breakfast, spicy, quick, easy.\\n\\n- Keto lunch in 30 minutes.\\n- What's a good keto lunch option for someone with only half an hour?\\n- Quick keto lunch, ready in 30 minutes, please.\\n- Can you suggest a keto lunch recipe that takes no more than 30 minutes?\\n- Give me ideas for a 30-minute keto lunch.\\n\\n- Heart-healthy dinner under 1 hour.\\n- What heart-healthy dinner can I make in less than an hour?\\n- I need a quick heart-healthy dinner that's ready in under 60 minutes.\\n- Suggest a heart-healthy dinner that doesn't take more than an hour.\\n- Dinner, heart-healthy, under an hour.\\n\\n- Low-salt snack, 10 minutes.\\n- What's a good low-salt snack I can make in 10 minutes?\\n- I need a snack ready in 10 minutes, low-salt.\\n- Give me ideas for a 10-minute low-salt snack.\\n- Snack, low-salt, 10 minutes.\\n\\n- Diabetic-friendly dessert, 45 minutes.\\n- What diabetic-friendly desserts can I make in 45 minutes?\\n- I'm looking for a diabetic-friendly dessert that takes around 45 minutes.\\n- Dessert, diabetic-friendly, 45-minute prep time.\\n- Can you suggest a diabetic-friendly dessert recipe that takes about 45 minutes?\\n\\n- High-fiber brunch, long prep.\\n- What are some high-fiber brunch recipes that take a long time to prepare?\\n- I'm looking for a high-fiber brunch that requires long prep.\\n- Suggest a high-fiber brunch that's time-consuming.\\n- Brunch, high-fiber, long preparation time.\\n\\n- Lactose-free appetizer in 15 minutes.\\n- What's an easy lactose-free appetizer I can make in 15 minutes?\\n- I need a lactose-free appetizer recipe that's ready in 15 minutes.\\n- Give me a 15-minute lactose-free appetizer.\\n- Appetizer, lactose-free, quick, 15 minutes.\\n\\n- Comfort food side dish in 20 minutes.\\n- What are some comfort food side dishes I can prepare in 20 minutes?\\n- I need a 20-minute comfort food side dish.\\n- Suggest a comfort food side that's ready in under 20 minutes.\\n- Side dish, comfort food, 20 minutes.\\n\\n- Paleo lite breakfast, less than 20 minutes.\\n- What's a quick, light, paleo breakfast recipe, under 20 minutes?\\n- I need a lite breakfast for paleo dieters that takes less than 20 minutes.\\n- Suggest a less than 20-minute paleo light breakfast.\\n- Lite breakfast, paleo, fast, under 20 minutes.\\n\\n- Holiday meal with hummus and carrots, overnight.\\n- What are some overnight holiday meal recipes featuring hummus and carrots?\\n- I'm planning a holiday meal with hummus and carrots that can be prepped overnight.\\n- Give me ideas for an overnight holiday meal with hummus and carrots.\\n- Holiday meal, hummus and carrots, takes overnight to prepare.\\n\\n- Not spicy party food, about an hour.\\n- What are some non-spicy party food ideas I can make in roughly an hour?\\n- I need party food that's not spicy and takes about an hour to prepare.\\n- Suggest an hour-long mild party recipe.\\n- Party food, not spicy, roughly 60 minutes.\\n\\n- Vegetarian quick bite, instant.\\n- What's a really quick vegetarian bite?\\n- I need an instant vegetarian snack.\\n- Give me an immediate vegetarian quick bite recipe.\\n- Vegetarian quick bite, takes no time.\\n\\n- Gluten-free dinner in 10 minutes.\\n- What's a good gluten-free dinner I can make in 10 minutes?\\n- I need a gluten-free dinner that's super fast, 10 minutes.\\n- Suggest a 10-minute gluten-free dinner.\\n- Dinner, gluten-free, 10 minutes.\\n\\n- Meaty lunch, something quick.\\n- What's a fast meaty lunch idea?\\n- I need a quick meaty lunch.\\n- Suggest a meaty lunch that's quick to prepare.\\n- Lunch, meaty, quick.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 3672,\n",
      "    \"totalTokenCount\": 4879,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 115\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"QxlgaKzeEdH4xN8PwJ6QuAg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a vegetarian lunch recipe that I can make in 30 minutes.\\n- What's a good vegetarian lunch option for someone with only half an hour?\\n- Quick vegetarian lunch, ready in 30 minutes, please.\\n- Can you suggest a vegetarian lunch recipe that takes no more than 30 minutes?\\n- Give me ideas for a 30-minute vegetarian lunch.\\n\\n- I'm looking for a meaty dinner recipe that takes under an hour.\\n- What are some dinner ideas with meat that take less than an hour to prepare?\\n- Meaty dinner, quick, under 60 minutes.\\n- Suggest a dinner with meat that's ready in under an hour.\\n- I need a meaty dinner recipe that won't take more than an hour.\\n\\n- Quick gluten-free snack ideas?\\n- What's a good gluten-free snack I can make quickly?\\n- I need a fast gluten-free snack.\\n- Give me a quick gluten-free snack recipe.\\n- Gluten-free snack, super quick.\\n\\n- Dairy-free dessert, about 45 minutes to make.\\n- What dairy-free desserts can I make in 45 minutes?\\n- I'm looking for a dairy-free dessert that takes around 45 minutes.\\n- Dessert, dairy-free, 45-minute prep time.\\n- Can you suggest a dairy-free dessert recipe that takes about 45 minutes?\\n\\n- I'm making brunch and want a vegan recipe, I have all day to cook.\\n- What are some elaborate vegan brunch recipes? I have unlimited time.\\n- Vegan brunch ideas, no time limit.\\n- Suggest a vegan brunch that takes a long time to prepare.\\n- I need a vegan brunch recipe, I have all the time in the world.\\n\\n- Quick low-carb appetizer for 15 minutes.\\n- What's an easy low-carb appetizer I can make in 15 minutes?\\n- I need a low-carb appetizer recipe that's ready in 15 minutes.\\n- Give me a 15-minute low-carb appetizer.\\n- Appetizer, low-carb, quick, 15 minutes.\\n\\n- High-protein side dish that takes about 20 minutes to make.\\n- What are some high-protein side dishes I can prepare in 20 minutes?\\n- I need a 20-minute high-protein side dish.\\n- Suggest a high-protein side that's ready in under 20 minutes.\\n- Side dish, high-protein, 20 minutes.\\n\\n- Kid-friendly lite breakfast, less than 20 minutes.\\n- What's a quick, light, kid-friendly breakfast recipe, under 20 minutes?\\n- I need a lite breakfast for kids that takes less than 20 minutes.\\n- Suggest a less than 20-minute kid-friendly light breakfast.\\n- Lite breakfast, kid-friendly, fast, under 20 minutes.\\n\\n- Pescatarian holiday meal, I have all day.\\n- What are some full-day pescatarian holiday meal recipes?\\n- I'm planning a holiday meal, pescatarian, with no time constraints.\\n- Give me ideas for an all-day pescatarian holiday meal.\\n- Holiday meal, pescatarian, takes all day to make.\\n\\n- Sugar-free party food that takes about an hour.\\n- What are some sugar-free party food ideas I can make in roughly an hour?\\n- I need party food that's sugar-free and takes about an hour to prepare.\\n- Suggest an hour-long sugar-free party food recipe.\\n- Party food, sugar-free, roughly 60 minutes.\\n\\n- Instant nut-free quick bite.\\n- What's a really quick nut-free bite?\\n- I need an instant nut-free snack.\\n- Give me an immediate nut-free quick bite recipe.\\n- Nut-free quick bite, takes no time.\\n\\n- Keto breakfast in 30 minutes.\\n- What's a quick keto breakfast recipe that takes 30 minutes?\\n- I need a keto-friendly breakfast I can make in half an hour.\\n- Suggest a 30-minute keto breakfast.\\n- Breakfast, keto, 30 minutes.\\n\\n- Heart-healthy lunch under 1 hour.\\n- What heart-healthy lunch can I make in less than an hour?\\n- I need a quick heart-healthy lunch that's ready in under 60 minutes.\\n- Suggest a heart-healthy lunch that doesn't take more than an hour.\\n- Lunch, heart-healthy, under an hour.\\n\\n- Spicy dinner ready in 45 minutes.\\n- What's a good spicy dinner recipe I can make in 45 minutes?\\n- I need a spicy dinner that takes about 45 minutes.\\n- Suggest a 45-minute spicy dinner.\\n- Dinner, spicy, 45 minutes.\\n\\n- Snack without fish or meat, 10 minutes.\\n- What's a quick 10-minute snack that's vegetarian (no fish or meat)?\\n- I need a snack ready in 10 minutes, no fish or meat.\\n- Give me ideas for a 10-minute snack, no animal products.\\n- Snack, plant-based, 10 minutes.\\n\\n- Meaty dessert with long prep.\\n- What are some meaty dessert recipes that take a long time to prepare?\\n- I'm looking for a meaty dessert that requires long prep.\\n- Suggest a meaty dessert that's time-consuming.\\n- Dessert, meaty, long preparation time.\\n\\n- Quick paleo brunch.\\n- What's a fast paleo brunch idea?\\n- I need a paleo brunch that's quick to make.\\n- Suggest a quick paleo-friendly brunch.\\n- Paleo brunch, quick.\\n\\n- Low-salt appetizer in 15 minutes.\\n- What's an easy low-salt appetizer I can make in 15 minutes?\\n- I need a low-salt appetizer recipe that's ready in 15 minutes.\\n- Give me a 15-minute low-salt appetizer.\\n- Appetizer, low-salt, quick, 15 minutes.\\n\\n- Diabetic-friendly side dish in 20 minutes.\\n- What are some diabetic-friendly side dishes I can prepare in 20 minutes?\\n- I need a 20-minute diabetic-friendly side dish.\\n- Suggest a diabetic-friendly side that's ready in under 20 minutes.\\n- Side dish, diabetic-friendly, 20 minutes.\\n\\n- High-fiber lite breakfast, less than 20 minutes.\\n- What's a quick, light, high-fiber breakfast recipe, under 20 minutes?\\n- I need a lite breakfast high in fiber that takes less than 20 minutes.\\n- Suggest a less than 20-minute high-fiber light breakfast.\\n- Lite breakfast, high-fiber, fast, under 20 minutes.\\n\\n- Lactose-free holiday meal, overnight prep.\\n- What are some overnight lactose-free holiday meal recipes?\\n- I'm planning a holiday meal, lactose-free, that can be prepped overnight.\\n- Give me ideas for an overnight lactose-free holiday meal.\\n- Holiday meal, lactose-free, takes overnight to prepare.\\n\\n- Comfort food party food, about an hour.\\n- What are some comfort food party food ideas I can make in roughly an hour?\\n- I need party food that's comfort food and takes about an hour to prepare.\\n- Suggest an hour-long comfort food party recipe.\\n- Party food, comfort food, roughly 60 minutes.\\n\\n- Instant quick bite with hummus and carrots.\\n- What's a really quick bite with hummus and carrots?\\n- I need an instant snack with hummus and carrots.\\n- Give me an immediate quick bite recipe using hummus and carrots.\\n- Hummus and carrots quick bite, takes no time.\\n\\n- Gluten-free breakfast, quick and easy.\\n- What's a fast and easy gluten-free breakfast idea?\\n- I need a gluten-free breakfast that's quick and simple to make.\\n- Suggest a quick and easy gluten-free breakfast.\\n- Breakfast, gluten-free, quick, easy.\\n\\n- Vegan lunch in 30 minutes.\\n- What's a good vegan lunch option for someone with only half an hour?\\n- Quick vegan lunch, ready in 30 minutes, please.\\n- Can you suggest a vegan lunch recipe that takes no more than 30 minutes?\\n- Give me ideas for a 30-minute vegan lunch.\\n\\n- High-protein dinner under 1 hour.\\n- What are some dinner ideas with high protein that take less than an hour to prepare?\\n- High-protein dinner, quick, under 60 minutes.\\n- Suggest a dinner with high protein that's ready in under an hour.\\n- I need a high-protein dinner recipe that won't take more than an hour.\\n\\n- Not spicy snack, 10 minutes.\\n- What's a good non-spicy snack I can make in 10 minutes?\\n- I need a snack ready in 10 minutes, not spicy.\\n- Give me ideas for a 10-minute mild snack.\\n- Snack, non-spicy, 10 minutes.\\n\\n- Vegetarian dessert, 45 minutes.\\n- What vegetarian desserts can I make in 45 minutes?\\n- I'm looking for a vegetarian dessert that takes around 45 minutes.\\n- Dessert, vegetarian, 45-minute prep time.\\n- Can you suggest a vegetarian dessert recipe that takes about 45 minutes?\\n\\n- Meaty brunch, I have all the time until retirement.\\n- What are some elaborate meaty brunch recipes? I have unlimited time.\\n- Meaty brunch ideas, no time limit.\\n- Suggest a meaty brunch that takes a long time to prepare.\\n- I need a meaty brunch recipe, I have all the time in the world.\\n\\n- Dairy-free appetizer in 15 minutes.\\n- What's an easy dairy-free appetizer I can make in 15 minutes?\\n- I need a dairy-free appetizer recipe that's ready in 15 minutes.\\n- Give me a 15-minute dairy-free appetizer.\\n- Appetizer, dairy-free, quick, 15 minutes.\\n\\n- Low-carb side dish in 20 minutes.\\n- What are some low-carb side dishes I can prepare in 20 minutes?\\n- I need a 20-minute low-carb side dish.\\n- Suggest a low-carb side that's ready in under 20 minutes.\\n- Side dish, low-carb, 20 minutes.\\n\\n- Pescatarian lite breakfast, less than 20 minutes.\\n- What's a quick, light, pescatarian breakfast recipe, under 20 minutes?\\n- I need a lite breakfast for pescatarians that takes less than 20 minutes.\\n- Suggest a less than 20-minute pescatarian light breakfast.\\n- Lite breakfast, pescatarian, fast, under 20 minutes.\\n\\n- Kid-friendly holiday meal, all day.\\n- What are some full-day kid-friendly holiday meal recipes?\\n- I'm planning a holiday meal, kid-friendly, with no time constraints.\\n- Give me ideas for an all-day kid-friendly holiday meal.\\n- Holiday meal, kid-friendly, takes all day to make.\\n\\n- Nut-free party food, about an hour.\\n- What are some nut-free party food ideas I can make in roughly an hour?\\n- I need party food that's nut-free and takes about an hour to prepare.\\n- Suggest an hour-long nut-free party recipe.\\n- Party food, nut-free, roughly 60 minutes.\\n\\n- Sugar-free quick bite, instant.\\n- What's a really quick sugar-free bite?\\n- I need an instant sugar-free snack.\\n- Give me an immediate sugar-free quick bite recipe.\\n- Sugar-free quick bite, takes no time.\\n\\n- Spicy breakfast, quick and easy.\\n- What's a fast and easy spicy breakfast idea?\\n- I need a spicy breakfast that's quick and simple to make.\\n- Suggest a quick and easy spicy breakfast.\\n- Breakfast, spicy, quick, easy.\\n\\n- Keto lunch in 30 minutes.\\n- What's a good keto lunch option for someone with only half an hour?\\n- Quick keto lunch, ready in 30 minutes, please.\\n- Can you suggest a keto lunch recipe that takes no more than 30 minutes?\\n- Give me ideas for a 30-minute keto lunch.\\n\\n- Heart-healthy dinner under 1 hour.\\n- What heart-healthy dinner can I make in less than an hour?\\n- I need a quick heart-healthy dinner that's ready in under 60 minutes.\\n- Suggest a heart-healthy dinner that doesn't take more than an hour.\\n- Dinner, heart-healthy, under an hour.\\n\\n- Low-salt snack, 10 minutes.\\n- What's a good low-salt snack I can make in 10 minutes?\\n- I need a snack ready in 10 minutes, low-salt.\\n- Give me ideas for a 10-minute low-salt snack.\\n- Snack, low-salt, 10 minutes.\\n\\n- Diabetic-friendly dessert, 45 minutes.\\n- What diabetic-friendly desserts can I make in 45 minutes?\\n- I'm looking for a diabetic-friendly dessert that takes around 45 minutes.\\n- Dessert, diabetic-friendly, 45-minute prep time.\\n- Can you suggest a diabetic-friendly dessert recipe that takes about 45 minutes?\\n\\n- High-fiber brunch, long prep.\\n- What are some high-fiber brunch recipes that take a long time to prepare?\\n- I'm looking for a high-fiber brunch that requires long prep.\\n- Suggest a high-fiber brunch that's time-consuming.\\n- Brunch, high-fiber, long preparation time.\\n\\n- Lactose-free appetizer in 15 minutes.\\n- What's an easy lactose-free appetizer I can make in 15 minutes?\\n- I need a lactose-free appetizer recipe that's ready in 15 minutes.\\n- Give me a 15-minute lactose-free appetizer.\\n- Appetizer, lactose-free, quick, 15 minutes.\\n\\n- Comfort food side dish in 20 minutes.\\n- What are some comfort food side dishes I can prepare in 20 minutes?\\n- I need a 20-minute comfort food side dish.\\n- Suggest a comfort food side that's ready in under 20 minutes.\\n- Side dish, comfort food, 20 minutes.\\n\\n- Paleo lite breakfast, less than 20 minutes.\\n- What's a quick, light, paleo breakfast recipe, under 20 minutes?\\n- I need a lite breakfast for paleo dieters that takes less than 20 minutes.\\n- Suggest a less than 20-minute paleo light breakfast.\\n- Lite breakfast, paleo, fast, under 20 minutes.\\n\\n- Holiday meal with hummus and carrots, overnight.\\n- What are some overnight holiday meal recipes featuring hummus and carrots?\\n- I'm planning a holiday meal with hummus and carrots that can be prepped overnight.\\n- Give me ideas for an overnight holiday meal with hummus and carrots.\\n- Holiday meal, hummus and carrots, takes overnight to prepare.\\n\\n- Not spicy party food, about an hour.\\n- What are some non-spicy party food ideas I can make in roughly an hour?\\n- I need party food that's not spicy and takes about an hour to prepare.\\n- Suggest an hour-long mild party recipe.\\n- Party food, not spicy, roughly 60 minutes.\\n\\n- Vegetarian quick bite, instant.\\n- What's a really quick vegetarian bite?\\n- I need an instant vegetarian snack.\\n- Give me an immediate vegetarian quick bite recipe.\\n- Vegetarian quick bite, takes no time.\\n\\n- Gluten-free dinner in 10 minutes.\\n- What's a good gluten-free dinner I can make in 10 minutes?\\n- I need a gluten-free dinner that's super fast, 10 minutes.\\n- Suggest a 10-minute gluten-free dinner.\\n- Dinner, gluten-free, 10 minutes.\\n\\n- Meaty lunch, something quick.\\n- What's a fast meaty lunch idea?\\n- I need a quick meaty lunch.\\n- Suggest a meaty lunch that's quick to prepare.\\n- Lunch, meaty, quick.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 3672,\n",
      "    \"totalTokenCount\": 4879,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 115\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"QxlgaKzeEdH4xN8PwJ6QuAg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:33:07 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:33:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:33:07 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:33:07 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:33:07 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:33:07 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.009795100000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.009795100000000001\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:33:07 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:33:07 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:33:07 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:33:07 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:33:07 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:33:07 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:33:07 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:33:07 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:33:07 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:33:07 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:33:07 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:33:07 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:33:07 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:33:07 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1456d31a0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x14549cf50> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1456d2ed0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:33:13 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=6278'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:33:13 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in about 30 minutes.\\n- What are some quick vegetarian lunch ideas that take half an hour?\\n- Give me a 30-minute vegetarian lunch.\\n- I need a vegetarian lunch recipe, something that's ready in 30 minutes.\\n- Show me some fast vegetarian lunch options, 30 min maximum prep time.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 87,\n",
      "    \"totalTokenCount\": 1286,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 107\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"SRlgaP-HL4mOxN8Plp2x6AU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in about 30 minutes.\\n- What are some quick vegetarian lunch ideas that take half an hour?\\n- Give me a 30-minute vegetarian lunch.\\n- I need a vegetarian lunch recipe, something that's ready in 30 minutes.\\n- Show me some fast vegetarian lunch options, 30 min maximum prep time.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 87,\n",
      "    \"totalTokenCount\": 1286,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 107\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"SRlgaP-HL4mOxN8Plp2x6AU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:33:13 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:33:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:33:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:33:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:33:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:33:13 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0008126\n",
      "DEBUG:LiteLLM:response_cost: 0.0008126\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:33:13 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:33:13 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:33:13 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:33:13 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:33:13 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:33:13 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:33:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:33:13 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:33:13 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:33:13 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:33:13 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:33:13 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:33:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:33:13 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1456d17f0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x145629b50> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1456d1df0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:33:19 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=5966'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:33:19 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in about 30 minutes.\\n- What's a good vegetarian lunch option that takes less than half an hour?\\n- Suggest some quick vegetarian lunch ideas, around 30 minutes prep time.\\n- Need a 30-minute vegetarian lunch.\\n- Show me some lunch recipes for vegetarians that are ready in 30 minutes.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 86,\n",
      "    \"totalTokenCount\": 1391,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 213\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"TxlgaITiNbvgvdIPhO79wAw\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in about 30 minutes.\\n- What's a good vegetarian lunch option that takes less than half an hour?\\n- Suggest some quick vegetarian lunch ideas, around 30 minutes prep time.\\n- Need a 30-minute vegetarian lunch.\\n- Show me some lunch recipes for vegetarians that are ready in 30 minutes.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 86,\n",
      "    \"totalTokenCount\": 1391,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 213\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"TxlgaITiNbvgvdIPhO79wAw\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:33:19 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:33:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:33:19 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:33:19 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:33:19 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:33:19 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0010751\n",
      "DEBUG:LiteLLM:response_cost: 0.0010751\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:33:19 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:33:19 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:33:19 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:33:19 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:33:19 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:33:19 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:33:19 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:33:19 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:33:19 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:33:19 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:33:19 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:33:19 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:33:19 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:33:19 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1456d2a20>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x14562a2d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145702210>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:33:25 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=5582'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:33:25 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in about 30 minutes.\\n- What can I cook for lunch that's vegetarian and ready in half an hour?\\n- Give me some quick vegetarian lunch ideas, 30 minutes max.\\n- I need a vegetarian lunch recipe that only takes 30 minutes.\\n- Lunch, vegetarian, 30 minutes, please.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 85,\n",
      "    \"totalTokenCount\": 1290,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 113\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"VRlgaIyVJtLZvdIP9LasqQg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in about 30 minutes.\\n- What can I cook for lunch that's vegetarian and ready in half an hour?\\n- Give me some quick vegetarian lunch ideas, 30 minutes max.\\n- I need a vegetarian lunch recipe that only takes 30 minutes.\\n- Lunch, vegetarian, 30 minutes, please.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 85,\n",
      "    \"totalTokenCount\": 1290,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 113\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"VRlgaIyVJtLZvdIP9LasqQg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:33:25 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:33:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:33:25 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:33:25 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:33:25 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:33:25 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0008225999999999999\n",
      "DEBUG:LiteLLM:response_cost: 0.0008225999999999999\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:33:25 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:33:25 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:33:25 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:33:25 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:33:25 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:33:25 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:33:25 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:33:25 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:33:25 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:33:25 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:33:25 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:33:25 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:33:25 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:33:25 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x135e0bc20>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1456c4dd0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x103143800>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:34:06 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=40801'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:34:06 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that takes about 30 minutes.\\n- Can you suggest a quick veggie lunch, around half an hour?\\n- Vegetarian lunch, 30 min.\\n- What's a good vegetarian lunch to make in 30 minutes or less?\\n- I need a plant-based lunch recipe, quick, about 30 mins.\\n\\n- Show me some meaty dinner recipes ready in under an hour.\\n- I want a dinner with meat that can be cooked in less than 60 minutes.\\n- Meaty dinner ideas, quick, under an hour.\\n- What can I make for dinner tonight that's meaty and doesn't take too long, say, less than an hour?\\n- Give me a quick meaty dinner recipe, under 60 mins.\\n\\n- I need a quick gluten-free snack.\\n- What are some fast GF snack ideas?\\n- Quick and easy gluten-free snacks, please.\\n- Gluten-free snack, super fast.\\n- Suggest a quick gluten-free bite.\\n\\n- I'm looking for a dairy-free dessert recipe that takes around 45 minutes.\\n- Can you find a dessert without dairy, ready in about 45 mins?\\n- Dairy-free dessert, 45 minutes prep time.\\n- I need a sweet treat, no dairy, under 45 minutes to make.\\n- What dairy-free desserts can I make in three-quarters of an hour?\\n\\n- I want an elaborate vegan brunch recipe. Time is not an issue.\\n- Give me a complex vegan brunch recipe, I have plenty of time.\\n- Vegan brunch, no time limit.\\n- Suggest a vegan brunch recipe that takes a long time to prepare.\\n- I'm retired and want to make a big vegan brunch.\\n\\n- Quick low-carb appetizer in 15 minutes.\\n- I need a low-carb app that's ready in about a quarter-hour.\\n- What appetizers are low in carbs and can be made fast, like 15 minutes?\\n- Give me a 15-minute, low-carb starter.\\n- Fast low-carb appetizer.\\n\\n- Can you find a high-protein side dish I can make in 20 minutes?\\n- I need a side that's high in protein and quick, under 20 mins.\\n- High-protein side, ready in 20.\\n- What 20-minute, high-protein side dishes do you have?\\n- Quick high-protein side.\\n\\n- Give me a light, kid-friendly breakfast that takes less than 20 minutes.\\n- I need a quick, easy breakfast for kids, under 20 mins.\\n- Kid-friendly, light breakfast, fast (under 20 minutes).\\n- What can I make for a quick and light breakfast for my children, in under 20 mins?\\n- Fast kid-friendly lite breakfast.\\n\\n- I'm planning a pescatarian holiday meal, I have all day to cook.\\n- Suggest a grand pescatarian holiday recipe that takes all day.\\n- Pescatarian holiday meal, long cooking time.\\n- What elaborate pescatarian dishes can I make for a holiday?\\n- I have all day to prepare a pescatarian holiday feast.\\n\\n- I need some sugar-free party food that takes about an hour to prepare.\\n- What party snacks are sugar-free and take around 60 minutes?\\n- Sugar-free party food, 1 hour prep.\\n- Show me party food recipes without sugar, ready in about an hour.\\n- About an hour for sugar-free party treats.\\n\\n- I need an instant nut-free quick bite.\\n- What are some nut-free snacks I can make instantly?\\n- Nut-free quick bite, super fast.\\n- Instant nut-free snack.\\n- Give me a nut-free bite, right now.\\n\\n- I'm looking for a keto breakfast recipe ready in 30 minutes.\\n- Can you suggest a quick keto breakfast, about half an hour?\\n- Keto breakfast, 30 min.\\n- What's a good keto breakfast to make in 30 minutes or less?\\n- I need a low-carb, high-fat breakfast recipe, quick, about 30 mins.\\n\\n- Show me some heart-healthy lunch recipes ready in under an hour.\\n- I want a heart-healthy lunch that can be cooked in less than 60 minutes.\\n- Heart-healthy lunch ideas, quick, under an hour.\\n- What can I make for lunch that's good for my heart and doesn't take too long, say, less than an hour?\\n- Give me a quick heart-healthy lunch recipe, under 60 mins.\\n\\n- I'm looking for a spicy dinner recipe that takes around 45 minutes.\\n- Can you find a dinner recipe that's spicy and ready in about 45 mins?\\n- Spicy dinner, 45 minutes prep time.\\n- I need a hot dinner meal, ready in under 45 minutes.\\n- What spicy dinners can I make in three-quarters of an hour?\\n\\n- I need a quick snack without fish or meat, ready in 10 minutes.\\n- What are some fast, meat-free snacks I can make in 10 mins?\\n- Vegetarian snack, 10 min.\\n- Give me a ten-minute snack without any animal protein.\\n- Quick 10-minute snack, no fish or meat.\\n\\n- I'm looking for a meaty dessert recipe with a long prep time.\\n- Can you suggest an elaborate meaty dessert?\\n- Meaty dessert, takes a long time to prepare.\\n- What lengthy meaty dessert recipes do you have?\\n- I have all day for a meaty dessert.\\n\\n- I need a quick paleo brunch.\\n- What are some fast paleo brunch ideas?\\n- Quick and easy paleo brunch, please.\\n- Paleo brunch, super fast.\\n- Suggest a quick paleo mid-morning meal.\\n\\n- Quick low-salt appetizer in 15 minutes.\\n- I need a low-sodium app that's ready in about a quarter-hour.\\n- What appetizers are low in salt and can be made fast, like 15 minutes?\\n- Give me a 15-minute, low-salt starter.\\n- Fast low-salt appetizer.\\n\\n- Can you find a diabetic-friendly side dish I can make in 20 minutes?\\n- I need a side that's good for diabetics and quick, under 20 mins.\\n- Diabetic-friendly side, ready in 20.\\n- What 20-minute, diabetic-friendly side dishes do you have?\\n- Quick diabetic-friendly side.\\n\\n- Give me a light, high-fiber breakfast that takes less than 20 minutes.\\n- I need a quick, easy, high-fiber breakfast, under 20 mins.\\n- High-fiber, light breakfast, fast (under 20 minutes).\\n- What can I make for a quick and light high-fiber breakfast in under 20 mins?\\n- Fast high-fiber lite breakfast.\\n\\n- I'm planning a lactose-free holiday meal, that requires overnight prep.\\n- Suggest an elaborate lactose-free holiday recipe that needs overnight preparation.\\n- Lactose-free holiday meal, overnight prep.\\n- What complex lactose-free holiday dishes need to be prepared overnight?\\n- I need an overnight lactose-free holiday feast.\\n\\n- I need some comfort food for a party that takes about an hour to prepare.\\n- What party snacks are comfort food and take around 60 minutes?\\n- Comfort food party food, 1 hour prep.\\n- Show me party food recipes that are comforting and ready in about an hour.\\n- About an hour for comfort food party treats.\\n\\n- I need an instant quick bite with hummus and carrots.\\n- What are some quick bites with hummus and carrots I can make instantly?\\n- Hummus and carrots quick bite, super fast.\\n- Instant snack with hummus and carrots.\\n- Give me a quick bite, hummus and carrots.\\n\\n- I want a quick and easy gluten-free breakfast.\\n- Give me a fast GF breakfast recipe.\\n- Gluten-free breakfast, quick and easy.\\n- What's an easy gluten-free breakfast I can make quickly?\\n- I need a simple, speedy gluten-free breakfast.\\n\\n- I'm looking for a vegan lunch recipe that takes about 30 minutes.\\n- Can you suggest a quick vegan lunch, around half an hour?\\n- Vegan lunch, 30 min.\\n- What's a good vegan lunch to make in 30 minutes or less?\\n- I need a plant-based lunch recipe, quick, about 30 mins.\\n\\n- Show me some high-protein dinner recipes ready in under an hour.\\n- I want a dinner with high protein that can be cooked in less than 60 minutes.\\n- High-protein dinner ideas, quick, under an hour.\\n- What can I make for dinner tonight that's high in protein and doesn't take too long, say, less than an hour?\\n- Give me a quick high-protein dinner recipe, under 60 mins.\\n\\n- I need a quick snack that is not spicy and ready in 10 minutes.\\n- What are some fast, mild snacks I can make in 10 mins?\\n- Non-spicy snack, 10 min.\\n- Give me a ten-minute snack that isn't hot.\\n- Quick 10-minute snack, mild to taste.\\n\\n- I'm looking for a vegetarian dessert recipe that takes around 45 minutes.\\n- Can you find a dessert that's vegetarian and ready in about 45 mins?\\n- Vegetarian dessert, 45 minutes prep time.\\n- I need a sweet treat, vegetarian, ready in under 45 minutes.\\n- What vegetarian desserts can I make in three-quarters of an hour?\\n\\n- I want an elaborate meaty brunch recipe. Time is not an issue.\\n- Give me a complex meaty brunch recipe, I have plenty of time.\\n- Meaty brunch, no time limit.\\n- Suggest a meaty brunch recipe that takes a long time to prepare.\\n- I'm retired and want to make a big meaty brunch.\\n\\n- Quick dairy-free appetizer in 15 minutes.\\n- I need a dairy-free app that's ready in about a quarter-hour.\\n- What appetizers are dairy-free and can be made fast, like 15 minutes?\\n- Give me a 15-minute, dairy-free starter.\\n- Fast dairy-free appetizer.\\n\\n- Can you find a low-carb side dish I can make in 20 minutes?\\n- I need a side that's low in carbs and quick, under 20 mins.\\n- Low-carb side, ready in 20.\\n- What 20-minute, low-carb side dishes do you have?\\n- Quick low-carb side.\\n\\n- Give me a light, pescatarian breakfast that takes less than 20 minutes.\\n- I need a quick, easy, pescatarian breakfast, under 20 mins.\\n- Pescatarian, light breakfast, fast (under 20 minutes).\\n- What can I make for a quick and light pescatarian breakfast in under 20 mins?\\n- Fast pescatarian lite breakfast.\\n\\n- I'm planning a kid-friendly holiday meal, I have all day to cook.\\n- Suggest an elaborate kid-friendly holiday recipe that takes all day.\\n- Kid-friendly holiday meal, long cooking time.\\n- What elaborate kid-friendly dishes can I make for a holiday?\\n- I have all day to prepare a kid-friendly holiday feast.\\n\\n- I need some nut-free party food that takes about an hour to prepare.\\n- What party snacks are nut-free and take around 60 minutes?\\n- Nut-free party food, 1 hour prep.\\n- Show me party food recipes without nuts, ready in about an hour.\\n- About an hour for nut-free party treats.\\n\\n- I need an instant sugar-free quick bite.\\n- What are some sugar-free snacks I can make instantly?\\n- Sugar-free quick bite, super fast.\\n- Instant sugar-free snack.\\n- Give me a sugar-free bite, right now.\\n\\n- I want a quick and easy spicy breakfast.\\n- Give me a fast hot breakfast recipe.\\n- Spicy breakfast, quick and easy.\\n- What's an easy spicy breakfast I can make quickly?\\n- I need a simple, speedy spicy breakfast.\\n\\n- I'm looking for a keto lunch recipe that takes about 30 minutes.\\n- Can you suggest a quick keto lunch, around half an hour?\\n- Keto lunch, 30 min.\\n- What's a good keto lunch to make in 30 minutes or less?\\n- I need a low-carb, high-fat lunch recipe, quick, about 30 mins.\\n\\n- Show me some heart-healthy dinner recipes ready in under an hour.\\n- I want a heart-healthy dinner that can be cooked in less than 60 minutes.\\n- Heart-healthy dinner ideas, quick, under an hour.\\n- What can I make for dinner tonight that's good for my heart and doesn't take too long, say, less than an hour?\\n- Give me a quick heart-healthy dinner recipe, under 60 mins.\\n\\n- I need a quick snack that is low-salt and ready in 10 minutes.\\n- What are some fast, low-sodium snacks I can make in 10 mins?\\n- Low-salt snack, 10 min.\\n- Give me a ten-minute snack that's low in salt.\\n- Quick 10-minute snack, low-sodium.\\n\\n- I'm looking for a diabetic-friendly dessert recipe that takes around 45 minutes.\\n- Can you find a dessert that's good for diabetics and ready in about 45 mins?\\n- Diabetic-friendly dessert, 45 minutes prep time.\\n- I need a sweet treat, diabetic-friendly, ready in under 45 minutes.\\n- What diabetic-friendly desserts can I make in three-quarters of an hour?\\n\\n- I want an elaborate high-fiber brunch recipe.\\n- Give me a complex high-fiber brunch recipe that takes a long time.\\n- High-fiber brunch, long prep time.\\n- Suggest a high-fiber brunch recipe that takes a lot of time to prepare.\\n- I have all day for a high-fiber brunch.\\n\\n- Quick lactose-free appetizer in 15 minutes.\\n- I need a lactose-free app that's ready in about a quarter-hour.\\n- What appetizers are lactose-free and can be made fast, like 15 minutes?\\n- Give me a 15-minute, lactose-free starter.\\n- Fast lactose-free appetizer.\\n\\n- Can you find a comfort food side dish I can make in 20 minutes?\\n- I need a side that's comforting and quick, under 20 mins.\\n- Comfort food side, ready in 20.\\n- What 20-minute, comfort food side dishes do you have?\\n- Quick comfort food side.\\n\\n- Give me a light, paleo breakfast that takes less than 20 minutes.\\n- I need a quick, easy, paleo breakfast, under 20 mins.\\n- Paleo, light breakfast, fast (under 20 minutes).\\n- What can I make for a quick and light paleo breakfast in under 20 mins?\\n- Fast paleo lite breakfast.\\n\\n- I'm planning a holiday meal with hummus and carrots, that requires overnight prep.\\n- Suggest an elaborate recipe for a holiday meal featuring hummus and carrots that needs overnight preparation.\\n- Holiday meal with hummus and carrots, overnight prep.\\n- What complex holiday dishes with hummus and carrots need to be prepared overnight?\\n- I need an overnight holiday feast including hummus and carrots.\\n\\n- I need some non-spicy party food that takes about an hour to prepare.\\n- What party snacks are mild and take around 60 minutes?\\n- Not spicy party food, 1 hour prep.\\n- Show me party food recipes that aren't spicy and ready in about an hour.\\n- About an hour for mild party treats.\\n\\n- I need an instant vegetarian quick bite.\\n- What are some vegetarian snacks I can make instantly?\\n- Vegetarian quick bite, super fast.\\n- Instant vegetarian snack.\\n- Give me a vegetarian bite, right now.\\n\\n- I'm looking for a gluten-free dinner recipe that takes around 10 minutes.\\n- Can you find a gluten-free dinner that's ready in about 10 mins?\\n- Gluten-free dinner, 10 minutes prep time.\\n- I need a GF dinner meal, ready in under 10 minutes.\\n- What gluten-free dinners can I make in ten minutes?\\n\\n- I'm looking for a quick meaty lunch.\\n- Can you suggest a fast meaty lunch recipe?\\n- Meaty lunch, quick.\\n- What's a good meaty lunch I can make quickly?\\n- I need a speedy lunch with meat.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 3749,\n",
      "    \"totalTokenCount\": 10753,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 5912\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"fhlgaJihHqSBvdIPyOL1wAg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that takes about 30 minutes.\\n- Can you suggest a quick veggie lunch, around half an hour?\\n- Vegetarian lunch, 30 min.\\n- What's a good vegetarian lunch to make in 30 minutes or less?\\n- I need a plant-based lunch recipe, quick, about 30 mins.\\n\\n- Show me some meaty dinner recipes ready in under an hour.\\n- I want a dinner with meat that can be cooked in less than 60 minutes.\\n- Meaty dinner ideas, quick, under an hour.\\n- What can I make for dinner tonight that's meaty and doesn't take too long, say, less than an hour?\\n- Give me a quick meaty dinner recipe, under 60 mins.\\n\\n- I need a quick gluten-free snack.\\n- What are some fast GF snack ideas?\\n- Quick and easy gluten-free snacks, please.\\n- Gluten-free snack, super fast.\\n- Suggest a quick gluten-free bite.\\n\\n- I'm looking for a dairy-free dessert recipe that takes around 45 minutes.\\n- Can you find a dessert without dairy, ready in about 45 mins?\\n- Dairy-free dessert, 45 minutes prep time.\\n- I need a sweet treat, no dairy, under 45 minutes to make.\\n- What dairy-free desserts can I make in three-quarters of an hour?\\n\\n- I want an elaborate vegan brunch recipe. Time is not an issue.\\n- Give me a complex vegan brunch recipe, I have plenty of time.\\n- Vegan brunch, no time limit.\\n- Suggest a vegan brunch recipe that takes a long time to prepare.\\n- I'm retired and want to make a big vegan brunch.\\n\\n- Quick low-carb appetizer in 15 minutes.\\n- I need a low-carb app that's ready in about a quarter-hour.\\n- What appetizers are low in carbs and can be made fast, like 15 minutes?\\n- Give me a 15-minute, low-carb starter.\\n- Fast low-carb appetizer.\\n\\n- Can you find a high-protein side dish I can make in 20 minutes?\\n- I need a side that's high in protein and quick, under 20 mins.\\n- High-protein side, ready in 20.\\n- What 20-minute, high-protein side dishes do you have?\\n- Quick high-protein side.\\n\\n- Give me a light, kid-friendly breakfast that takes less than 20 minutes.\\n- I need a quick, easy breakfast for kids, under 20 mins.\\n- Kid-friendly, light breakfast, fast (under 20 minutes).\\n- What can I make for a quick and light breakfast for my children, in under 20 mins?\\n- Fast kid-friendly lite breakfast.\\n\\n- I'm planning a pescatarian holiday meal, I have all day to cook.\\n- Suggest a grand pescatarian holiday recipe that takes all day.\\n- Pescatarian holiday meal, long cooking time.\\n- What elaborate pescatarian dishes can I make for a holiday?\\n- I have all day to prepare a pescatarian holiday feast.\\n\\n- I need some sugar-free party food that takes about an hour to prepare.\\n- What party snacks are sugar-free and take around 60 minutes?\\n- Sugar-free party food, 1 hour prep.\\n- Show me party food recipes without sugar, ready in about an hour.\\n- About an hour for sugar-free party treats.\\n\\n- I need an instant nut-free quick bite.\\n- What are some nut-free snacks I can make instantly?\\n- Nut-free quick bite, super fast.\\n- Instant nut-free snack.\\n- Give me a nut-free bite, right now.\\n\\n- I'm looking for a keto breakfast recipe ready in 30 minutes.\\n- Can you suggest a quick keto breakfast, about half an hour?\\n- Keto breakfast, 30 min.\\n- What's a good keto breakfast to make in 30 minutes or less?\\n- I need a low-carb, high-fat breakfast recipe, quick, about 30 mins.\\n\\n- Show me some heart-healthy lunch recipes ready in under an hour.\\n- I want a heart-healthy lunch that can be cooked in less than 60 minutes.\\n- Heart-healthy lunch ideas, quick, under an hour.\\n- What can I make for lunch that's good for my heart and doesn't take too long, say, less than an hour?\\n- Give me a quick heart-healthy lunch recipe, under 60 mins.\\n\\n- I'm looking for a spicy dinner recipe that takes around 45 minutes.\\n- Can you find a dinner recipe that's spicy and ready in about 45 mins?\\n- Spicy dinner, 45 minutes prep time.\\n- I need a hot dinner meal, ready in under 45 minutes.\\n- What spicy dinners can I make in three-quarters of an hour?\\n\\n- I need a quick snack without fish or meat, ready in 10 minutes.\\n- What are some fast, meat-free snacks I can make in 10 mins?\\n- Vegetarian snack, 10 min.\\n- Give me a ten-minute snack without any animal protein.\\n- Quick 10-minute snack, no fish or meat.\\n\\n- I'm looking for a meaty dessert recipe with a long prep time.\\n- Can you suggest an elaborate meaty dessert?\\n- Meaty dessert, takes a long time to prepare.\\n- What lengthy meaty dessert recipes do you have?\\n- I have all day for a meaty dessert.\\n\\n- I need a quick paleo brunch.\\n- What are some fast paleo brunch ideas?\\n- Quick and easy paleo brunch, please.\\n- Paleo brunch, super fast.\\n- Suggest a quick paleo mid-morning meal.\\n\\n- Quick low-salt appetizer in 15 minutes.\\n- I need a low-sodium app that's ready in about a quarter-hour.\\n- What appetizers are low in salt and can be made fast, like 15 minutes?\\n- Give me a 15-minute, low-salt starter.\\n- Fast low-salt appetizer.\\n\\n- Can you find a diabetic-friendly side dish I can make in 20 minutes?\\n- I need a side that's good for diabetics and quick, under 20 mins.\\n- Diabetic-friendly side, ready in 20.\\n- What 20-minute, diabetic-friendly side dishes do you have?\\n- Quick diabetic-friendly side.\\n\\n- Give me a light, high-fiber breakfast that takes less than 20 minutes.\\n- I need a quick, easy, high-fiber breakfast, under 20 mins.\\n- High-fiber, light breakfast, fast (under 20 minutes).\\n- What can I make for a quick and light high-fiber breakfast in under 20 mins?\\n- Fast high-fiber lite breakfast.\\n\\n- I'm planning a lactose-free holiday meal, that requires overnight prep.\\n- Suggest an elaborate lactose-free holiday recipe that needs overnight preparation.\\n- Lactose-free holiday meal, overnight prep.\\n- What complex lactose-free holiday dishes need to be prepared overnight?\\n- I need an overnight lactose-free holiday feast.\\n\\n- I need some comfort food for a party that takes about an hour to prepare.\\n- What party snacks are comfort food and take around 60 minutes?\\n- Comfort food party food, 1 hour prep.\\n- Show me party food recipes that are comforting and ready in about an hour.\\n- About an hour for comfort food party treats.\\n\\n- I need an instant quick bite with hummus and carrots.\\n- What are some quick bites with hummus and carrots I can make instantly?\\n- Hummus and carrots quick bite, super fast.\\n- Instant snack with hummus and carrots.\\n- Give me a quick bite, hummus and carrots.\\n\\n- I want a quick and easy gluten-free breakfast.\\n- Give me a fast GF breakfast recipe.\\n- Gluten-free breakfast, quick and easy.\\n- What's an easy gluten-free breakfast I can make quickly?\\n- I need a simple, speedy gluten-free breakfast.\\n\\n- I'm looking for a vegan lunch recipe that takes about 30 minutes.\\n- Can you suggest a quick vegan lunch, around half an hour?\\n- Vegan lunch, 30 min.\\n- What's a good vegan lunch to make in 30 minutes or less?\\n- I need a plant-based lunch recipe, quick, about 30 mins.\\n\\n- Show me some high-protein dinner recipes ready in under an hour.\\n- I want a dinner with high protein that can be cooked in less than 60 minutes.\\n- High-protein dinner ideas, quick, under an hour.\\n- What can I make for dinner tonight that's high in protein and doesn't take too long, say, less than an hour?\\n- Give me a quick high-protein dinner recipe, under 60 mins.\\n\\n- I need a quick snack that is not spicy and ready in 10 minutes.\\n- What are some fast, mild snacks I can make in 10 mins?\\n- Non-spicy snack, 10 min.\\n- Give me a ten-minute snack that isn't hot.\\n- Quick 10-minute snack, mild to taste.\\n\\n- I'm looking for a vegetarian dessert recipe that takes around 45 minutes.\\n- Can you find a dessert that's vegetarian and ready in about 45 mins?\\n- Vegetarian dessert, 45 minutes prep time.\\n- I need a sweet treat, vegetarian, ready in under 45 minutes.\\n- What vegetarian desserts can I make in three-quarters of an hour?\\n\\n- I want an elaborate meaty brunch recipe. Time is not an issue.\\n- Give me a complex meaty brunch recipe, I have plenty of time.\\n- Meaty brunch, no time limit.\\n- Suggest a meaty brunch recipe that takes a long time to prepare.\\n- I'm retired and want to make a big meaty brunch.\\n\\n- Quick dairy-free appetizer in 15 minutes.\\n- I need a dairy-free app that's ready in about a quarter-hour.\\n- What appetizers are dairy-free and can be made fast, like 15 minutes?\\n- Give me a 15-minute, dairy-free starter.\\n- Fast dairy-free appetizer.\\n\\n- Can you find a low-carb side dish I can make in 20 minutes?\\n- I need a side that's low in carbs and quick, under 20 mins.\\n- Low-carb side, ready in 20.\\n- What 20-minute, low-carb side dishes do you have?\\n- Quick low-carb side.\\n\\n- Give me a light, pescatarian breakfast that takes less than 20 minutes.\\n- I need a quick, easy, pescatarian breakfast, under 20 mins.\\n- Pescatarian, light breakfast, fast (under 20 minutes).\\n- What can I make for a quick and light pescatarian breakfast in under 20 mins?\\n- Fast pescatarian lite breakfast.\\n\\n- I'm planning a kid-friendly holiday meal, I have all day to cook.\\n- Suggest an elaborate kid-friendly holiday recipe that takes all day.\\n- Kid-friendly holiday meal, long cooking time.\\n- What elaborate kid-friendly dishes can I make for a holiday?\\n- I have all day to prepare a kid-friendly holiday feast.\\n\\n- I need some nut-free party food that takes about an hour to prepare.\\n- What party snacks are nut-free and take around 60 minutes?\\n- Nut-free party food, 1 hour prep.\\n- Show me party food recipes without nuts, ready in about an hour.\\n- About an hour for nut-free party treats.\\n\\n- I need an instant sugar-free quick bite.\\n- What are some sugar-free snacks I can make instantly?\\n- Sugar-free quick bite, super fast.\\n- Instant sugar-free snack.\\n- Give me a sugar-free bite, right now.\\n\\n- I want a quick and easy spicy breakfast.\\n- Give me a fast hot breakfast recipe.\\n- Spicy breakfast, quick and easy.\\n- What's an easy spicy breakfast I can make quickly?\\n- I need a simple, speedy spicy breakfast.\\n\\n- I'm looking for a keto lunch recipe that takes about 30 minutes.\\n- Can you suggest a quick keto lunch, around half an hour?\\n- Keto lunch, 30 min.\\n- What's a good keto lunch to make in 30 minutes or less?\\n- I need a low-carb, high-fat lunch recipe, quick, about 30 mins.\\n\\n- Show me some heart-healthy dinner recipes ready in under an hour.\\n- I want a heart-healthy dinner that can be cooked in less than 60 minutes.\\n- Heart-healthy dinner ideas, quick, under an hour.\\n- What can I make for dinner tonight that's good for my heart and doesn't take too long, say, less than an hour?\\n- Give me a quick heart-healthy dinner recipe, under 60 mins.\\n\\n- I need a quick snack that is low-salt and ready in 10 minutes.\\n- What are some fast, low-sodium snacks I can make in 10 mins?\\n- Low-salt snack, 10 min.\\n- Give me a ten-minute snack that's low in salt.\\n- Quick 10-minute snack, low-sodium.\\n\\n- I'm looking for a diabetic-friendly dessert recipe that takes around 45 minutes.\\n- Can you find a dessert that's good for diabetics and ready in about 45 mins?\\n- Diabetic-friendly dessert, 45 minutes prep time.\\n- I need a sweet treat, diabetic-friendly, ready in under 45 minutes.\\n- What diabetic-friendly desserts can I make in three-quarters of an hour?\\n\\n- I want an elaborate high-fiber brunch recipe.\\n- Give me a complex high-fiber brunch recipe that takes a long time.\\n- High-fiber brunch, long prep time.\\n- Suggest a high-fiber brunch recipe that takes a lot of time to prepare.\\n- I have all day for a high-fiber brunch.\\n\\n- Quick lactose-free appetizer in 15 minutes.\\n- I need a lactose-free app that's ready in about a quarter-hour.\\n- What appetizers are lactose-free and can be made fast, like 15 minutes?\\n- Give me a 15-minute, lactose-free starter.\\n- Fast lactose-free appetizer.\\n\\n- Can you find a comfort food side dish I can make in 20 minutes?\\n- I need a side that's comforting and quick, under 20 mins.\\n- Comfort food side, ready in 20.\\n- What 20-minute, comfort food side dishes do you have?\\n- Quick comfort food side.\\n\\n- Give me a light, paleo breakfast that takes less than 20 minutes.\\n- I need a quick, easy, paleo breakfast, under 20 mins.\\n- Paleo, light breakfast, fast (under 20 minutes).\\n- What can I make for a quick and light paleo breakfast in under 20 mins?\\n- Fast paleo lite breakfast.\\n\\n- I'm planning a holiday meal with hummus and carrots, that requires overnight prep.\\n- Suggest an elaborate recipe for a holiday meal featuring hummus and carrots that needs overnight preparation.\\n- Holiday meal with hummus and carrots, overnight prep.\\n- What complex holiday dishes with hummus and carrots need to be prepared overnight?\\n- I need an overnight holiday feast including hummus and carrots.\\n\\n- I need some non-spicy party food that takes about an hour to prepare.\\n- What party snacks are mild and take around 60 minutes?\\n- Not spicy party food, 1 hour prep.\\n- Show me party food recipes that aren't spicy and ready in about an hour.\\n- About an hour for mild party treats.\\n\\n- I need an instant vegetarian quick bite.\\n- What are some vegetarian snacks I can make instantly?\\n- Vegetarian quick bite, super fast.\\n- Instant vegetarian snack.\\n- Give me a vegetarian bite, right now.\\n\\n- I'm looking for a gluten-free dinner recipe that takes around 10 minutes.\\n- Can you find a gluten-free dinner that's ready in about 10 mins?\\n- Gluten-free dinner, 10 minutes prep time.\\n- I need a GF dinner meal, ready in under 10 minutes.\\n- What gluten-free dinners can I make in ten minutes?\\n\\n- I'm looking for a quick meaty lunch.\\n- Can you suggest a fast meaty lunch recipe?\\n- Meaty lunch, quick.\\n- What's a good meaty lunch I can make quickly?\\n- I need a speedy lunch with meat.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 3749,\n",
      "    \"totalTokenCount\": 10753,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 5912\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"fhlgaJihHqSBvdIPyOL1wAg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:34:06 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:34:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:34:06 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:06 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:06 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:06 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0244801\n",
      "DEBUG:LiteLLM:response_cost: 0.0244801\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:34:06 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:34:06 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:34:06 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:34:06 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:34:06 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:34:06 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:34:06 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:34:06 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:34:06 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:34:06 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:34:06 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:34:06 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:34:06 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:06 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145702ba0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x14549d9d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1457035f0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:34:13 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=7162'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:34:13 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a meaty dinner recipe that takes less than an hour to make.\\n- What's a good meaty dinner I can prepare in under 60 minutes?\\n- Show me quick, meat-based dinner ideas, less than an hour.\\n- Meaty dinner, under 1 hour prep time, please.\\n- Something meaty for dinner tonight, needs to be done within an hour.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 83,\n",
      "    \"totalTokenCount\": 1882,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 707\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"hRlgaM2QM8-jkdUPpeypuAk\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a meaty dinner recipe that takes less than an hour to make.\\n- What's a good meaty dinner I can prepare in under 60 minutes?\\n- Show me quick, meat-based dinner ideas, less than an hour.\\n- Meaty dinner, under 1 hour prep time, please.\\n- Something meaty for dinner tonight, needs to be done within an hour.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 83,\n",
      "    \"totalTokenCount\": 1882,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 707\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"hRlgaM2QM8-jkdUPpeypuAk\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:34:13 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:34:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:34:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:13 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0023026\n",
      "DEBUG:LiteLLM:response_cost: 0.0023026\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:34:13 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:34:13 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:34:13 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:34:13 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:34:13 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:34:13 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:34:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:34:13 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:34:13 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:34:13 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:34:13 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:34:13 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:34:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:13 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1456d2ba0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x14562a6d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145702e10>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:34:19 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=5492'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:34:19 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in 30 minutes.\\n- What can I cook for a plant-based lunch in under half an hour?\\n- Need a quick vegetarian lunch, ready in about 30 minutes.\\n- Show me some meat-free lunch ideas that take around 30 minutes to prepare.\\n- Vegetarian lunch, 30-minute prep time.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 87,\n",
      "    \"totalTokenCount\": 1670,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 491\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"ixlgaNf0G-2kvdIP3-n8oAg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in 30 minutes.\\n- What can I cook for a plant-based lunch in under half an hour?\\n- Need a quick vegetarian lunch, ready in about 30 minutes.\\n- Show me some meat-free lunch ideas that take around 30 minutes to prepare.\\n- Vegetarian lunch, 30-minute prep time.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 87,\n",
      "    \"totalTokenCount\": 1670,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 491\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"ixlgaNf0G-2kvdIP3-n8oAg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:34:19 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:34:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:34:19 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:19 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:19 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:19 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0017726\n",
      "DEBUG:LiteLLM:response_cost: 0.0017726\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:34:19 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:34:19 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:34:19 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:34:19 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:34:19 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:34:19 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:34:19 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:34:19 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:34:19 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:34:19 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:34:19 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:34:19 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:34:19 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:19 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1362cbcb0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x14562a650> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1363c1100>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:34:23 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=4373'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:34:23 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in 30 minutes.\\n- Give me some quick vegetarian lunch ideas, around half an hour prep time.\\n- Vegetarian lunch, 30 minutes.\\n- What can I cook for a plant-based lunch that takes no more than 30 minutes?\\n- I need a vegetarian meal for lunch, ready in 30 mins.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 86,\n",
      "    \"totalTokenCount\": 1374,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 196\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"jxlgaLrGOqyixN8PzumMqQk\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in 30 minutes.\\n- Give me some quick vegetarian lunch ideas, around half an hour prep time.\\n- Vegetarian lunch, 30 minutes.\\n- What can I cook for a plant-based lunch that takes no more than 30 minutes?\\n- I need a vegetarian meal for lunch, ready in 30 mins.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 86,\n",
      "    \"totalTokenCount\": 1374,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 196\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"jxlgaLrGOqyixN8PzumMqQk\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:34:23 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:34:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:34:23 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:23 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:23 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:23 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0010326\n",
      "DEBUG:LiteLLM:response_cost: 0.0010326\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:34:23 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:34:23 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:34:23 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:34:23 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:34:23 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:34:23 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:34:23 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:34:23 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:34:23 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:34:23 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:34:23 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:34:23 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:34:23 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:23 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1456d1640>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x14562a8d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1456d3620>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:34:30 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=6092'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:34:30 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch that I can make in 30 minutes.\\n- Give me some ideas for a 30-minute vegetarian lunch.\\n- What's a quick vegetarian lunch recipe I can make in half an hour?\\n- I need a vegetarian lunch that takes around 30 minutes.\\n- Lunch, vegetarian, 30 minutes.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 79,\n",
      "    \"totalTokenCount\": 1418,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 247\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"lhlgaKLHCo7WvdIP7KT5sAg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch that I can make in 30 minutes.\\n- Give me some ideas for a 30-minute vegetarian lunch.\\n- What's a quick vegetarian lunch recipe I can make in half an hour?\\n- I need a vegetarian lunch that takes around 30 minutes.\\n- Lunch, vegetarian, 30 minutes.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 79,\n",
      "    \"totalTokenCount\": 1418,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 247\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"lhlgaKLHCo7WvdIP7KT5sAg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:34:30 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:34:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:34:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0011426000000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.0011426000000000001\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:34:30 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:34:30 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:34:30 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:34:30 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:34:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:34:30 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:34:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:34:30 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:34:30 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:34:30 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:34:30 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:34:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:34:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145702f60>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x14560b0d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1456d38c0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:34:32 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=1644'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:34:31 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in about 30 minutes.\\n- What can I cook for a quick vegetarian lunch in half an hour?\\n- Give me some lunch ideas for vegetarians, ready in 30 minutes.\\n- Need a vegetarian lunch that takes no more than 30 minutes to prepare.\\n- Recommend a 30-minute vegetarian lunch.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 84,\n",
      "    \"totalTokenCount\": 1261,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 85\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"mBlgaKbcBKnQvdIP7PyC8AU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in about 30 minutes.\\n- What can I cook for a quick vegetarian lunch in half an hour?\\n- Give me some lunch ideas for vegetarians, ready in 30 minutes.\\n- Need a vegetarian lunch that takes no more than 30 minutes to prepare.\\n- Recommend a 30-minute vegetarian lunch.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 84,\n",
      "    \"totalTokenCount\": 1261,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 85\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"mBlgaKbcBKnQvdIP7PyC8AU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:34:32 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:34:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:34:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:32 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0007501000000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.0007501000000000001\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:34:32 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:34:32 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:34:32 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:34:32 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:34:32 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:34:32 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:34:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:34:32 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:34:32 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:34:32 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:34:32 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:34:32 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:34:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:32 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1362523c0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1456839d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x135e0bc20>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:34:33 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=1381'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:34:33 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in 30 minutes.\\n- What can I cook for a quick vegetarian lunch in half an hour?\\n- Give me some lunch ideas, vegetarian, ready in 30 mins.\\n- Need a 30-minute vegetarian lunch.\\n- Suggest a meat-free lunch that takes about 30 minutes to prepare.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 83,\n",
      "    \"totalTokenCount\": 1268,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 93\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"mRlgaN6SJJqZvdIPi-nJsQg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in 30 minutes.\\n- What can I cook for a quick vegetarian lunch in half an hour?\\n- Give me some lunch ideas, vegetarian, ready in 30 mins.\\n- Need a 30-minute vegetarian lunch.\\n- Suggest a meat-free lunch that takes about 30 minutes to prepare.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 83,\n",
      "    \"totalTokenCount\": 1268,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 93\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"mRlgaN6SJJqZvdIPi-nJsQg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:34:33 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:34:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:34:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0007676\n",
      "DEBUG:LiteLLM:response_cost: 0.0007676\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:34:33 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:34:33 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:34:33 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:34:33 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:34:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:34:33 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:34:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:34:33 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:34:33 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:34:33 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:34:33 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:34:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:34:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1456d00e0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x14549fa50> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1456d05c0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:34:55 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=22010'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:34:55 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a vegetarian lunch recipe that takes about 30 minutes.\\n- What can I make for a vegetarian lunch in half an hour?\\n- Quick vegetarian lunch ideas for a 30-minute prep time.\\n- Show me some 30-minute vegetarian lunch options.\\n- I'm looking for a vegetarian lunch that's ready in 30 minutes.\\n\\n- Recommend a meaty dinner recipe that takes under an hour.\\n- I want a dinner with meat, quick, ready in less than an hour.\\n- What's a good meaty dinner that can be made in under 60 minutes?\\n- Meaty dinner ideas, less than one hour prep.\\n- Dinner: something meaty, less than an hour.\\n\\n- Give me a quick, gluten-free snack recipe.\\n- I need a fast gluten-free snack.\\n- What are some quick gluten-free snacks?\\n- Gluten-free snack, instant prep.\\n- Quick snack, no gluten.\\n\\n- I'd like a dairy-free dessert that takes around 45 minutes to make.\\n- Can you suggest a dairy-free dessert for 45 minutes of cooking time?\\n- Dairy-free dessert, about three-quarters of an hour.\\n- Recipes for a 45-minute dairy-free dessert.\\n- I want a dessert without dairy, ready in 45 minutes.\\n\\n- I need a vegan brunch recipe for when I have plenty of time.\\n- Suggest a leisurely vegan brunch meal.\\n- Vegan brunch, no time constraints.\\n- What can I cook for a vegan brunch when time isn't an issue?\\n- A vegan brunch recipe where prep time doesn't matter.\\n\\n- I'm looking for a low-carb appetizer that takes 15 minutes.\\n- Quick low-carb appetizer, ready in a quarter-hour.\\n- What are some 15-minute low-carb appetizers?\\n- Low-carb appetizer, needs to be done in 15 minutes.\\n- Give me a fast, low-carb starter.\\n\\n- Show me a high-protein side dish that takes 20 minutes to prepare.\\n- I need a side dish with high protein, done in 20 minutes.\\n- What's a good 20-minute high-protein side?\\n- High-protein side dish, quick, 20 minutes max.\\n- Give me high-protein side recipes ready in 20 mins.\\n\\n- I need a kid-friendly lite breakfast recipe in less than 20 minutes.\\n- What are some quick, kid-friendly light breakfast ideas under 20 minutes?\\n- Lite breakfast for kids, under 20 mins.\\n- Give me easy, quick breakfast recipes for children, less than 20 minutes.\\n- Kid-friendly lite breakfast, fast, under 20 min.\\n\\n- I'm planning a holiday meal, pescatarian, and I have all day to cook.\\n- Suggest an all-day pescatarian holiday meal.\\n- Holiday meal ideas for pescatarians, no time limit.\\n- What can I make for a pescatarian holiday meal that takes all day?\\n- Pescatarian holiday feast, I have endless time.\\n\\n- I need party food that's sugar-free and takes about an hour.\\n- What are some sugar-free party food ideas ready in about an hour?\\n- Party food, no sugar, around 60 minutes prep.\\n- Sugar-free party snacks, roughly an hour to make.\\n- Give me sugar-free party recipes that take about an hour.\\n\\n- I want a quick bite that's nut-free and instant.\\n- Show me instant, nut-free quick bite recipes.\\n- Nut-free quick bite, super fast.\\n- What's a good instant nut-free snack?\\n- Give me ideas for a nut-free quick bite that's ready immediately.\\n\\n- I need a keto breakfast recipe that takes 30 minutes.\\n- What's a good keto breakfast I can make in half an hour?\\n- Keto breakfast ideas for 30 minutes.\\n- 30-minute keto breakfast.\\n- Give me quick keto breakfast recipes for 30 minutes.\\n\\n- I'm looking for a heart-healthy lunch that takes under an hour.\\n- Recommend a heart-healthy lunch I can make in less than 60 minutes.\\n- Heart-healthy lunch ideas, under an hour prep.\\n- Lunch recipes that are heart-healthy and quick, less than an hour.\\n- What's a quick heart-healthy lunch under an hour?\\n\\n- I want a spicy dinner that takes 45 minutes to prepare.\\n- Give me a spicy dinner recipe for 45 minutes.\\n- What can I make for a spicy dinner in 45 minutes?\\n- Spicy dinner ideas, ready in 45 minutes.\\n- Dinner, hot and spicy, 45-minute cook time.\\n\\n- I need a snack without fish or meat that takes 10 minutes.\\n- Give me a quick 10-minute snack, no fish or meat.\\n- Snack ideas without fish or meat, ready in 10 minutes.\\n- What's a fast non-meat/non-fish snack for 10 minutes?\\n- 10-minute snack, vegetarian/vegan options.\\n\\n- I'm looking for a meaty dessert that requires long prep.\\n- Suggest a meaty dessert that takes a long time to prepare.\\n- Long prep meaty dessert ideas.\\n- What are some meaty desserts with extensive cooking times?\\n- Give me a meaty dessert recipe for when I have lots of time.\\n\\n- I need a quick paleo brunch.\\n- What are some fast paleo brunch ideas?\\n- Paleo brunch, quick and easy.\\n- Give me a speedy paleo brunch recipe.\\n- Brunch recipes for paleo, fast.\\n\\n- Show me a low-salt appetizer that takes 15 minutes.\\n- I need a quick low-salt appetizer, ready in 15 minutes.\\n- What's a good 15-minute low-salt starter?\\n- Low-salt appetizer, fast, done in a quarter-hour.\\n- Give me low-sodium appetizer recipes for 15 minutes.\\n\\n- I need a diabetic-friendly side dish that takes 20 minutes.\\n- What are some 20-minute diabetic-friendly side dish recipes?\\n- Diabetic-friendly side, ready in 20 minutes.\\n- Quick side dish for diabetics, 20 minutes.\\n- Give me a fast diabetic-friendly side.\\n\\n- I'm looking for a high-fiber lite breakfast in less than 20 minutes.\\n- Recommend a quick high-fiber light breakfast under 20 minutes.\\n- High-fiber lite breakfast, less than 20 mins.\\n- What's a fast high-fiber breakfast for under 20 minutes?\\n- Lite breakfast with high fiber, quick, under 20 min.\\n\\n- I need a lactose-free holiday meal that can be prepared overnight.\\n- Suggest an overnight lactose-free holiday meal.\\n- Holiday meal ideas, lactose-free, needs overnight prep.\\n- What can I cook for a lactose-free holiday meal overnight?\\n- Lactose-free holiday feast with overnight preparation.\\n\\n- I want comfort food for a party that takes about an hour.\\n- Party food comfort recipes for about an hour.\\n- What comfort food can I make for a party in roughly an hour?\\n- Comfort food party dishes, ready in about 60 minutes.\\n- Give me party comfort food ideas that take around an hour.\\n\\n- Show me an instant quick bite with hummus and carrots.\\n- I need a super fast snack with hummus and carrots.\\n- Quick bite, instant, featuring hummus and carrots.\\n- What's an instant quick bite involving hummus and carrots?\\n- Hummus and carrots, immediate snack.\\n\\n- I need a gluten-free breakfast that's quick and easy.\\n- Give me a fast and simple gluten-free breakfast recipe.\\n- Quick and easy gluten-free breakfast ideas.\\n- Gluten-free breakfast, minimal effort, fast.\\n- Fast, easy, gluten-free breakfast.\\n\\n- I want a vegan lunch that takes 30 minutes.\\n- Give me a 30-minute vegan lunch recipe.\\n- What can I make for a vegan lunch in half an hour?\\n- Vegan lunch ideas, quick, 30 minutes.\\n- I need a vegan lunch ready in 30 minutes.\\n\\n- Show me a high-protein dinner that takes under an hour.\\n- I need a quick high-protein dinner, less than 60 minutes.\\n- What's a good high-protein dinner I can make in under an hour?\\n- High-protein dinner ideas, under an hour prep.\\n- Dinner: high protein, fast, under an hour.\\n\\n- I need a snack that's not spicy and takes 10 minutes.\\n- Give me a quick, non-spicy snack for 10 minutes.\\n- What's a mild snack I can make in 10 minutes?\\n- 10-minute snack, not spicy.\\n- Fast and plain snack, 10 minutes.\\n\\n- I'm looking for a vegetarian dessert that takes 45 minutes.\\n- Suggest a vegetarian dessert recipe for 45 minutes.\\n- Vegetarian dessert ideas, ready in three-quarters of an hour.\\n- What vegetarian dessert can I make in 45 minutes?\\n- Dessert, vegetarian, 45-minute prep.\\n\\n- I want a meaty brunch where I have all the time until retirement.\\n- Recommend a leisurely meaty brunch recipe.\\n- Meaty brunch ideas, no time limit whatsoever.\\n- What can I cook for a meaty brunch when time is irrelevant?\\n- A meaty brunch recipe with unlimited prep time.\\n\\n- I need a dairy-free appetizer that takes 15 minutes.\\n- Give me a quick dairy-free appetizer for a quarter-hour.\\n- What's a good 15-minute dairy-free starter?\\n- Dairy-free appetizer, fast, done in 15 minutes.\\n- Show me dairy-free appetizer recipes for 15 minutes.\\n\\n- Show me a low-carb side dish that takes 20 minutes.\\n- I need a quick low-carb side dish, ready in 20 minutes.\\n- What's a good 20-minute low-carb side?\\n- Low-carb side dish, fast, 20 minutes max.\\n- Give me low-carb side recipes ready in 20 mins.\\n\\n- I need a pescatarian lite breakfast in less than 20 minutes.\\n- What are some quick pescatarian light breakfast ideas under 20 minutes?\\n- Lite breakfast for pescatarians, under 20 mins.\\n- Give me easy, quick breakfast recipes for pescatarians, less than 20 minutes.\\n- Pescatarian lite breakfast, fast, under 20 min.\\n\\n- I'm planning a kid-friendly holiday meal and I have all day.\\n- Suggest an all-day kid-friendly holiday meal.\\n- Holiday meal ideas for kids, no time limit.\\n- What can I cook for a kid-friendly holiday meal that takes all day?\\n- Kid-friendly holiday feast, I have endless time.\\n\\n- I need nut-free party food that takes about an hour.\\n- What are some nut-free party food ideas ready in about an hour?\\n- Party food, no nuts, around 60 minutes prep.\\n- Nut-free party snacks, roughly an hour to make.\\n- Give me nut-free party recipes that take about an hour.\\n\\n- I want a quick bite that's sugar-free and instant.\\n- Show me instant, sugar-free quick bite recipes.\\n- Sugar-free quick bite, super fast.\\n- What's a good instant sugar-free snack?\\n- Give me ideas for a sugar-free quick bite that's ready immediately.\\n\\n- I need a spicy breakfast that's quick and easy.\\n- Give me a fast and simple spicy breakfast recipe.\\n- Quick and easy spicy breakfast ideas.\\n- Spicy breakfast, minimal effort, fast.\\n- Fast, easy, spicy breakfast.\\n\\n- I want a keto lunch that takes 30 minutes.\\n- Give me a 30-minute keto lunch recipe.\\n- What can I make for a keto lunch in half an hour?\\n- Keto lunch ideas, quick, 30 minutes.\\n- I need a keto lunch ready in 30 minutes.\\n\\n- Show me a heart-healthy dinner that takes under an hour.\\n- I need a quick heart-healthy dinner, less than 60 minutes.\\n- What's a good heart-healthy dinner I can make in under an hour?\\n- Heart-healthy dinner ideas, under an hour prep.\\n- Dinner: heart-healthy, fast, under an hour.\\n\\n- I need a low-salt snack that takes 10 minutes.\\n- Give me a quick, low-salt snack for 10 minutes.\\n- What's a low-sodium snack I can make in 10 minutes?\\n- 10-minute snack, low-salt.\\n- Fast and low-salt snack, 10 minutes.\\n\\n- I'm looking for a diabetic-friendly dessert that takes 45 minutes.\\n- Suggest a diabetic-friendly dessert recipe for 45 minutes.\\n- Diabetic-friendly dessert ideas, ready in three-quarters of an hour.\\n- What diabetic-friendly dessert can I make in 45 minutes?\\n- Dessert, diabetic-friendly, 45-minute prep.\\n\\n- I need a high-fiber brunch that requires long prep.\\n- Suggest a high-fiber brunch that takes a long time to prepare.\\n- Long prep high-fiber brunch ideas.\\n- What are some high-fiber brunches with extensive cooking times?\\n- Give me a high-fiber brunch recipe for when I have lots of time.\\n\\n- I need a lactose-free appetizer that takes 15 minutes.\\n- Give me a quick lactose-free appetizer for a quarter-hour.\\n- What's a good 15-minute lactose-free starter?\\n- Lactose-free appetizer, fast, done in 15 minutes.\\n- Show me lactose-free appetizer recipes for 15 minutes.\\n\\n- Show me a comfort food side dish that takes 20 minutes.\\n- I need a quick comfort food side dish, ready in 20 minutes.\\n- What's a good 20-minute comfort food side?\\n- Comfort food side dish, fast, 20 minutes max.\\n- Give me comfort food side recipes ready in 20 mins.\\n\\n- I need a paleo lite breakfast in less than 20 minutes.\\n- What are some quick paleo light breakfast ideas under 20 minutes?\\n- Lite breakfast for paleo, under 20 mins.\\n- Give me easy, quick breakfast recipes for paleo, less than 20 minutes.\\n- Paleo lite breakfast, fast, under 20 min.\\n\\n- I'm planning a holiday meal with hummus and carrots, prepared overnight.\\n- Suggest an overnight holiday meal featuring hummus and carrots.\\n- Holiday meal ideas, with hummus and carrots, needs overnight prep.\\n- What can I cook for a holiday meal with hummus and carrots overnight?\\n- Hummus and carrots holiday feast with overnight preparation.\\n\\n- I need party food that's not spicy and takes about an hour.\\n- What are some non-spicy party food ideas ready in about an hour?\\n- Party food, not spicy, around 60 minutes prep.\\n- Mild party snacks, roughly an hour to make.\\n- Give me non-spicy party recipes that take about an hour.\\n\\n- I want a quick bite that's vegetarian and instant.\\n- Show me instant, vegetarian quick bite recipes.\\n- Vegetarian quick bite, super fast.\\n- What's a good instant vegetarian snack?\\n- Give me ideas for a vegetarian quick bite that's ready immediately.\\n\\n- I need a gluten-free dinner that takes 10 minutes.\\n- Give me a 10-minute gluten-free dinner recipe.\\n- What can I make for a gluten-free dinner in 10 minutes?\\n- Gluten-free dinner ideas, quick, 10 minutes.\\n- I need a gluten-free dinner ready in 10 minutes.\\n\\n- I want a meaty lunch that's something quick.\\n- Give me a fast meaty lunch recipe.\\n- What can I make for a quick meaty lunch?\\n- Meaty lunch ideas, fast.\\n- I need a quick meaty lunch.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 3623,\n",
      "    \"totalTokenCount\": 5098,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 383\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"rxlgaKKqKrbZvdIP5LbJoQY\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a vegetarian lunch recipe that takes about 30 minutes.\\n- What can I make for a vegetarian lunch in half an hour?\\n- Quick vegetarian lunch ideas for a 30-minute prep time.\\n- Show me some 30-minute vegetarian lunch options.\\n- I'm looking for a vegetarian lunch that's ready in 30 minutes.\\n\\n- Recommend a meaty dinner recipe that takes under an hour.\\n- I want a dinner with meat, quick, ready in less than an hour.\\n- What's a good meaty dinner that can be made in under 60 minutes?\\n- Meaty dinner ideas, less than one hour prep.\\n- Dinner: something meaty, less than an hour.\\n\\n- Give me a quick, gluten-free snack recipe.\\n- I need a fast gluten-free snack.\\n- What are some quick gluten-free snacks?\\n- Gluten-free snack, instant prep.\\n- Quick snack, no gluten.\\n\\n- I'd like a dairy-free dessert that takes around 45 minutes to make.\\n- Can you suggest a dairy-free dessert for 45 minutes of cooking time?\\n- Dairy-free dessert, about three-quarters of an hour.\\n- Recipes for a 45-minute dairy-free dessert.\\n- I want a dessert without dairy, ready in 45 minutes.\\n\\n- I need a vegan brunch recipe for when I have plenty of time.\\n- Suggest a leisurely vegan brunch meal.\\n- Vegan brunch, no time constraints.\\n- What can I cook for a vegan brunch when time isn't an issue?\\n- A vegan brunch recipe where prep time doesn't matter.\\n\\n- I'm looking for a low-carb appetizer that takes 15 minutes.\\n- Quick low-carb appetizer, ready in a quarter-hour.\\n- What are some 15-minute low-carb appetizers?\\n- Low-carb appetizer, needs to be done in 15 minutes.\\n- Give me a fast, low-carb starter.\\n\\n- Show me a high-protein side dish that takes 20 minutes to prepare.\\n- I need a side dish with high protein, done in 20 minutes.\\n- What's a good 20-minute high-protein side?\\n- High-protein side dish, quick, 20 minutes max.\\n- Give me high-protein side recipes ready in 20 mins.\\n\\n- I need a kid-friendly lite breakfast recipe in less than 20 minutes.\\n- What are some quick, kid-friendly light breakfast ideas under 20 minutes?\\n- Lite breakfast for kids, under 20 mins.\\n- Give me easy, quick breakfast recipes for children, less than 20 minutes.\\n- Kid-friendly lite breakfast, fast, under 20 min.\\n\\n- I'm planning a holiday meal, pescatarian, and I have all day to cook.\\n- Suggest an all-day pescatarian holiday meal.\\n- Holiday meal ideas for pescatarians, no time limit.\\n- What can I make for a pescatarian holiday meal that takes all day?\\n- Pescatarian holiday feast, I have endless time.\\n\\n- I need party food that's sugar-free and takes about an hour.\\n- What are some sugar-free party food ideas ready in about an hour?\\n- Party food, no sugar, around 60 minutes prep.\\n- Sugar-free party snacks, roughly an hour to make.\\n- Give me sugar-free party recipes that take about an hour.\\n\\n- I want a quick bite that's nut-free and instant.\\n- Show me instant, nut-free quick bite recipes.\\n- Nut-free quick bite, super fast.\\n- What's a good instant nut-free snack?\\n- Give me ideas for a nut-free quick bite that's ready immediately.\\n\\n- I need a keto breakfast recipe that takes 30 minutes.\\n- What's a good keto breakfast I can make in half an hour?\\n- Keto breakfast ideas for 30 minutes.\\n- 30-minute keto breakfast.\\n- Give me quick keto breakfast recipes for 30 minutes.\\n\\n- I'm looking for a heart-healthy lunch that takes under an hour.\\n- Recommend a heart-healthy lunch I can make in less than 60 minutes.\\n- Heart-healthy lunch ideas, under an hour prep.\\n- Lunch recipes that are heart-healthy and quick, less than an hour.\\n- What's a quick heart-healthy lunch under an hour?\\n\\n- I want a spicy dinner that takes 45 minutes to prepare.\\n- Give me a spicy dinner recipe for 45 minutes.\\n- What can I make for a spicy dinner in 45 minutes?\\n- Spicy dinner ideas, ready in 45 minutes.\\n- Dinner, hot and spicy, 45-minute cook time.\\n\\n- I need a snack without fish or meat that takes 10 minutes.\\n- Give me a quick 10-minute snack, no fish or meat.\\n- Snack ideas without fish or meat, ready in 10 minutes.\\n- What's a fast non-meat/non-fish snack for 10 minutes?\\n- 10-minute snack, vegetarian/vegan options.\\n\\n- I'm looking for a meaty dessert that requires long prep.\\n- Suggest a meaty dessert that takes a long time to prepare.\\n- Long prep meaty dessert ideas.\\n- What are some meaty desserts with extensive cooking times?\\n- Give me a meaty dessert recipe for when I have lots of time.\\n\\n- I need a quick paleo brunch.\\n- What are some fast paleo brunch ideas?\\n- Paleo brunch, quick and easy.\\n- Give me a speedy paleo brunch recipe.\\n- Brunch recipes for paleo, fast.\\n\\n- Show me a low-salt appetizer that takes 15 minutes.\\n- I need a quick low-salt appetizer, ready in 15 minutes.\\n- What's a good 15-minute low-salt starter?\\n- Low-salt appetizer, fast, done in a quarter-hour.\\n- Give me low-sodium appetizer recipes for 15 minutes.\\n\\n- I need a diabetic-friendly side dish that takes 20 minutes.\\n- What are some 20-minute diabetic-friendly side dish recipes?\\n- Diabetic-friendly side, ready in 20 minutes.\\n- Quick side dish for diabetics, 20 minutes.\\n- Give me a fast diabetic-friendly side.\\n\\n- I'm looking for a high-fiber lite breakfast in less than 20 minutes.\\n- Recommend a quick high-fiber light breakfast under 20 minutes.\\n- High-fiber lite breakfast, less than 20 mins.\\n- What's a fast high-fiber breakfast for under 20 minutes?\\n- Lite breakfast with high fiber, quick, under 20 min.\\n\\n- I need a lactose-free holiday meal that can be prepared overnight.\\n- Suggest an overnight lactose-free holiday meal.\\n- Holiday meal ideas, lactose-free, needs overnight prep.\\n- What can I cook for a lactose-free holiday meal overnight?\\n- Lactose-free holiday feast with overnight preparation.\\n\\n- I want comfort food for a party that takes about an hour.\\n- Party food comfort recipes for about an hour.\\n- What comfort food can I make for a party in roughly an hour?\\n- Comfort food party dishes, ready in about 60 minutes.\\n- Give me party comfort food ideas that take around an hour.\\n\\n- Show me an instant quick bite with hummus and carrots.\\n- I need a super fast snack with hummus and carrots.\\n- Quick bite, instant, featuring hummus and carrots.\\n- What's an instant quick bite involving hummus and carrots?\\n- Hummus and carrots, immediate snack.\\n\\n- I need a gluten-free breakfast that's quick and easy.\\n- Give me a fast and simple gluten-free breakfast recipe.\\n- Quick and easy gluten-free breakfast ideas.\\n- Gluten-free breakfast, minimal effort, fast.\\n- Fast, easy, gluten-free breakfast.\\n\\n- I want a vegan lunch that takes 30 minutes.\\n- Give me a 30-minute vegan lunch recipe.\\n- What can I make for a vegan lunch in half an hour?\\n- Vegan lunch ideas, quick, 30 minutes.\\n- I need a vegan lunch ready in 30 minutes.\\n\\n- Show me a high-protein dinner that takes under an hour.\\n- I need a quick high-protein dinner, less than 60 minutes.\\n- What's a good high-protein dinner I can make in under an hour?\\n- High-protein dinner ideas, under an hour prep.\\n- Dinner: high protein, fast, under an hour.\\n\\n- I need a snack that's not spicy and takes 10 minutes.\\n- Give me a quick, non-spicy snack for 10 minutes.\\n- What's a mild snack I can make in 10 minutes?\\n- 10-minute snack, not spicy.\\n- Fast and plain snack, 10 minutes.\\n\\n- I'm looking for a vegetarian dessert that takes 45 minutes.\\n- Suggest a vegetarian dessert recipe for 45 minutes.\\n- Vegetarian dessert ideas, ready in three-quarters of an hour.\\n- What vegetarian dessert can I make in 45 minutes?\\n- Dessert, vegetarian, 45-minute prep.\\n\\n- I want a meaty brunch where I have all the time until retirement.\\n- Recommend a leisurely meaty brunch recipe.\\n- Meaty brunch ideas, no time limit whatsoever.\\n- What can I cook for a meaty brunch when time is irrelevant?\\n- A meaty brunch recipe with unlimited prep time.\\n\\n- I need a dairy-free appetizer that takes 15 minutes.\\n- Give me a quick dairy-free appetizer for a quarter-hour.\\n- What's a good 15-minute dairy-free starter?\\n- Dairy-free appetizer, fast, done in 15 minutes.\\n- Show me dairy-free appetizer recipes for 15 minutes.\\n\\n- Show me a low-carb side dish that takes 20 minutes.\\n- I need a quick low-carb side dish, ready in 20 minutes.\\n- What's a good 20-minute low-carb side?\\n- Low-carb side dish, fast, 20 minutes max.\\n- Give me low-carb side recipes ready in 20 mins.\\n\\n- I need a pescatarian lite breakfast in less than 20 minutes.\\n- What are some quick pescatarian light breakfast ideas under 20 minutes?\\n- Lite breakfast for pescatarians, under 20 mins.\\n- Give me easy, quick breakfast recipes for pescatarians, less than 20 minutes.\\n- Pescatarian lite breakfast, fast, under 20 min.\\n\\n- I'm planning a kid-friendly holiday meal and I have all day.\\n- Suggest an all-day kid-friendly holiday meal.\\n- Holiday meal ideas for kids, no time limit.\\n- What can I cook for a kid-friendly holiday meal that takes all day?\\n- Kid-friendly holiday feast, I have endless time.\\n\\n- I need nut-free party food that takes about an hour.\\n- What are some nut-free party food ideas ready in about an hour?\\n- Party food, no nuts, around 60 minutes prep.\\n- Nut-free party snacks, roughly an hour to make.\\n- Give me nut-free party recipes that take about an hour.\\n\\n- I want a quick bite that's sugar-free and instant.\\n- Show me instant, sugar-free quick bite recipes.\\n- Sugar-free quick bite, super fast.\\n- What's a good instant sugar-free snack?\\n- Give me ideas for a sugar-free quick bite that's ready immediately.\\n\\n- I need a spicy breakfast that's quick and easy.\\n- Give me a fast and simple spicy breakfast recipe.\\n- Quick and easy spicy breakfast ideas.\\n- Spicy breakfast, minimal effort, fast.\\n- Fast, easy, spicy breakfast.\\n\\n- I want a keto lunch that takes 30 minutes.\\n- Give me a 30-minute keto lunch recipe.\\n- What can I make for a keto lunch in half an hour?\\n- Keto lunch ideas, quick, 30 minutes.\\n- I need a keto lunch ready in 30 minutes.\\n\\n- Show me a heart-healthy dinner that takes under an hour.\\n- I need a quick heart-healthy dinner, less than 60 minutes.\\n- What's a good heart-healthy dinner I can make in under an hour?\\n- Heart-healthy dinner ideas, under an hour prep.\\n- Dinner: heart-healthy, fast, under an hour.\\n\\n- I need a low-salt snack that takes 10 minutes.\\n- Give me a quick, low-salt snack for 10 minutes.\\n- What's a low-sodium snack I can make in 10 minutes?\\n- 10-minute snack, low-salt.\\n- Fast and low-salt snack, 10 minutes.\\n\\n- I'm looking for a diabetic-friendly dessert that takes 45 minutes.\\n- Suggest a diabetic-friendly dessert recipe for 45 minutes.\\n- Diabetic-friendly dessert ideas, ready in three-quarters of an hour.\\n- What diabetic-friendly dessert can I make in 45 minutes?\\n- Dessert, diabetic-friendly, 45-minute prep.\\n\\n- I need a high-fiber brunch that requires long prep.\\n- Suggest a high-fiber brunch that takes a long time to prepare.\\n- Long prep high-fiber brunch ideas.\\n- What are some high-fiber brunches with extensive cooking times?\\n- Give me a high-fiber brunch recipe for when I have lots of time.\\n\\n- I need a lactose-free appetizer that takes 15 minutes.\\n- Give me a quick lactose-free appetizer for a quarter-hour.\\n- What's a good 15-minute lactose-free starter?\\n- Lactose-free appetizer, fast, done in 15 minutes.\\n- Show me lactose-free appetizer recipes for 15 minutes.\\n\\n- Show me a comfort food side dish that takes 20 minutes.\\n- I need a quick comfort food side dish, ready in 20 minutes.\\n- What's a good 20-minute comfort food side?\\n- Comfort food side dish, fast, 20 minutes max.\\n- Give me comfort food side recipes ready in 20 mins.\\n\\n- I need a paleo lite breakfast in less than 20 minutes.\\n- What are some quick paleo light breakfast ideas under 20 minutes?\\n- Lite breakfast for paleo, under 20 mins.\\n- Give me easy, quick breakfast recipes for paleo, less than 20 minutes.\\n- Paleo lite breakfast, fast, under 20 min.\\n\\n- I'm planning a holiday meal with hummus and carrots, prepared overnight.\\n- Suggest an overnight holiday meal featuring hummus and carrots.\\n- Holiday meal ideas, with hummus and carrots, needs overnight prep.\\n- What can I cook for a holiday meal with hummus and carrots overnight?\\n- Hummus and carrots holiday feast with overnight preparation.\\n\\n- I need party food that's not spicy and takes about an hour.\\n- What are some non-spicy party food ideas ready in about an hour?\\n- Party food, not spicy, around 60 minutes prep.\\n- Mild party snacks, roughly an hour to make.\\n- Give me non-spicy party recipes that take about an hour.\\n\\n- I want a quick bite that's vegetarian and instant.\\n- Show me instant, vegetarian quick bite recipes.\\n- Vegetarian quick bite, super fast.\\n- What's a good instant vegetarian snack?\\n- Give me ideas for a vegetarian quick bite that's ready immediately.\\n\\n- I need a gluten-free dinner that takes 10 minutes.\\n- Give me a 10-minute gluten-free dinner recipe.\\n- What can I make for a gluten-free dinner in 10 minutes?\\n- Gluten-free dinner ideas, quick, 10 minutes.\\n- I need a gluten-free dinner ready in 10 minutes.\\n\\n- I want a meaty lunch that's something quick.\\n- Give me a fast meaty lunch recipe.\\n- What can I make for a quick meaty lunch?\\n- Meaty lunch ideas, fast.\\n- I need a quick meaty lunch.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 3623,\n",
      "    \"totalTokenCount\": 5098,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 383\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"rxlgaKKqKrbZvdIP5LbJoQY\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:34:55 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:34:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:34:55 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:55 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:55 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:55 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.010342600000000002\n",
      "DEBUG:LiteLLM:response_cost: 0.010342600000000002\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:34:55 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:34:55 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:34:55 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:34:55 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:34:55 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:34:55 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:34:55 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:34:55 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:34:55 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:34:55 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:34:55 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:34:55 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:34:55 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:55 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1456d21b0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1454e46d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1456d2ed0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:34:57 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=1650'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:34:57 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a quick meaty lunch.\\n- What's a fast recipe for a meaty lunch?\\n- Give me ideas for a speedy lunch that includes meat.\\n- Meaty lunch, but I'm short on time.\\n- I'm looking for something quick and meaty for lunch.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 62,\n",
      "    \"totalTokenCount\": 1313,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 159\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"sRlgaMKqHr3i7M8PoPTluAk\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a quick meaty lunch.\\n- What's a fast recipe for a meaty lunch?\\n- Give me ideas for a speedy lunch that includes meat.\\n- Meaty lunch, but I'm short on time.\\n- I'm looking for something quick and meaty for lunch.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 62,\n",
      "    \"totalTokenCount\": 1313,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 159\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"sRlgaMKqHr3i7M8PoPTluAk\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:34:57 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:34:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:34:57 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:57 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:57 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0008801\n",
      "DEBUG:LiteLLM:response_cost: 0.0008801\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:34:57 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:34:57 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:34:57 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:34:57 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:34:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:34:57 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:34:57 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:34:57 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:34:57 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:34:57 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:34:57 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:34:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:34:57 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x135e0bc20>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x14560abd0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1363c1100>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:35:02 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=4941'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:35:02 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for an elaborate vegan brunch recipe. Time is not an issue, I want to spend all day cooking!\\n- What are some gourmet vegan brunch ideas that take a lot of time to prepare?\\n- Suggest a complex, time-consuming vegan brunch. I'm retired and have nothing but time.\\n- Give me an extensive vegan brunch menu that requires a whole day of cooking.\\n- I want to make a special vegan brunch, something that's an all-day project. Any ideas?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 107,\n",
      "    \"totalTokenCount\": 1457,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 258\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"thlgaPiSIvO2xN8P2rKZ4AU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for an elaborate vegan brunch recipe. Time is not an issue, I want to spend all day cooking!\\n- What are some gourmet vegan brunch ideas that take a lot of time to prepare?\\n- Suggest a complex, time-consuming vegan brunch. I'm retired and have nothing but time.\\n- Give me an extensive vegan brunch menu that requires a whole day of cooking.\\n- I want to make a special vegan brunch, something that's an all-day project. Any ideas?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 107,\n",
      "    \"totalTokenCount\": 1457,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 258\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"thlgaPiSIvO2xN8P2rKZ4AU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:35:02 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:35:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:35:02 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:35:02 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:35:02 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:35:02 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0012401\n",
      "DEBUG:LiteLLM:response_cost: 0.0012401\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:35:02 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:35:02 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:35:02 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:35:02 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:35:02 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:35:02 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:35:02 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:35:02 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:35:02 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:35:02 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:35:02 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:35:02 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:35:02 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:35:02 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1456d2d80>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x14560aad0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1456d2600>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:35:48 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=45677'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:35:48 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- What can I cook for a vegetarian lunch in 30 minutes?\\n- I need a quick vegetarian lunch recipe, around half an hour.\\n- Suggest some vegetarian lunch ideas that take 30 minutes or less.\\n- Vegetarian lunch, 30-minute prep.\\n- Help me find a recipe for a 30-minute vegetarian lunch.\\n\\n- I'm looking for a meaty dinner recipe that takes less than an hour.\\n- What are some good meaty dinners I can make in under 60 minutes?\\n- Meaty dinner, quick to prepare, less than an hour.\\n- Suggest a hearty dinner that's ready in under 60 minutes.\\n- Dinner ideas, meaty, quick, under an hour.\\n\\n- Give me some ideas for a quick, gluten-free snack.\\n- I need a fast gluten-free snack.\\n- What are some quick and easy gluten-free snacks?\\n- Gluten-free snack, quick prep.\\n- Recipes for quick gluten-free snacks, please.\\n\\n- Can you suggest a dairy-free dessert that takes about 45 minutes to make?\\n- I need a dairy-free dessert recipe, ready in 45 minutes.\\n- Dessert, dairy-free, 45-minute cook time.\\n- Looking for a dairy-free dessert that's done in under an hour, say 45 mins.\\n- What's a good 45-minute dairy-free dessert?\\n\\n- I want to make an elaborate vegan brunch. Time is not an issue.\\n- Suggest some fancy vegan brunch recipes, I have plenty of time.\\n- Vegan brunch, no time limit.\\n- What are some amazing vegan brunch dishes I can make if I have unlimited time?\\n- Show me complex vegan brunch recipes.\\n\\n- What are some quick low-carb appetizers I can make in 15 minutes?\\n- I need a low-carb appetizer recipe that's ready in 15 mins.\\n- Fast low-carb appetizers, 15 minutes max.\\n- Suggest a quick low-carb starter.\\n- 15-minute low-carb appetizer ideas.\\n\\n- Can you find me a high-protein side dish recipe that takes 20 minutes?\\n- I'm looking for a quick, high-protein side. 20 mins.\\n- High-protein side dish, 20-minute prep.\\n- What are some good high-protein sides ready in under 20 minutes?\\n- Suggest a 20-minute high-protein side dish.\\n\\n- I need a light, kid-friendly breakfast that's ready in less than 20 minutes.\\n- What's a quick, kid-friendly light breakfast for under 20 minutes?\\n- Kid-friendly lite breakfast, less than 20 mins.\\n- Suggest some fast, light breakfast ideas for kids.\\n- Easy, light breakfast for children, under 20 mins.\\n\\n- I'm planning a pescatarian holiday meal and have all day to cook.\\n- Suggest elaborate pescatarian holiday recipes.\\n- Pescatarian holiday feast, full-day prep.\\n- What are some extensive pescatarian dishes for a holiday dinner?\\n- Holiday meal, pescatarian, takes all day.\\n\\n- I need some sugar-free party food recipes that take about an hour to make.\\n- What are good sugar-free party snacks ready in about an hour?\\n- Sugar-free party food, 60-minute prep.\\n- Suggest party dishes that are sugar-free and can be made in roughly an hour.\\n- Party food, sugar-free, approximately an hour.\\n\\n- Give me an instant, nut-free quick bite recipe.\\n- I need a nut-free quick bite that's ready instantly.\\n- What are some super fast nut-free snacks?\\n- Nut-free quick bite, instant prep.\\n- Suggest something nut-free for an instant snack.\\n\\n- What are some keto breakfast ideas that take 30 minutes to make?\\n- I need a quick keto breakfast recipe, around half an hour.\\n- Keto breakfast, 30-minute prep.\\n- Suggest a fast keto breakfast under 30 minutes.\\n- Show me 30-minute keto breakfast recipes.\\n\\n- I'm looking for a heart-healthy lunch recipe that takes less than an hour.\\n- What are some good heart-healthy lunches I can make in under 60 minutes?\\n- Heart-healthy lunch, quick to prepare, less than an hour.\\n- Suggest a heart-healthy lunch that's ready in under 60 minutes.\\n- Lunch ideas, heart-healthy, quick, under an hour.\\n\\n- Can you suggest a spicy dinner that takes about 45 minutes to make?\\n- I need a spicy dinner recipe, ready in 45 minutes.\\n- Dinner, spicy, 45-minute cook time.\\n- Looking for a spicy dinner that's done in under an hour, say 45 mins.\\n- What's a good 45-minute spicy dinner?\\n\\n- Give me a quick snack recipe without fish or meat that takes 10 minutes.\\n- I need a fast, plant-based snack, ready in 10 minutes.\\n- Snack, no fish or meat, 10-minute prep.\\n- What are some quick vegetarian snacks under 10 minutes?\\n- Suggest a 10-minute snack that's free from meat and fish.\\n\\n- I'm looking for a meaty dessert that requires a long preparation time.\\n- Suggest an elaborate meaty dessert.\\n- Meaty dessert, long prep.\\n- What are some complex meaty dessert recipes?\\n- Show me meaty dessert ideas with extended cooking times.\\n\\n- What can I make for a quick paleo brunch?\\n- I need a fast paleo brunch recipe.\\n- Suggest some quick paleo brunch ideas.\\n- Paleo brunch, quick prep.\\n- Help me find a recipe for a speedy paleo brunch.\\n\\n- What are some quick low-salt appetizers I can make in 15 minutes?\\n- I need a low-salt appetizer recipe that's ready in 15 mins.\\n- Fast low-salt appetizers, 15 minutes max.\\n- Suggest a quick low-salt starter.\\n- 15-minute low-salt appetizer ideas.\\n\\n- Can you find me a diabetic-friendly side dish recipe that takes 20 minutes?\\n- I'm looking for a quick, diabetic-friendly side. 20 mins.\\n- Diabetic-friendly side dish, 20-minute prep.\\n- What are some good diabetic-friendly sides ready in under 20 minutes?\\n- Suggest a 20-minute diabetic-friendly side dish.\\n\\n- I need a light, high-fiber breakfast that's ready in less than 20 minutes.\\n- What's a quick, high-fiber light breakfast for under 20 minutes?\\n- High-fiber lite breakfast, less than 20 mins.\\n- Suggest some fast, light breakfast ideas that are high in fiber.\\n- Easy, high-fiber light breakfast, under 20 mins.\\n\\n- I'm planning a lactose-free holiday meal that requires overnight preparation.\\n- Suggest elaborate lactose-free holiday recipes that can be made overnight.\\n- Lactose-free holiday feast, overnight prep.\\n- What are some extensive lactose-free dishes for a holiday dinner that need to sit overnight?\\n- Holiday meal, lactose-free, overnight.\\n\\n- I need some comfort food party recipes that take about an hour to make.\\n- What are good comfort food party snacks ready in about an hour?\\n- Comfort food party dishes, 60-minute prep.\\n- Suggest party dishes that are comfort food and can be made in roughly an hour.\\n- Party food, comfort food, approximately an hour.\\n\\n- Give me an instant quick bite recipe with hummus and carrots.\\n- I need an instant snack that uses hummus and carrots.\\n- What are some super fast snacks with hummus and carrots?\\n- Hummus and carrots quick bite, instant prep.\\n- Suggest something instant with hummus and carrots for a quick bite.\\n\\n- I want a quick and easy gluten-free breakfast.\\n- Give me a simple gluten-free breakfast recipe.\\n- Gluten-free breakfast, fast and easy.\\n- What are some quick and easy gluten-free breakfast ideas?\\n- Easy gluten-free breakfast.\\n\\n- What can I cook for a vegan lunch in 30 minutes?\\n- I need a quick vegan lunch recipe, around half an hour.\\n- Suggest some vegan lunch ideas that take 30 minutes or less.\\n- Vegan lunch, 30-minute prep.\\n- Help me find a recipe for a 30-minute vegan lunch.\\n\\n- I'm looking for a high-protein dinner recipe that takes less than an hour.\\n- What are some good high-protein dinners I can make in under 60 minutes?\\n- High-protein dinner, quick to prepare, less than an hour.\\n- Suggest a high-protein dinner that's ready in under 60 minutes.\\n- Dinner ideas, high-protein, quick, under an hour.\\n\\n- Give me a quick snack recipe that is not spicy and takes 10 minutes.\\n- I need a fast, non-spicy snack, ready in 10 minutes.\\n- Snack, mild, 10-minute prep.\\n- What are some quick non-spicy snacks under 10 minutes?\\n- Suggest a 10-minute snack that's mild to taste.\\n\\n- Can you suggest a vegetarian dessert that takes about 45 minutes to make?\\n- I need a vegetarian dessert recipe, ready in 45 minutes.\\n- Dessert, vegetarian, 45-minute cook time.\\n- Looking for a vegetarian dessert that's done in under an hour, say 45 mins.\\n- What's a good 45-minute vegetarian dessert?\\n\\n- I want to make an elaborate meaty brunch. Time is not an issue.\\n- Suggest some fancy meaty brunch recipes, I have plenty of time.\\n- Meaty brunch, no time limit.\\n- What are some amazing meaty brunch dishes I can make if I have unlimited time?\\n- Show me complex meaty brunch recipes.\\n\\n- What are some quick dairy-free appetizers I can make in 15 minutes?\\n- I need a dairy-free appetizer recipe that's ready in 15 mins.\\n- Fast dairy-free appetizers, 15 minutes max.\\n- Suggest a quick dairy-free starter.\\n- 15-minute dairy-free appetizer ideas.\\n\\n- Can you find me a low-carb side dish recipe that takes 20 minutes?\\n- I'm looking for a quick, low-carb side. 20 mins.\\n- Low-carb side dish, 20-minute prep.\\n- What are some good low-carb sides ready in under 20 minutes?\\n- Suggest a 20-minute low-carb side dish.\\n\\n- I need a light, pescatarian breakfast that's ready in less than 20 minutes.\\n- What's a quick, pescatarian light breakfast for under 20 minutes?\\n- Pescatarian lite breakfast, less than 20 mins.\\n- Suggest some fast, light breakfast ideas that are pescatarian.\\n- Easy, pescatarian light breakfast, under 20 mins.\\n\\n- I'm planning a kid-friendly holiday meal and have all day to cook.\\n- Suggest elaborate kid-friendly holiday recipes.\\n- Kid-friendly holiday feast, full-day prep.\\n- What are some extensive kid-friendly dishes for a holiday dinner?\\n- Holiday meal, kid-friendly, takes all day.\\n\\n- I need some nut-free party food recipes that take about an hour to make.\\n- What are good nut-free party snacks ready in about an hour?\\n- Nut-free party food, 60-minute prep.\\n- Suggest party dishes that are nut-free and can be made in roughly an hour.\\n- Party food, nut-free, approximately an hour.\\n\\n- Give me an instant, sugar-free quick bite recipe.\\n- I need a sugar-free quick bite that's ready instantly.\\n- What are some super fast sugar-free snacks?\\n- Sugar-free quick bite, instant prep.\\n- Suggest something sugar-free for an instant snack.\\n\\n- I want a quick and easy spicy breakfast.\\n- Give me a simple spicy breakfast recipe.\\n- Spicy breakfast, fast and easy.\\n- What are some quick and easy spicy breakfast ideas?\\n- Easy spicy breakfast.\\n\\n- What can I cook for a keto lunch in 30 minutes?\\n- I need a quick keto lunch recipe, around half an hour.\\n- Suggest some keto lunch ideas that take 30 minutes or less.\\n- Keto lunch, 30-minute prep.\\n- Help me find a recipe for a 30-minute keto lunch.\\n\\n- I'm looking for a heart-healthy dinner recipe that takes less than an hour.\\n- What are some good heart-healthy dinners I can make in under 60 minutes?\\n- Heart-healthy dinner, quick to prepare, less than an hour.\\n- Suggest a heart-healthy dinner that's ready in under 60 minutes.\\n- Dinner ideas, heart-healthy, quick, under an hour.\\n\\n- Give me a quick snack recipe that is low-salt and takes 10 minutes.\\n- I need a fast, low-salt snack, ready in 10 minutes.\\n- Snack, low-salt, 10-minute prep.\\n- What are some quick low-salt snacks under 10 minutes?\\n- Suggest a 10-minute low-salt snack.\\n\\n- Can you suggest a diabetic-friendly dessert that takes about 45 minutes to make?\\n- I need a diabetic-friendly dessert recipe, ready in 45 minutes.\\n- Dessert, diabetic-friendly, 45-minute cook time.\\n- Looking for a diabetic-friendly dessert that's done in under an hour, say 45 mins.\\n- What's a good 45-minute diabetic-friendly dessert?\\n\\n- I'm looking for a high-fiber brunch that requires a long preparation time.\\n- Suggest an elaborate high-fiber brunch.\\n- High-fiber brunch, long prep.\\n- What are some complex high-fiber brunch recipes?\\n- Show me high-fiber brunch ideas with extended cooking times.\\n\\n- What are some quick lactose-free appetizers I can make in 15 minutes?\\n- I need a lactose-free appetizer recipe that's ready in 15 mins.\\n- Fast lactose-free appetizers, 15 minutes max.\\n- Suggest a quick lactose-free starter.\\n- 15-minute lactose-free appetizer ideas.\\n\\n- Can you find me a comfort food side dish recipe that takes 20 minutes?\\n- I'm looking for a quick, comfort food side. 20 mins.\\n- Comfort food side dish, 20-minute prep.\\n- What are some good comfort food sides ready in under 20 minutes?\\n- Suggest a 20-minute comfort food side dish.\\n\\n- I need a light, paleo breakfast that's ready in less than 20 minutes.\\n- What's a quick, paleo light breakfast for under 20 minutes?\\n- Paleo lite breakfast, less than 20 mins.\\n- Suggest some fast, light breakfast ideas that are paleo.\\n- Easy, paleo light breakfast, under 20 mins.\\n\\n- I'm planning a holiday meal with hummus and carrots that requires overnight preparation.\\n- Suggest elaborate holiday recipes using hummus and carrots that can be made overnight.\\n- Holiday feast with hummus and carrots, overnight prep.\\n- What are some extensive dishes for a holiday dinner using hummus and carrots that need to sit overnight?\\n- Holiday meal, hummus and carrots, overnight.\\n\\n- I need some non-spicy party food recipes that take about an hour to make.\\n- What are good mild party snacks ready in about an hour?\\n- Non-spicy party food, 60-minute prep.\\n- Suggest party dishes that are not spicy and can be made in roughly an hour.\\n- Party food, not spicy, approximately an hour.\\n\\n- Give me an instant, vegetarian quick bite recipe.\\n- I need a vegetarian quick bite that's ready instantly.\\n- What are some super fast vegetarian snacks?\\n- Vegetarian quick bite, instant prep.\\n- Suggest something vegetarian for an instant snack.\\n\\n- I need a quick gluten-free dinner that takes only 10 minutes.\\n- What can I make for a gluten-free dinner in 10 minutes?\\n- Gluten-free dinner, 10-minute prep.\\n- Suggest a super fast gluten-free dinner.\\n- 10-minute gluten-free dinner recipe.\\n\\n- I need a quick meaty lunch recipe.\\n- What's a fast meaty lunch I can make?\\n- Meaty lunch, quick prep.\\n- Suggest some speedy meaty lunch ideas.\\n- Give me ideas for a quick meaty lunch.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 3725,\n",
      "    \"totalTokenCount\": 11810,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 6993\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"5BlgaIXrELmEvdIPgbOvsQg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- What can I cook for a vegetarian lunch in 30 minutes?\\n- I need a quick vegetarian lunch recipe, around half an hour.\\n- Suggest some vegetarian lunch ideas that take 30 minutes or less.\\n- Vegetarian lunch, 30-minute prep.\\n- Help me find a recipe for a 30-minute vegetarian lunch.\\n\\n- I'm looking for a meaty dinner recipe that takes less than an hour.\\n- What are some good meaty dinners I can make in under 60 minutes?\\n- Meaty dinner, quick to prepare, less than an hour.\\n- Suggest a hearty dinner that's ready in under 60 minutes.\\n- Dinner ideas, meaty, quick, under an hour.\\n\\n- Give me some ideas for a quick, gluten-free snack.\\n- I need a fast gluten-free snack.\\n- What are some quick and easy gluten-free snacks?\\n- Gluten-free snack, quick prep.\\n- Recipes for quick gluten-free snacks, please.\\n\\n- Can you suggest a dairy-free dessert that takes about 45 minutes to make?\\n- I need a dairy-free dessert recipe, ready in 45 minutes.\\n- Dessert, dairy-free, 45-minute cook time.\\n- Looking for a dairy-free dessert that's done in under an hour, say 45 mins.\\n- What's a good 45-minute dairy-free dessert?\\n\\n- I want to make an elaborate vegan brunch. Time is not an issue.\\n- Suggest some fancy vegan brunch recipes, I have plenty of time.\\n- Vegan brunch, no time limit.\\n- What are some amazing vegan brunch dishes I can make if I have unlimited time?\\n- Show me complex vegan brunch recipes.\\n\\n- What are some quick low-carb appetizers I can make in 15 minutes?\\n- I need a low-carb appetizer recipe that's ready in 15 mins.\\n- Fast low-carb appetizers, 15 minutes max.\\n- Suggest a quick low-carb starter.\\n- 15-minute low-carb appetizer ideas.\\n\\n- Can you find me a high-protein side dish recipe that takes 20 minutes?\\n- I'm looking for a quick, high-protein side. 20 mins.\\n- High-protein side dish, 20-minute prep.\\n- What are some good high-protein sides ready in under 20 minutes?\\n- Suggest a 20-minute high-protein side dish.\\n\\n- I need a light, kid-friendly breakfast that's ready in less than 20 minutes.\\n- What's a quick, kid-friendly light breakfast for under 20 minutes?\\n- Kid-friendly lite breakfast, less than 20 mins.\\n- Suggest some fast, light breakfast ideas for kids.\\n- Easy, light breakfast for children, under 20 mins.\\n\\n- I'm planning a pescatarian holiday meal and have all day to cook.\\n- Suggest elaborate pescatarian holiday recipes.\\n- Pescatarian holiday feast, full-day prep.\\n- What are some extensive pescatarian dishes for a holiday dinner?\\n- Holiday meal, pescatarian, takes all day.\\n\\n- I need some sugar-free party food recipes that take about an hour to make.\\n- What are good sugar-free party snacks ready in about an hour?\\n- Sugar-free party food, 60-minute prep.\\n- Suggest party dishes that are sugar-free and can be made in roughly an hour.\\n- Party food, sugar-free, approximately an hour.\\n\\n- Give me an instant, nut-free quick bite recipe.\\n- I need a nut-free quick bite that's ready instantly.\\n- What are some super fast nut-free snacks?\\n- Nut-free quick bite, instant prep.\\n- Suggest something nut-free for an instant snack.\\n\\n- What are some keto breakfast ideas that take 30 minutes to make?\\n- I need a quick keto breakfast recipe, around half an hour.\\n- Keto breakfast, 30-minute prep.\\n- Suggest a fast keto breakfast under 30 minutes.\\n- Show me 30-minute keto breakfast recipes.\\n\\n- I'm looking for a heart-healthy lunch recipe that takes less than an hour.\\n- What are some good heart-healthy lunches I can make in under 60 minutes?\\n- Heart-healthy lunch, quick to prepare, less than an hour.\\n- Suggest a heart-healthy lunch that's ready in under 60 minutes.\\n- Lunch ideas, heart-healthy, quick, under an hour.\\n\\n- Can you suggest a spicy dinner that takes about 45 minutes to make?\\n- I need a spicy dinner recipe, ready in 45 minutes.\\n- Dinner, spicy, 45-minute cook time.\\n- Looking for a spicy dinner that's done in under an hour, say 45 mins.\\n- What's a good 45-minute spicy dinner?\\n\\n- Give me a quick snack recipe without fish or meat that takes 10 minutes.\\n- I need a fast, plant-based snack, ready in 10 minutes.\\n- Snack, no fish or meat, 10-minute prep.\\n- What are some quick vegetarian snacks under 10 minutes?\\n- Suggest a 10-minute snack that's free from meat and fish.\\n\\n- I'm looking for a meaty dessert that requires a long preparation time.\\n- Suggest an elaborate meaty dessert.\\n- Meaty dessert, long prep.\\n- What are some complex meaty dessert recipes?\\n- Show me meaty dessert ideas with extended cooking times.\\n\\n- What can I make for a quick paleo brunch?\\n- I need a fast paleo brunch recipe.\\n- Suggest some quick paleo brunch ideas.\\n- Paleo brunch, quick prep.\\n- Help me find a recipe for a speedy paleo brunch.\\n\\n- What are some quick low-salt appetizers I can make in 15 minutes?\\n- I need a low-salt appetizer recipe that's ready in 15 mins.\\n- Fast low-salt appetizers, 15 minutes max.\\n- Suggest a quick low-salt starter.\\n- 15-minute low-salt appetizer ideas.\\n\\n- Can you find me a diabetic-friendly side dish recipe that takes 20 minutes?\\n- I'm looking for a quick, diabetic-friendly side. 20 mins.\\n- Diabetic-friendly side dish, 20-minute prep.\\n- What are some good diabetic-friendly sides ready in under 20 minutes?\\n- Suggest a 20-minute diabetic-friendly side dish.\\n\\n- I need a light, high-fiber breakfast that's ready in less than 20 minutes.\\n- What's a quick, high-fiber light breakfast for under 20 minutes?\\n- High-fiber lite breakfast, less than 20 mins.\\n- Suggest some fast, light breakfast ideas that are high in fiber.\\n- Easy, high-fiber light breakfast, under 20 mins.\\n\\n- I'm planning a lactose-free holiday meal that requires overnight preparation.\\n- Suggest elaborate lactose-free holiday recipes that can be made overnight.\\n- Lactose-free holiday feast, overnight prep.\\n- What are some extensive lactose-free dishes for a holiday dinner that need to sit overnight?\\n- Holiday meal, lactose-free, overnight.\\n\\n- I need some comfort food party recipes that take about an hour to make.\\n- What are good comfort food party snacks ready in about an hour?\\n- Comfort food party dishes, 60-minute prep.\\n- Suggest party dishes that are comfort food and can be made in roughly an hour.\\n- Party food, comfort food, approximately an hour.\\n\\n- Give me an instant quick bite recipe with hummus and carrots.\\n- I need an instant snack that uses hummus and carrots.\\n- What are some super fast snacks with hummus and carrots?\\n- Hummus and carrots quick bite, instant prep.\\n- Suggest something instant with hummus and carrots for a quick bite.\\n\\n- I want a quick and easy gluten-free breakfast.\\n- Give me a simple gluten-free breakfast recipe.\\n- Gluten-free breakfast, fast and easy.\\n- What are some quick and easy gluten-free breakfast ideas?\\n- Easy gluten-free breakfast.\\n\\n- What can I cook for a vegan lunch in 30 minutes?\\n- I need a quick vegan lunch recipe, around half an hour.\\n- Suggest some vegan lunch ideas that take 30 minutes or less.\\n- Vegan lunch, 30-minute prep.\\n- Help me find a recipe for a 30-minute vegan lunch.\\n\\n- I'm looking for a high-protein dinner recipe that takes less than an hour.\\n- What are some good high-protein dinners I can make in under 60 minutes?\\n- High-protein dinner, quick to prepare, less than an hour.\\n- Suggest a high-protein dinner that's ready in under 60 minutes.\\n- Dinner ideas, high-protein, quick, under an hour.\\n\\n- Give me a quick snack recipe that is not spicy and takes 10 minutes.\\n- I need a fast, non-spicy snack, ready in 10 minutes.\\n- Snack, mild, 10-minute prep.\\n- What are some quick non-spicy snacks under 10 minutes?\\n- Suggest a 10-minute snack that's mild to taste.\\n\\n- Can you suggest a vegetarian dessert that takes about 45 minutes to make?\\n- I need a vegetarian dessert recipe, ready in 45 minutes.\\n- Dessert, vegetarian, 45-minute cook time.\\n- Looking for a vegetarian dessert that's done in under an hour, say 45 mins.\\n- What's a good 45-minute vegetarian dessert?\\n\\n- I want to make an elaborate meaty brunch. Time is not an issue.\\n- Suggest some fancy meaty brunch recipes, I have plenty of time.\\n- Meaty brunch, no time limit.\\n- What are some amazing meaty brunch dishes I can make if I have unlimited time?\\n- Show me complex meaty brunch recipes.\\n\\n- What are some quick dairy-free appetizers I can make in 15 minutes?\\n- I need a dairy-free appetizer recipe that's ready in 15 mins.\\n- Fast dairy-free appetizers, 15 minutes max.\\n- Suggest a quick dairy-free starter.\\n- 15-minute dairy-free appetizer ideas.\\n\\n- Can you find me a low-carb side dish recipe that takes 20 minutes?\\n- I'm looking for a quick, low-carb side. 20 mins.\\n- Low-carb side dish, 20-minute prep.\\n- What are some good low-carb sides ready in under 20 minutes?\\n- Suggest a 20-minute low-carb side dish.\\n\\n- I need a light, pescatarian breakfast that's ready in less than 20 minutes.\\n- What's a quick, pescatarian light breakfast for under 20 minutes?\\n- Pescatarian lite breakfast, less than 20 mins.\\n- Suggest some fast, light breakfast ideas that are pescatarian.\\n- Easy, pescatarian light breakfast, under 20 mins.\\n\\n- I'm planning a kid-friendly holiday meal and have all day to cook.\\n- Suggest elaborate kid-friendly holiday recipes.\\n- Kid-friendly holiday feast, full-day prep.\\n- What are some extensive kid-friendly dishes for a holiday dinner?\\n- Holiday meal, kid-friendly, takes all day.\\n\\n- I need some nut-free party food recipes that take about an hour to make.\\n- What are good nut-free party snacks ready in about an hour?\\n- Nut-free party food, 60-minute prep.\\n- Suggest party dishes that are nut-free and can be made in roughly an hour.\\n- Party food, nut-free, approximately an hour.\\n\\n- Give me an instant, sugar-free quick bite recipe.\\n- I need a sugar-free quick bite that's ready instantly.\\n- What are some super fast sugar-free snacks?\\n- Sugar-free quick bite, instant prep.\\n- Suggest something sugar-free for an instant snack.\\n\\n- I want a quick and easy spicy breakfast.\\n- Give me a simple spicy breakfast recipe.\\n- Spicy breakfast, fast and easy.\\n- What are some quick and easy spicy breakfast ideas?\\n- Easy spicy breakfast.\\n\\n- What can I cook for a keto lunch in 30 minutes?\\n- I need a quick keto lunch recipe, around half an hour.\\n- Suggest some keto lunch ideas that take 30 minutes or less.\\n- Keto lunch, 30-minute prep.\\n- Help me find a recipe for a 30-minute keto lunch.\\n\\n- I'm looking for a heart-healthy dinner recipe that takes less than an hour.\\n- What are some good heart-healthy dinners I can make in under 60 minutes?\\n- Heart-healthy dinner, quick to prepare, less than an hour.\\n- Suggest a heart-healthy dinner that's ready in under 60 minutes.\\n- Dinner ideas, heart-healthy, quick, under an hour.\\n\\n- Give me a quick snack recipe that is low-salt and takes 10 minutes.\\n- I need a fast, low-salt snack, ready in 10 minutes.\\n- Snack, low-salt, 10-minute prep.\\n- What are some quick low-salt snacks under 10 minutes?\\n- Suggest a 10-minute low-salt snack.\\n\\n- Can you suggest a diabetic-friendly dessert that takes about 45 minutes to make?\\n- I need a diabetic-friendly dessert recipe, ready in 45 minutes.\\n- Dessert, diabetic-friendly, 45-minute cook time.\\n- Looking for a diabetic-friendly dessert that's done in under an hour, say 45 mins.\\n- What's a good 45-minute diabetic-friendly dessert?\\n\\n- I'm looking for a high-fiber brunch that requires a long preparation time.\\n- Suggest an elaborate high-fiber brunch.\\n- High-fiber brunch, long prep.\\n- What are some complex high-fiber brunch recipes?\\n- Show me high-fiber brunch ideas with extended cooking times.\\n\\n- What are some quick lactose-free appetizers I can make in 15 minutes?\\n- I need a lactose-free appetizer recipe that's ready in 15 mins.\\n- Fast lactose-free appetizers, 15 minutes max.\\n- Suggest a quick lactose-free starter.\\n- 15-minute lactose-free appetizer ideas.\\n\\n- Can you find me a comfort food side dish recipe that takes 20 minutes?\\n- I'm looking for a quick, comfort food side. 20 mins.\\n- Comfort food side dish, 20-minute prep.\\n- What are some good comfort food sides ready in under 20 minutes?\\n- Suggest a 20-minute comfort food side dish.\\n\\n- I need a light, paleo breakfast that's ready in less than 20 minutes.\\n- What's a quick, paleo light breakfast for under 20 minutes?\\n- Paleo lite breakfast, less than 20 mins.\\n- Suggest some fast, light breakfast ideas that are paleo.\\n- Easy, paleo light breakfast, under 20 mins.\\n\\n- I'm planning a holiday meal with hummus and carrots that requires overnight preparation.\\n- Suggest elaborate holiday recipes using hummus and carrots that can be made overnight.\\n- Holiday feast with hummus and carrots, overnight prep.\\n- What are some extensive dishes for a holiday dinner using hummus and carrots that need to sit overnight?\\n- Holiday meal, hummus and carrots, overnight.\\n\\n- I need some non-spicy party food recipes that take about an hour to make.\\n- What are good mild party snacks ready in about an hour?\\n- Non-spicy party food, 60-minute prep.\\n- Suggest party dishes that are not spicy and can be made in roughly an hour.\\n- Party food, not spicy, approximately an hour.\\n\\n- Give me an instant, vegetarian quick bite recipe.\\n- I need a vegetarian quick bite that's ready instantly.\\n- What are some super fast vegetarian snacks?\\n- Vegetarian quick bite, instant prep.\\n- Suggest something vegetarian for an instant snack.\\n\\n- I need a quick gluten-free dinner that takes only 10 minutes.\\n- What can I make for a gluten-free dinner in 10 minutes?\\n- Gluten-free dinner, 10-minute prep.\\n- Suggest a super fast gluten-free dinner.\\n- 10-minute gluten-free dinner recipe.\\n\\n- I need a quick meaty lunch recipe.\\n- What's a fast meaty lunch I can make?\\n- Meaty lunch, quick prep.\\n- Suggest some speedy meaty lunch ideas.\\n- Give me ideas for a quick meaty lunch.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 3725,\n",
      "    \"totalTokenCount\": 11810,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 6993\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"5BlgaIXrELmEvdIPgbOvsQg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:35:48 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:35:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:35:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:35:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:35:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:35:48 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.027122600000000004\n",
      "DEBUG:LiteLLM:response_cost: 0.027122600000000004\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:35:48 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:35:48 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:35:48 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:35:48 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:35:48 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:35:48 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:35:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:35:48 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:35:48 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:35:48 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:35:48 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:35:48 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:35:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:35:48 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1456d33e0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x145653250> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1456d27b0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:35:52 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=4382'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:35:52 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that takes about 30 minutes.\\n- Can you suggest a quick vegetarian lunch for 30 minutes?\\n- I need a plant-based lunch idea that I can make in under half an hour.\\n- Show me some vegetarian lunch recipes that can be done in 30 minutes.\\n- What can I cook for a vegetarian lunch that's ready in 30 minutes?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 91,\n",
      "    \"totalTokenCount\": 1472,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 289\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"6BlgaOHTObK1nsEPmICxaA\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that takes about 30 minutes.\\n- Can you suggest a quick vegetarian lunch for 30 minutes?\\n- I need a plant-based lunch idea that I can make in under half an hour.\\n- Show me some vegetarian lunch recipes that can be done in 30 minutes.\\n- What can I cook for a vegetarian lunch that's ready in 30 minutes?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 91,\n",
      "    \"totalTokenCount\": 1472,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 289\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"6BlgaOHTObK1nsEPmICxaA\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:35:52 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:35:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:35:52 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:35:52 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:35:52 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:35:52 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0012776\n",
      "DEBUG:LiteLLM:response_cost: 0.0012776\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:35:52 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:35:52 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:35:52 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:35:52 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:35:52 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:35:52 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:35:52 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:35:52 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:35:52 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:35:52 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:35:52 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:35:52 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:35:52 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:35:52 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145702210>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1456c6050> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x13602be00>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:36:01 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=8220'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:36:01 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that takes about 30 minutes.\\n- Find me a quick vegetarian lunch for 30 minutes.\\n- Vegetarian lunch, 30 minutes prep time.\\n- Need a 30-minute vegetarian lunch idea.\\n- What can I make for lunch that's vegetarian and done in half an hour?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 76,\n",
      "    \"totalTokenCount\": 1446,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 278\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"8RlgaOX-EYjPnsEP-si8wQk\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that takes about 30 minutes.\\n- Find me a quick vegetarian lunch for 30 minutes.\\n- Vegetarian lunch, 30 minutes prep time.\\n- Need a 30-minute vegetarian lunch idea.\\n- What can I make for lunch that's vegetarian and done in half an hour?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 76,\n",
      "    \"totalTokenCount\": 1446,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 278\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"8RlgaOX-EYjPnsEP-si8wQk\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:36:01 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:36:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:36:01 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:01 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:01 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0012126\n",
      "DEBUG:LiteLLM:response_cost: 0.0012126\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:36:01 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:36:01 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:36:01 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:36:01 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:36:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:36:01 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:36:01 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:36:01 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:36:01 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:36:01 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:36:01 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:36:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:36:01 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1456d1dc0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x145651850> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145702210>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:36:05 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=3690'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:36:05 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a quick meaty lunch idea.\\n- What's a fast lunch recipe with meat?\\n- Suggest some quick meaty lunches.\\n- Looking for a speedy, meat-based lunch.\\n- Can you give me a quick recipe for lunch, something meaty?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 56,\n",
      "    \"totalTokenCount\": 1329,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 181\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"9RlgaL7KCZW6xN8Pg7C98QU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a quick meaty lunch idea.\\n- What's a fast lunch recipe with meat?\\n- Suggest some quick meaty lunches.\\n- Looking for a speedy, meat-based lunch.\\n- Can you give me a quick recipe for lunch, something meaty?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 56,\n",
      "    \"totalTokenCount\": 1329,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 181\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"9RlgaL7KCZW6xN8Pg7C98QU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:36:05 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:36:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:36:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:05 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0009201000000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.0009201000000000001\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:36:05 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:36:05 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:36:05 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:36:05 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:36:05 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:36:05 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:36:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:36:05 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:36:05 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:36:05 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:36:05 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:36:05 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:36:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:05 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145703740>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x14542b2d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1456d0a40>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:36:08 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=3489'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:36:08 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for vegetarian lunch ideas that take around 30 minutes to make.\\n- Can you suggest a quick vegetarian lunch recipe ready in half an hour?\\n- What's a good vegetarian lunch that I can cook in 30 minutes?\\n- I need a 30-minute vegetarian lunch recipe, please.\\n- Give me some ideas for a quick, plant-based lunch ready in about 30 minutes.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 92,\n",
      "    \"totalTokenCount\": 1347,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 163\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"-BlgaKDyM7mEvdIPgbOvsQg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for vegetarian lunch ideas that take around 30 minutes to make.\\n- Can you suggest a quick vegetarian lunch recipe ready in half an hour?\\n- What's a good vegetarian lunch that I can cook in 30 minutes?\\n- I need a 30-minute vegetarian lunch recipe, please.\\n- Give me some ideas for a quick, plant-based lunch ready in about 30 minutes.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 92,\n",
      "    \"totalTokenCount\": 1347,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 163\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"-BlgaKDyM7mEvdIPgbOvsQg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:36:08 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:36:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:36:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:08 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0009651\n",
      "DEBUG:LiteLLM:response_cost: 0.0009651\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:36:08 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:36:08 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:36:08 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:36:08 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:36:08 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:36:08 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:36:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:36:08 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:36:08 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:36:08 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:36:08 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:36:08 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:36:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:08 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145703740>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1457b4c50> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x13602be00>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:36:11 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=2090'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:36:11 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch that I can make in 30 minutes.\\n- Suggest a quick vegetarian lunch recipe, please, something I can prepare in half an hour.\\n- What's a good vegetarian lunch that takes around 30 minutes to cook?\\n- I need a 30-minute vegetarian lunch.\\n- Give me ideas for a vegetarian lunch, done in 30 mins.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 88,\n",
      "    \"totalTokenCount\": 1417,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 237\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"-xlgaL_pBNiExs0PlsnR4QU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch that I can make in 30 minutes.\\n- Suggest a quick vegetarian lunch recipe, please, something I can prepare in half an hour.\\n- What's a good vegetarian lunch that takes around 30 minutes to cook?\\n- I need a 30-minute vegetarian lunch.\\n- Give me ideas for a vegetarian lunch, done in 30 mins.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 88,\n",
      "    \"totalTokenCount\": 1417,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 237\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"-xlgaL_pBNiExs0PlsnR4QU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:36:11 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:36:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:36:11 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:11 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:11 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:11 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0011401\n",
      "DEBUG:LiteLLM:response_cost: 0.0011401\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:36:11 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:36:11 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:36:11 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:36:11 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:36:11 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:36:11 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:36:11 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:36:11 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:36:11 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:36:11 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:36:11 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:36:11 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:36:11 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:11 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1457000b0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1456501d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145700b30>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:36:15 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=4528'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:36:15 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that takes about 30 minutes to make.\\n- What can I cook for lunch that's vegetarian and ready in half an hour?\\n- Need a 30-minute vegetarian lunch.\\n- Show me quick vegetarian lunch ideas, 30 min max.\\n- Vegetarian lunch, please, under 30 minutes.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 79,\n",
      "    \"totalTokenCount\": 1397,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 226\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"_xlgaIClMvaIvdIPv8_KmQg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that takes about 30 minutes to make.\\n- What can I cook for lunch that's vegetarian and ready in half an hour?\\n- Need a 30-minute vegetarian lunch.\\n- Show me quick vegetarian lunch ideas, 30 min max.\\n- Vegetarian lunch, please, under 30 minutes.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 79,\n",
      "    \"totalTokenCount\": 1397,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 226\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"_xlgaIClMvaIvdIPv8_KmQg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:36:15 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:36:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:36:15 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:15 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:15 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:15 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0010901\n",
      "DEBUG:LiteLLM:response_cost: 0.0010901\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:36:15 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:36:15 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:36:15 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:36:15 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:36:15 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:36:15 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:36:15 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:36:15 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:36:15 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:36:15 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:36:15 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:36:15 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:36:15 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:15 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1454791f0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1457b54d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145702de0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:36:21 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=5997'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:36:21 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a vegetarian lunch that can be prepared in 30 minutes.\\n- What are some quick vegetarian lunch recipes I can make in half an hour?\\n- Give me ideas for a 30-minute vegetarian lunch.\\n- I'm looking for a vegetarian lunch that takes no more than 30 minutes.\\n- Fast vegetarian lunch, 30 minutes.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 79,\n",
      "    \"totalTokenCount\": 1362,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 191\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"BRpgaJL6Off3xN8Pm4HL2Ag\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a vegetarian lunch that can be prepared in 30 minutes.\\n- What are some quick vegetarian lunch recipes I can make in half an hour?\\n- Give me ideas for a 30-minute vegetarian lunch.\\n- I'm looking for a vegetarian lunch that takes no more than 30 minutes.\\n- Fast vegetarian lunch, 30 minutes.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 79,\n",
      "    \"totalTokenCount\": 1362,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 191\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"BRpgaJL6Off3xN8Pm4HL2Ag\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:36:21 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:36:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:36:21 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:21 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:21 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:21 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0010026\n",
      "DEBUG:LiteLLM:response_cost: 0.0010026\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:36:21 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:36:21 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:36:21 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:36:21 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:36:21 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:36:21 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:36:21 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:36:21 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:36:21 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:36:21 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:36:21 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:36:21 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:36:21 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:21 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x13602be00>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1457b6750> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1456d0650>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:36:25 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=3719'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:36:25 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in 30 minutes.\\n- What are some good vegetarian lunch options that take about half an hour to prepare?\\n- Need a quick plant-based lunch, done in 30 mins.\\n- Show me some 30-minute vegetarian lunch ideas.\\n- Recipes for a meat-free lunch that takes no more than 30 minutes.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 88,\n",
      "    \"totalTokenCount\": 1578,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 398\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"CRpgaLXOMKeIxN8PvIrBsQg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in 30 minutes.\\n- What are some good vegetarian lunch options that take about half an hour to prepare?\\n- Need a quick plant-based lunch, done in 30 mins.\\n- Show me some 30-minute vegetarian lunch ideas.\\n- Recipes for a meat-free lunch that takes no more than 30 minutes.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 88,\n",
      "    \"totalTokenCount\": 1578,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 398\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"CRpgaLXOMKeIxN8PvIrBsQg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:36:25 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:36:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:36:25 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:25 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:25 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:25 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0015425999999999999\n",
      "DEBUG:LiteLLM:response_cost: 0.0015425999999999999\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:36:25 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:36:25 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:36:25 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:36:25 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:36:25 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:36:25 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:36:25 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:36:25 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:36:25 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:36:25 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:36:25 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:36:25 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:36:25 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:25 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1456d1670>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1457b45d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145703a40>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:36:28 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=2093'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in 30 minutes.\\n- What can I cook for a vegetarian lunch that's ready in half an hour?\\n- Give me some quick vegetarian lunch ideas, around 30 minutes.\\n- I need a vegetarian lunch recipe that won't take longer than 30 minutes to prepare.\\n- Any suggestions for a 30-minute vegetarian lunch?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 91,\n",
      "    \"totalTokenCount\": 1262,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 79\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"DBpgaNjuArvgvdIPhO79wAw\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in 30 minutes.\\n- What can I cook for a vegetarian lunch that's ready in half an hour?\\n- Give me some quick vegetarian lunch ideas, around 30 minutes.\\n- I need a vegetarian lunch recipe that won't take longer than 30 minutes to prepare.\\n- Any suggestions for a 30-minute vegetarian lunch?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 91,\n",
      "    \"totalTokenCount\": 1262,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 79\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"DBpgaNjuArvgvdIPhO79wAw\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:36:27 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:36:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:27 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0007526\n",
      "DEBUG:LiteLLM:response_cost: 0.0007526\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:36:27 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:36:27 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:36:27 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:27 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1456d39e0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1457b69d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1456d2b70>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:36:30 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=2108'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in about 30 minutes.\\n- What's a good vegetarian lunch option that takes 30 minutes or less to prepare?\\n- Give me some lunch ideas, strictly vegetarian, ready in half an hour.\\n- I need a quick vegetarian lunch; 30 minutes is my limit.\\n- Suggest a 30-minute vegetarian lunch.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 88,\n",
      "    \"totalTokenCount\": 1263,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 83\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"DhpgaMabEf-exN8PzJzu8QU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in about 30 minutes.\\n- What's a good vegetarian lunch option that takes 30 minutes or less to prepare?\\n- Give me some lunch ideas, strictly vegetarian, ready in half an hour.\\n- I need a quick vegetarian lunch; 30 minutes is my limit.\\n- Suggest a 30-minute vegetarian lunch.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 88,\n",
      "    \"totalTokenCount\": 1263,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 83\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"DhpgaMabEf-exN8PzJzu8QU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:36:30 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:36:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0007551000000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.0007551000000000001\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:36:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:36:30 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:36:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145701be0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1457b6e50> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x13602be00>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:36:34 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=4118'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in 30 minutes.\\n- What can I cook for a quick vegetarian lunch, ready in half an hour?\\n- Vegetarian lunch ideas, 30-minute prep.\\n- Show me some easy vegetarian lunch recipes that take about 30 minutes.\\n- Need a vegetarian lunch recipe, 30 minutes or less.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 83,\n",
      "    \"totalTokenCount\": 1605,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 430\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"EhpgaIHGI-WEvdIPz--V2AE\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in 30 minutes.\\n- What can I cook for a quick vegetarian lunch, ready in half an hour?\\n- Vegetarian lunch ideas, 30-minute prep.\\n- Show me some easy vegetarian lunch recipes that take about 30 minutes.\\n- Need a vegetarian lunch recipe, 30 minutes or less.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 83,\n",
      "    \"totalTokenCount\": 1605,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 430\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"EhpgaIHGI-WEvdIPz--V2AE\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:36:34 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:36:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0016101\n",
      "DEBUG:LiteLLM:response_cost: 0.0016101\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:36:34 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14547ad50>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1457b7650> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14547aba0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Type', b'application/json; charset=UTF-8'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:36:34 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=151'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 429 Too Many Requests\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:36:34 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2268 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "DEBUG:LiteLLM:Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m18:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2165 - Logging Details LiteLLM-Failure Call: []\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Failure Call: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m18:32:45 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:32:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:32:45 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:32:45 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:32:45 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:32:45 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0017726\n",
      "DEBUG:LiteLLM:response_cost: 0.0017726\n",
      "\u001b[92m18:32:45 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:32:45 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:32:45 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:32:45 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:32:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:32:45 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:32:45 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:32:45 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:32:45 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0017726\n",
      "DEBUG:LiteLLM:response_cost: 0.0017726\n",
      "\u001b[92m18:32:45 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:32:45 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:32:45 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:32:46 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:32:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:32:46 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:32:46 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:32:46 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:32:46 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0007226000000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.0007226000000000001\n",
      "\u001b[92m18:32:46 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:32:46 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:32:46 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:32:46 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:32:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:32:46 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:32:46 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:32:46 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:32:46 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0007226000000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.0007226000000000001\n",
      "\u001b[92m18:32:46 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:32:46 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:32:46 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:32:48 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:32:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:32:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:32:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:32:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:32:48 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0010426\n",
      "DEBUG:LiteLLM:response_cost: 0.0010426\n",
      "\u001b[92m18:32:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:32:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:32:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:32:48 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:32:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:32:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:32:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:32:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:32:48 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0010426\n",
      "DEBUG:LiteLLM:response_cost: 0.0010426\n",
      "\u001b[92m18:32:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:32:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:32:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:32:50 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:32:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:32:50 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:32:50 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:32:50 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:32:50 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0011051000000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.0011051000000000001\n",
      "\u001b[92m18:32:50 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:32:50 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:32:50 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:32:50 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:32:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:32:50 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:32:50 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:32:50 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:32:50 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0011051000000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.0011051000000000001\n",
      "\u001b[92m18:32:50 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:32:50 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:32:50 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:33:07 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:33:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:33:07 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:33:07 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:33:07 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:33:07 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.009795100000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.009795100000000001\n",
      "\u001b[92m18:33:07 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:33:07 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:33:07 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:33:07 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:33:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:33:07 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:33:07 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:33:07 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:33:07 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.009795100000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.009795100000000001\n",
      "\u001b[92m18:33:07 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:33:07 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:33:07 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:33:13 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:33:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:33:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:33:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:33:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:33:13 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0008126\n",
      "DEBUG:LiteLLM:response_cost: 0.0008126\n",
      "\u001b[92m18:33:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:33:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:33:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:33:13 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:33:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:33:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:33:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:33:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:33:13 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0008126\n",
      "DEBUG:LiteLLM:response_cost: 0.0008126\n",
      "\u001b[92m18:33:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:33:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:33:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:33:19 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:33:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:33:19 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:33:19 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:33:19 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:33:19 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0010751\n",
      "DEBUG:LiteLLM:response_cost: 0.0010751\n",
      "\u001b[92m18:33:19 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:33:19 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:33:19 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:33:19 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:33:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:33:19 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:33:19 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:33:19 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:33:19 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0010751\n",
      "DEBUG:LiteLLM:response_cost: 0.0010751\n",
      "\u001b[92m18:33:19 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:33:19 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:33:19 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:33:25 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:33:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:33:25 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:33:25 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:33:25 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:33:25 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0008225999999999999\n",
      "DEBUG:LiteLLM:response_cost: 0.0008225999999999999\n",
      "\u001b[92m18:33:25 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:33:25 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:33:25 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:33:25 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:33:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:33:25 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:33:25 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:33:25 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:33:25 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0008225999999999999\n",
      "DEBUG:LiteLLM:response_cost: 0.0008225999999999999\n",
      "\u001b[92m18:33:25 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:33:25 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:33:25 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:06 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:34:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:34:06 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:06 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:06 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:06 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0244801\n",
      "DEBUG:LiteLLM:response_cost: 0.0244801\n",
      "\u001b[92m18:34:06 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:34:06 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:06 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:06 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:34:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:34:06 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:06 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:06 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:06 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0244801\n",
      "DEBUG:LiteLLM:response_cost: 0.0244801\n",
      "\u001b[92m18:34:06 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:34:06 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:06 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:13 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:34:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:34:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:13 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0023026\n",
      "DEBUG:LiteLLM:response_cost: 0.0023026\n",
      "\u001b[92m18:34:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:34:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:13 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:34:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:34:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:13 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0023026\n",
      "DEBUG:LiteLLM:response_cost: 0.0023026\n",
      "\u001b[92m18:34:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:34:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:19 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:34:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:34:19 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:19 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:19 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:19 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0017726\n",
      "DEBUG:LiteLLM:response_cost: 0.0017726\n",
      "\u001b[92m18:34:19 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:34:19 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:19 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:19 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:34:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:34:19 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:19 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:19 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:19 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0017726\n",
      "DEBUG:LiteLLM:response_cost: 0.0017726\n",
      "\u001b[92m18:34:19 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:34:19 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:19 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:23 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:34:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:34:23 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:23 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:23 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:23 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0010326\n",
      "DEBUG:LiteLLM:response_cost: 0.0010326\n",
      "\u001b[92m18:34:23 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:34:23 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:23 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:23 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:34:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:34:23 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:23 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:23 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:23 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0010326\n",
      "DEBUG:LiteLLM:response_cost: 0.0010326\n",
      "\u001b[92m18:34:23 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:34:23 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:23 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:34:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:34:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0011426000000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.0011426000000000001\n",
      "\u001b[92m18:34:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:34:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:34:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:34:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0011426000000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.0011426000000000001\n",
      "\u001b[92m18:34:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:34:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:32 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:34:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:34:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:32 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0007501000000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.0007501000000000001\n",
      "\u001b[92m18:34:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:34:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:32 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:34:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:34:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:32 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0007501000000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.0007501000000000001\n",
      "\u001b[92m18:34:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:34:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:34:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:34:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0007676\n",
      "DEBUG:LiteLLM:response_cost: 0.0007676\n",
      "\u001b[92m18:34:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:34:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:34:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:34:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0007676\n",
      "DEBUG:LiteLLM:response_cost: 0.0007676\n",
      "\u001b[92m18:34:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:34:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:55 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:34:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:34:55 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:55 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:55 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:55 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.010342600000000002\n",
      "DEBUG:LiteLLM:response_cost: 0.010342600000000002\n",
      "\u001b[92m18:34:55 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:34:55 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:55 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:55 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:34:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:34:55 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:55 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:55 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:55 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.010342600000000002\n",
      "DEBUG:LiteLLM:response_cost: 0.010342600000000002\n",
      "\u001b[92m18:34:55 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:34:55 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:55 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:34:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:34:57 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:57 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:57 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0008801\n",
      "DEBUG:LiteLLM:response_cost: 0.0008801\n",
      "\u001b[92m18:34:57 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:34:57 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:57 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:34:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:34:57 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:57 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:57 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:34:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0008801\n",
      "DEBUG:LiteLLM:response_cost: 0.0008801\n",
      "\u001b[92m18:34:57 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:34:57 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:34:57 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:35:02 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:35:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:35:02 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:35:02 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:35:02 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:35:02 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0012401\n",
      "DEBUG:LiteLLM:response_cost: 0.0012401\n",
      "\u001b[92m18:35:02 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:35:02 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:35:02 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:35:02 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:35:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:35:02 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:35:02 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:35:02 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:35:02 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0012401\n",
      "DEBUG:LiteLLM:response_cost: 0.0012401\n",
      "\u001b[92m18:35:02 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:35:02 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:35:02 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:35:48 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:35:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:35:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:35:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:35:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:35:48 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.027122600000000004\n",
      "DEBUG:LiteLLM:response_cost: 0.027122600000000004\n",
      "\u001b[92m18:35:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:35:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:35:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:35:48 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:35:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:35:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:35:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:35:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:35:48 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.027122600000000004\n",
      "DEBUG:LiteLLM:response_cost: 0.027122600000000004\n",
      "\u001b[92m18:35:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:35:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:35:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:35:52 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:35:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:35:52 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:35:52 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:35:52 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:35:52 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0012776\n",
      "DEBUG:LiteLLM:response_cost: 0.0012776\n",
      "\u001b[92m18:35:52 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:35:52 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:35:52 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:35:52 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:35:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:35:52 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:35:52 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:35:52 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:35:52 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0012776\n",
      "DEBUG:LiteLLM:response_cost: 0.0012776\n",
      "\u001b[92m18:35:52 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:35:52 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:35:52 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:36:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:36:01 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:01 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:01 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0012126\n",
      "DEBUG:LiteLLM:response_cost: 0.0012126\n",
      "\u001b[92m18:36:01 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:36:01 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:01 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:36:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:36:01 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:01 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:01 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0012126\n",
      "DEBUG:LiteLLM:response_cost: 0.0012126\n",
      "\u001b[92m18:36:01 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:36:01 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:01 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:05 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:36:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:36:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:05 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0009201000000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.0009201000000000001\n",
      "\u001b[92m18:36:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:36:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:05 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:36:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:36:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:05 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0009201000000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.0009201000000000001\n",
      "\u001b[92m18:36:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:36:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:08 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:36:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:36:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:08 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0009651\n",
      "DEBUG:LiteLLM:response_cost: 0.0009651\n",
      "\u001b[92m18:36:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:36:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:08 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:36:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:36:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:08 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0009651\n",
      "DEBUG:LiteLLM:response_cost: 0.0009651\n",
      "\u001b[92m18:36:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:36:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:11 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:36:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:36:11 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:11 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:11 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:11 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0011401\n",
      "DEBUG:LiteLLM:response_cost: 0.0011401\n",
      "\u001b[92m18:36:11 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:36:11 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:11 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:11 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:36:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:36:11 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:11 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:11 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:11 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0011401\n",
      "DEBUG:LiteLLM:response_cost: 0.0011401\n",
      "\u001b[92m18:36:11 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:36:11 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:11 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:15 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:36:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:36:15 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:15 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:15 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:15 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0010901\n",
      "DEBUG:LiteLLM:response_cost: 0.0010901\n",
      "\u001b[92m18:36:15 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:36:15 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:15 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:15 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:36:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:36:15 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:15 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:15 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:15 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0010901\n",
      "DEBUG:LiteLLM:response_cost: 0.0010901\n",
      "\u001b[92m18:36:15 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:36:15 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:15 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:21 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:36:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:36:21 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:21 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:21 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:21 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0010026\n",
      "DEBUG:LiteLLM:response_cost: 0.0010026\n",
      "\u001b[92m18:36:21 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:36:21 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:21 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:21 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:36:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:36:21 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:21 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:21 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:21 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0010026\n",
      "DEBUG:LiteLLM:response_cost: 0.0010026\n",
      "\u001b[92m18:36:21 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:36:21 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:21 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:25 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:36:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:36:25 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:25 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:25 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:25 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0015425999999999999\n",
      "DEBUG:LiteLLM:response_cost: 0.0015425999999999999\n",
      "\u001b[92m18:36:25 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:36:25 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:25 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:25 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:36:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:36:25 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:25 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:25 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:25 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0015425999999999999\n",
      "DEBUG:LiteLLM:response_cost: 0.0015425999999999999\n",
      "\u001b[92m18:36:25 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:36:25 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:25 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:27 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:36:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:27 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0007526\n",
      "DEBUG:LiteLLM:response_cost: 0.0007526\n",
      "\u001b[92m18:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:27 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:36:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:27 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0007526\n",
      "DEBUG:LiteLLM:response_cost: 0.0007526\n",
      "\u001b[92m18:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:36:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0007551000000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.0007551000000000001\n",
      "\u001b[92m18:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:36:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0007551000000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.0007551000000000001\n",
      "\u001b[92m18:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:36:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0016101\n",
      "DEBUG:LiteLLM:response_cost: 0.0016101\n",
      "\u001b[92m18:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:36:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0016101\n",
      "DEBUG:LiteLLM:response_cost: 0.0016101\n",
      "\u001b[92m18:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:37:07 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:37:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:37:07 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:07 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:07 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:37:07 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0009201000000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.0009201000000000001\n",
      "\u001b[92m18:37:07 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:37:07 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:07 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:37:07 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:37:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:37:07 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:07 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:07 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:37:07 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0009201000000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.0009201000000000001\n",
      "\u001b[92m18:37:07 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:37:07 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:07 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:37:10 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:37:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:37:10 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:10 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:10 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:37:10 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0015951\n",
      "DEBUG:LiteLLM:response_cost: 0.0015951\n",
      "\u001b[92m18:37:10 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:37:10 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:10 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:37:10 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:37:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:37:10 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:10 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:10 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:37:10 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0015951\n",
      "DEBUG:LiteLLM:response_cost: 0.0015951\n",
      "\u001b[92m18:37:10 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:37:10 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:10 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:37:13 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:37:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:37:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:37:13 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0012076\n",
      "DEBUG:LiteLLM:response_cost: 0.0012076\n",
      "\u001b[92m18:37:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:37:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:37:13 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:37:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:37:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:37:13 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0012076\n",
      "DEBUG:LiteLLM:response_cost: 0.0012076\n",
      "\u001b[92m18:37:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:37:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:37:16 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:37:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:37:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:37:16 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0010551\n",
      "DEBUG:LiteLLM:response_cost: 0.0010551\n",
      "\u001b[92m18:37:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:37:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:37:16 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:37:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:37:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:37:16 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0010551\n",
      "DEBUG:LiteLLM:response_cost: 0.0010551\n",
      "\u001b[92m18:37:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:37:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:37:20 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:37:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:37:20 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:20 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:20 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:37:20 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0011076\n",
      "DEBUG:LiteLLM:response_cost: 0.0011076\n",
      "\u001b[92m18:37:20 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:37:20 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:20 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:37:20 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:37:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:37:20 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:20 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:20 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:37:20 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0011076\n",
      "DEBUG:LiteLLM:response_cost: 0.0011076\n",
      "\u001b[92m18:37:20 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:37:20 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:20 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:38:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:01 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:01 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:01 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.024962600000000005\n",
      "DEBUG:LiteLLM:response_cost: 0.024962600000000005\n",
      "\u001b[92m18:38:01 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:01 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:01 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:38:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:01 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:01 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:01 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.024962600000000005\n",
      "DEBUG:LiteLLM:response_cost: 0.024962600000000005\n",
      "\u001b[92m18:38:01 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:01 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:01 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:05 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:38:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:05 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0010501\n",
      "DEBUG:LiteLLM:response_cost: 0.0010501\n",
      "\u001b[92m18:38:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:05 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:38:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:05 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0010501\n",
      "DEBUG:LiteLLM:response_cost: 0.0010501\n",
      "\u001b[92m18:38:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:08 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:38:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:08 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0016101\n",
      "DEBUG:LiteLLM:response_cost: 0.0016101\n",
      "\u001b[92m18:38:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:08 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:38:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:08 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0016101\n",
      "DEBUG:LiteLLM:response_cost: 0.0016101\n",
      "\u001b[92m18:38:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:12 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:38:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:12 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:12 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:12 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:12 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0017776000000000003\n",
      "DEBUG:LiteLLM:response_cost: 0.0017776000000000003\n",
      "\u001b[92m18:38:12 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:12 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:12 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:12 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:38:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:12 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:12 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:12 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:12 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0017776000000000003\n",
      "DEBUG:LiteLLM:response_cost: 0.0017776000000000003\n",
      "\u001b[92m18:38:12 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:12 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:12 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:16 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:38:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:16 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0010476\n",
      "DEBUG:LiteLLM:response_cost: 0.0010476\n",
      "\u001b[92m18:38:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:16 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:38:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:16 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0010476\n",
      "DEBUG:LiteLLM:response_cost: 0.0010476\n",
      "\u001b[92m18:38:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:18 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:38:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:18 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0008225999999999999\n",
      "DEBUG:LiteLLM:response_cost: 0.0008225999999999999\n",
      "\u001b[92m18:38:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:18 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:38:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:18 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0008225999999999999\n",
      "DEBUG:LiteLLM:response_cost: 0.0008225999999999999\n",
      "\u001b[92m18:38:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:20 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:38:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:20 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:20 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:20 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:20 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0011176\n",
      "DEBUG:LiteLLM:response_cost: 0.0011176\n",
      "\u001b[92m18:38:20 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:20 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:20 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:20 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:38:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:20 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:20 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:20 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:20 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0011176\n",
      "DEBUG:LiteLLM:response_cost: 0.0011176\n",
      "\u001b[92m18:38:20 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:20 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:20 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:26 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:38:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:26 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:26 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:26 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:26 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0008001\n",
      "DEBUG:LiteLLM:response_cost: 0.0008001\n",
      "\u001b[92m18:38:26 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:26 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:26 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:26 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:38:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:26 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:26 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:26 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:26 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0008001\n",
      "DEBUG:LiteLLM:response_cost: 0.0008001\n",
      "\u001b[92m18:38:26 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:26 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:26 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:28 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:38:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:28 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:28 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:28 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:28 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0010826\n",
      "DEBUG:LiteLLM:response_cost: 0.0010826\n",
      "\u001b[92m18:38:28 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:28 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:28 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:28 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:38:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:28 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:28 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:28 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:28 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0010826\n",
      "DEBUG:LiteLLM:response_cost: 0.0010826\n",
      "\u001b[92m18:38:28 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:28 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:28 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:32 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:38:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:32 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0009976\n",
      "DEBUG:LiteLLM:response_cost: 0.0009976\n",
      "\u001b[92m18:38:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:32 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:38:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:32 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0009976\n",
      "DEBUG:LiteLLM:response_cost: 0.0009976\n",
      "\u001b[92m18:38:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:38:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0008975999999999999\n",
      "DEBUG:LiteLLM:response_cost: 0.0008975999999999999\n",
      "\u001b[92m18:38:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:38:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0008975999999999999\n",
      "DEBUG:LiteLLM:response_cost: 0.0008975999999999999\n",
      "\u001b[92m18:38:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:39 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:38:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:39 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:39 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:39 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:39 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0016326000000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.0016326000000000001\n",
      "\u001b[92m18:38:39 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:39 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:39 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:39 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:38:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:39 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:39 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:39 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:39 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0016326000000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.0016326000000000001\n",
      "\u001b[92m18:38:39 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:39 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:39 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:48 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:38:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:48 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0009951\n",
      "DEBUG:LiteLLM:response_cost: 0.0009951\n",
      "\u001b[92m18:38:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:48 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:38:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:48 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0009951\n",
      "DEBUG:LiteLLM:response_cost: 0.0009951\n",
      "\u001b[92m18:38:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:50 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:38:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:50 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:50 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:50 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:50 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0008151\n",
      "DEBUG:LiteLLM:response_cost: 0.0008151\n",
      "\u001b[92m18:38:50 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:50 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:50 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:50 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:38:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:50 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:50 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:50 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:50 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0008151\n",
      "DEBUG:LiteLLM:response_cost: 0.0008151\n",
      "\u001b[92m18:38:50 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:50 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:50 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:55 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1353 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m18:38:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:55 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:55 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:55 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:55 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0011426000000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.0011426000000000001\n",
      "\u001b[92m18:38:55 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:55 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:55 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:55 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1378 - Logging Details LiteLLM-Success Call streaming complete\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m18:38:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:55 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:55 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:55 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:55 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0011426000000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.0011426000000000001\n",
      "\u001b[92m18:38:55 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:55 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:55 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 2nd time calling it.\n",
      "\u001b[92m18:36:39 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:36:39 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:36:39 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:36:39 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:36:39 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:36:39 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:36:39 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:36:39 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:36:39 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:36:39 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:36:39 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:36:39 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:36:39 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:39 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14547ac00>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1457b7750> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1454787a0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Type', b'application/json; charset=UTF-8'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:36:39 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=49'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 429 Too Many Requests\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:36:39 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2268 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "DEBUG:LiteLLM:Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m18:36:39 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2165 - Logging Details LiteLLM-Failure Call: []\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Failure Call: []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 3rd time calling it.\n",
      "\u001b[92m18:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:36:44 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:36:44 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:36:44 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:36:44 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1444d4830>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1457b03d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14149b680>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "IOStream.flush timed out\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "IOStream.flush timed out\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:37:07 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=12528'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:37:07 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that takes around 30 minutes to make.\\n- Can you suggest a quick vegetarian lunch, ready in half an hour?\\n- What's a good lunch idea for vegetarians that I can cook in 30 minutes?\\n- I need a vegetarian lunch that's done in 30 minutes.\\n- Give me a 30-minute vegetarian lunch recipe.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 88,\n",
      "    \"totalTokenCount\": 1329,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 149\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"MxpgaMvOBceIvdIPpNuZwAg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that takes around 30 minutes to make.\\n- Can you suggest a quick vegetarian lunch, ready in half an hour?\\n- What's a good lunch idea for vegetarians that I can cook in 30 minutes?\\n- I need a vegetarian lunch that's done in 30 minutes.\\n- Give me a 30-minute vegetarian lunch recipe.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 88,\n",
      "    \"totalTokenCount\": 1329,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 149\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"MxpgaMvOBceIvdIPpNuZwAg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:37:07 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:37:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:37:07 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:07 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:07 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:37:07 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0009201000000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.0009201000000000001\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:37:07 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:37:07 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:37:07 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:37:07 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:37:07 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:37:07 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:37:07 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:37:07 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:37:07 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:37:07 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:37:07 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:37:07 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:37:07 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:07 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1456d3620>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1457b4c50> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1456d10a0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:37:10 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=3431'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:37:10 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a vegetarian lunch recipe that I can make in about 30 minutes.\\n- Show me some quick vegetarian lunch ideas, around half an hour prep time.\\n- Vegetarian lunch, 30 minutes.\\n- What can I cook for a vegetarian lunch that only takes 30 minutes?\\n- I'm looking for a vegetarian lunch that's ready in under 30 minutes.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 84,\n",
      "    \"totalTokenCount\": 1599,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 423\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"NhpgaI-4LZ3PnsEPz87bsQk\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a vegetarian lunch recipe that I can make in about 30 minutes.\\n- Show me some quick vegetarian lunch ideas, around half an hour prep time.\\n- Vegetarian lunch, 30 minutes.\\n- What can I cook for a vegetarian lunch that only takes 30 minutes?\\n- I'm looking for a vegetarian lunch that's ready in under 30 minutes.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 84,\n",
      "    \"totalTokenCount\": 1599,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 423\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"NhpgaI-4LZ3PnsEPz87bsQk\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:37:10 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:37:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:37:10 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:10 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:10 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:37:10 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0015951\n",
      "DEBUG:LiteLLM:response_cost: 0.0015951\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:37:10 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:37:10 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:37:10 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:37:10 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:37:10 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:37:10 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:37:10 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:37:10 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:37:10 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:37:10 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:37:10 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:37:10 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:37:10 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:10 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1456d2ae0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1457b0ad0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145701be0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:37:13 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=2770'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:37:13 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a vegetarian lunch recipe that I can make in 30 minutes.\\n- What are some quick vegetarian lunch ideas for under half an hour?\\n- Give me a 30-minute vegetarian lunch.\\n- Looking for a fast lunch recipe, make it vegetarian and ready in 30 mins.\\n- Can you suggest a vegetarian lunch that takes about 30 minutes?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 81,\n",
      "    \"totalTokenCount\": 1444,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 271\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"ORpgaIiMJ8iynsEP6diZ4Ak\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a vegetarian lunch recipe that I can make in 30 minutes.\\n- What are some quick vegetarian lunch ideas for under half an hour?\\n- Give me a 30-minute vegetarian lunch.\\n- Looking for a fast lunch recipe, make it vegetarian and ready in 30 mins.\\n- Can you suggest a vegetarian lunch that takes about 30 minutes?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 81,\n",
      "    \"totalTokenCount\": 1444,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 271\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"ORpgaIiMJ8iynsEP6diZ4Ak\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:37:13 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:37:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:37:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:37:13 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0012076\n",
      "DEBUG:LiteLLM:response_cost: 0.0012076\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:37:13 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:37:13 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:37:13 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:37:13 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:37:13 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:37:13 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:37:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:37:13 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:37:13 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:37:13 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:37:13 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:37:13 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:37:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:13 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x135e0bc20>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1457b2550> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145701be0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:37:16 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=2583'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:37:16 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a vegetarian lunch recipe that I can prepare in 30 minutes.\\n- What are some quick vegetarian lunch ideas for around 30 minutes?\\n- Give me a recipe for a vegetarian lunch that takes about half an hour.\\n- I'm looking for a 30-minute vegetarian lunch.\\n- Can you suggest a quick vegetarian lunch I can make in 30 minutes?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 84,\n",
      "    \"totalTokenCount\": 1383,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 207\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"PBpgaLOOFcGwxN8PlsOrmAY\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a vegetarian lunch recipe that I can prepare in 30 minutes.\\n- What are some quick vegetarian lunch ideas for around 30 minutes?\\n- Give me a recipe for a vegetarian lunch that takes about half an hour.\\n- I'm looking for a 30-minute vegetarian lunch.\\n- Can you suggest a quick vegetarian lunch I can make in 30 minutes?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 84,\n",
      "    \"totalTokenCount\": 1383,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 207\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"PBpgaLOOFcGwxN8PlsOrmAY\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:37:16 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:37:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:37:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:37:16 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0010551\n",
      "DEBUG:LiteLLM:response_cost: 0.0010551\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:37:16 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:37:16 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:37:16 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:37:16 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:37:16 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:37:16 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:37:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:37:16 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:37:16 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:37:16 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:37:16 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:37:16 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:37:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:16 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1457035f0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1457b00d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14149ac30>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:37:20 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=3830'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:37:20 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in 30 minutes.\\n- What's a good lunch idea that's vegetarian and ready in half an hour?\\n- Need a quick 30-minute vegetarian lunch.\\n- Show me some vegetarian lunch options that take about 30 minutes to cook.\\n- Recipes for a vegetarian lunch, quick (30 mins).\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 85,\n",
      "    \"totalTokenCount\": 1404,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 227\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"QBpgaIHeFca6xN8PwdXX-AU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in 30 minutes.\\n- What's a good lunch idea that's vegetarian and ready in half an hour?\\n- Need a quick 30-minute vegetarian lunch.\\n- Show me some vegetarian lunch options that take about 30 minutes to cook.\\n- Recipes for a vegetarian lunch, quick (30 mins).\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 85,\n",
      "    \"totalTokenCount\": 1404,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 227\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"QBpgaIHeFca6xN8PwdXX-AU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:37:20 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:37:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:37:20 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:20 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:20 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:37:20 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0011076\n",
      "DEBUG:LiteLLM:response_cost: 0.0011076\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:37:20 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:37:20 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:37:20 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:37:20 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:37:20 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:37:20 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:37:20 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:37:20 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:37:20 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:37:20 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:37:20 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:37:20 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:37:20 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:37:20 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14149bcb0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1457b3350> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1457001d0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:38:01 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=40843'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:38:01 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"\\u003cdimension\\u003e\\n('Lunch', 'Vegetarian', '30 minutes')\\n\\u003c/dimension\\u003e\\n\\n- I'm looking for a vegetarian lunch recipe that takes about 30 minutes.\\n- Can you find me a quick meat-free lunch, ready in half an hour?\\n- Vegetarian lunch, 30-minute prep.\\n- Need a 30-minute vegetarian lunch idea.\\n- What can I make for lunch, vegetarian, taking around 30 mins?\\n\\n\\u003cdimension\\u003e\\n('Dinner', 'Meaty', 'Under 1 hour')\\n\\u003c/dimension\\u003e\\n\\n- I need a meaty dinner recipe that I can cook in under an hour.\\n- What's a good dinner option with meat that takes less than 60 minutes?\\n- Find me a quick meaty dinner, under 1 hour.\\n- Dinner ideas, meaty, less than 60 mins.\\n- Something meaty for dinner, and I only have an hour.\\n\\n\\u003cdimension\\u003e\\n('Snack', 'Gluten-free', 'Quick')\\n\\u003c/dimension\\u003e\\n\\n- Give me a quick gluten-free snack idea.\\n- I need a fast gluten-free snack.\\n- Gluten-free snack, quick to make.\\n- What's a quick, celiac-friendly snack?\\n- Fast snack, no gluten.\\n\\n\\u003cdimension\\u003e\\n('Dessert', 'Dairy-free', '45 minutes')\\n\\u003c/dimension\\u003e\\n\\n- I'd like a dairy-free dessert that takes about 45 minutes to prepare.\\n- Find me a dessert without dairy, ready in 45 minutes.\\n- Dairy-free dessert, 45-minute cook time.\\n- Looking for a 45-minute, non-dairy sweet treat.\\n- Can you suggest a dairy-free dessert that's done in 45 mins?\\n\\n\\u003cdimension\\u003e\\n('Brunch', 'Vegan', 'I have all the time until retirement')\\n\\u003c/dimension\\u003e\\n\\n- I'm planning a vegan brunch and have plenty of time, no rush at all.\\n- Give me an elaborate vegan brunch recipe; time is not an issue.\\n- Vegan brunch, something that takes all day.\\n- What are some long-form vegan brunch recipes?\\n- I have unlimited time for a vegan brunch.\\n\\n\\u003cdimension\\u003e\\n('Appetizer', 'Low-carb', '15 minutes')\\n\\u003c/dimension\\u003e\\n\\n- I need a low-carb appetizer that's quick, about 15 minutes.\\n- Find me a 15-minute low-carb starter.\\n- Quick low-carb appetizer, done in 15 mins.\\n- 15-minute, keto-friendly app.\\n- Low-carb finger food, fast, 15 min.\\n\\n\\u003cdimension\\u003e\\n('Side Dish', 'High-protein', '20 minutes')\\n\\u003c/dimension\\u003e\\n\\n- What's a good high-protein side dish that takes 20 minutes?\\n- I need a side with lots of protein, ready in 20 minutes.\\n- 20-minute high-protein side.\\n- Quick high-protein accompaniment, 20 mins.\\n- High-protein side dish, around 20 minutes prep.\\n\\n\\u003cdimension\\u003e\\n('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n\\u003c/dimension\\u003e\\n\\n- Give me a light, kid-friendly breakfast recipe, under 20 minutes.\\n- Quick and easy light breakfast for kids, less than 20 mins.\\n- Kid-friendly light breakfast, fast, under 20.\\n- What can I make for a light breakfast for my kids in under 20 minutes?\\n- Fast, light, kid-approved breakfast.\\n\\n\\u003cdimension\\u003e\\n('Holiday meal', 'Pescatarian', 'All day')\\n\\u003c/dimension\\u003e\\n\\n- I'm looking for an all-day pescatarian holiday meal recipe.\\n- Give me a grand pescatarian holiday meal, something that takes all day.\\n- Pescatarian holiday feast, long cook time.\\n- An all-day cooking project for a pescatarian holiday meal.\\n- I want a time-consuming pescatarian holiday dinner.\\n\\n\\u003cdimension\\u003e\\n('Party food', 'Sugar-free', 'About an hour')\\n\\u003c/dimension\\u003e\\n\\n- I need sugar-free party food that takes about an hour to make.\\n- Find me a party snack that's sugar-free and takes roughly an hour.\\n- Sugar-free party recipe, around 60 minutes.\\n- What are some hour-long sugar-free party food ideas?\\n- Party food, no sugar, takes about an hour.\\n\\n\\u003cdimension\\u003e\\n('Quick bite', 'Nut-free', 'Instant')\\n\\u003c/dimension\\u003e\\n\\n- Give me an instant nut-free quick bite.\\n- I need a super fast, nut-free snack.\\n- Instant, nut-free quick snack.\\n- Something quick and nut-free right now.\\n- What's a literally instant nut-free bite?\\n\\n\\u003cdimension\\u003e\\n('Breakfast', 'Keto', '30 minutes')\\n\\u003c/dimension\\u003e\\n\\n- I need a keto breakfast recipe that takes about 30 minutes.\\n- Find me a 30-minute keto breakfast.\\n- Keto breakfast, half an hour prep.\\n- What's a quick keto breakfast I can make in 30 minutes?\\n- 30-min low-carb breakfast.\\n\\n\\u003cdimension\\u003e\\n('Lunch', 'Heart-healthy', 'Under 1 hour')\\n\\u003c/dimension\\u003e\\n\\n- I'm looking for a heart-healthy lunch recipe under an hour.\\n- Give me a heart-healthy lunch that's ready in less than 60 minutes.\\n- Heart-healthy lunch, less than an hour.\\n- Quick heart-healthy lunch, under 60 mins.\\n- What's a good heart-healthy lunch I can make in under an hour?\\n\\n\\u003cdimension\\u003e\\n('Dinner', 'Spicy', '45 minutes')\\n\\u003c/dimension\\u003e\\n\\n- I want a spicy dinner recipe that takes 45 minutes.\\n- Find me a 45-minute spicy dinner.\\n- Spicy dinner, ready in 45 mins.\\n- What's a quick and spicy dinner for tonight, done in 45?\\n- 45-minute fiery dinner.\\n\\n\\u003cdimension\\u003e\\n('Snack', 'without fish or meat', '10 minutes')\\n\\u003c/dimension\\u003e\\n\\n- I need a 10-minute snack without fish or meat.\\n- Quick snack, no animal protein, 10 minutes.\\n- Find me a fast plant-based snack, ready in 10 minutes.\\n- 10-minute snack, no meat or fish.\\n- What's a quick 10-minute vegetarian snack?\\n\\n\\u003cdimension\\u003e\\n('Dessert', 'Meaty', 'Long prep')\\n\\u003c/dimension\\u003e\\n\\n- I'm looking for a meaty dessert that takes a long time to prepare.\\n- Give me a meaty sweet recipe with long prep time.\\n- Meaty dessert, slow cook.\\n- What's a long-prep meaty dessert?\\n- Something meaty for dessert that requires a lot of time.\\n\\n\\u003cdimension\\u003e\\n('Brunch', 'Paleo', 'Quick')\\n\\u003c/dimension\\u003e\\n\\n- I need a quick paleo brunch recipe.\\n- Find me a fast paleo brunch.\\n- Paleo brunch, quick to make.\\n- What's a quick paleo option for brunch?\\n- Fast and easy paleo brunch.\\n\\n\\u003cdimension\\u003e\\n('Appetizer', 'Low-salt', '15 minutes')\\n\\u003c/dimension\\u003e\\n\\n- Give me a 15-minute low-salt appetizer.\\n- I need a quick low-sodium starter, ready in 15 minutes.\\n- Low-salt appetizer, fast, 15 mins.\\n- What's a 15-minute low-sodium finger food?\\n- Quick low-salt app, 15 minutes.\\n\\n\\u003cdimension\\u003e\\n('Side Dish', 'Diabetic-friendly', '20 minutes')\\n\\u003c/dimension\\u003e\\n\\n- I'm looking for a diabetic-friendly side dish that takes 20 minutes.\\n- Find me a 20-minute side for diabetics.\\n- Diabetic-friendly side, ready in 20 minutes.\\n- Quick 20-minute side dish suitable for diabetics.\\n- What's a good diabetic-friendly side that takes about 20 minutes?\\n\\n\\u003cdimension\\u003e\\n('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n\\u003c/dimension\\u003e\\n\\n- I need a high-fiber light breakfast that takes less than 20 minutes.\\n- Give me a quick, high-fiber light breakfast, under 20 minutes.\\n- High-fiber light breakfast, less than 20 mins.\\n- Fast and light high-fiber breakfast, under 20.\\n- What's a quick, high-fiber breakfast for a lighter meal?\\n\\n\\u003cdimension\\u003e\\n('Holiday meal', 'Lactose-free', 'Overnight')\\n\\u003c/dimension\\u003e\\n\\n- I need an overnight lactose-free holiday meal.\\n- Find me a lactose-free holiday recipe that cooks overnight.\\n- Lactose-free holiday meal, slow cooker/overnight.\\n- What's a good lactose-free holiday meal to prepare overnight?\\n- Overnight lactose-free holiday feast.\\n\\n\\u003cdimension\\u003e\\n('Party food', 'Comfort food', 'About an hour')\\n\\u003c/dimension\\u003e\\n\\n- I'm looking for comfort food party recipes that take about an hour.\\n- Give me a comforting party dish that's ready in about 60 minutes.\\n- Comfort food for a party, around an hour.\\n- What are some hour-long comforting party food ideas?\\n- Party food, comfort style, takes about an hour.\\n\\n\\u003cdimension\\u003e\\n('Quick bite', 'with hummus and carrots', 'Instant')\\n\\u003c/dimension\\u003e\\n\\n- Give me an instant quick bite featuring hummus and carrots.\\n- I need a really fast snack with hummus and carrots.\\n- Instant bite, hummus and carrots.\\n- What's a quick snack I can make right now using hummus and carrots?\\n- Super quick bite, hummus and carrots.\\n\\n\\u003cdimension\\u003e\\n('Breakfast', 'Gluten-free', 'Quick and easy')\\n\\u003c/dimension\\u003e\\n\\n- I need a quick and easy gluten-free breakfast.\\n- Find me a fast and simple gluten-free breakfast.\\n- Gluten-free breakfast, quick and easy.\\n- What's an easy, quick gluten-free breakfast?\\n- Simple, celiac-friendly breakfast, fast.\\n\\n\\u003cdimension\\u003e\\n('Lunch', 'Vegan', '30 minutes')\\n\\u003c/dimension\\u003e\\n\\n- I need a vegan lunch recipe that takes 30 minutes.\\n- Find me a 30-minute vegan lunch.\\n- Vegan lunch, half an hour.\\n- What's a quick vegan lunch idea for 30 minutes?\\n- 30-minute plant-based lunch.\\n\\n\\u003cdimension\\u003e\\n('Dinner', 'High-protein', 'Under 1 hour')\\n\\u003c/dimension\\u003e\\n\\n- I'm looking for a high-protein dinner recipe under an hour.\\n- Give me a high-protein dinner that's ready in less than 60 minutes.\\n- High-protein dinner, less than an hour.\\n- Quick high-protein dinner, under 60 mins.\\n- What's a good high-protein dinner I can make in under an hour?\\n\\n\\u003cdimension\\u003e\\n('Snack', 'Not spicy', '10 minutes')\\n\\u003c/dimension\\u003e\\n\\n- I want a quick, non-spicy snack that takes 10 minutes.\\n- Give me a mild snack, ready in 10 minutes.\\n- Not spicy snack, 10-minute prep.\\n- A ten-minute, plain snack.\\n- I need a quick, non-fiery snack under 10 minutes.\\n\\n\\u003cdimension\\u003e\\n('Dessert', 'Vegetarian', '45 minutes')\\n\\u003c/dimension\\u003e\\n\\n- I'd like a vegetarian dessert that takes about 45 minutes to prepare.\\n- Find me a dessert without meat, ready in 45 minutes.\\n- Vegetarian dessert, 45-minute cook time.\\n- Looking for a 45-minute, meat-free sweet treat.\\n- Can you suggest a vegetarian dessert that's done in 45 mins?\\n\\n\\u003cdimension\\u003e\\n('Brunch', 'Meaty', 'I have all the time until retirement')\\n\\u003c/dimension\\u003e\\n\\n- I'm planning a meaty brunch and have plenty of time, no rush at all.\\n- Give me an elaborate meaty brunch recipe; time is not an issue.\\n- Meaty brunch, something that takes all day.\\n- What are some long-form meaty brunch recipes?\\n- I have unlimited time for a meaty brunch.\\n\\n\\u003cdimension\\u003e\\n('Appetizer', 'Dairy-free', '15 minutes')\\n\\u003c/dimension\\u003e\\n\\n- I need a dairy-free appetizer that's quick, about 15 minutes.\\n- Find me a 15-minute dairy-free starter.\\n- Quick dairy-free appetizer, done in 15 mins.\\n- 15-minute, non-dairy app.\\n- Dairy-free finger food, fast, 15 min.\\n\\n\\u003cdimension\\u003e\\n('Side Dish', 'Low-carb', '20 minutes')\\n\\u003c/dimension\\u003e\\n\\n- What's a good low-carb side dish that takes 20 minutes?\\n- I need a side with low carbs, ready in 20 minutes.\\n- 20-minute low-carb side.\\n- Quick low-carb accompaniment, 20 mins.\\n- Low-carb side dish, around 20 minutes prep.\\n\\n\\u003cdimension\\u003e\\n('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n\\u003c/dimension\\u003e\\n\\n- Give me a light, pescatarian breakfast recipe, under 20 minutes.\\n- Quick and easy light pescatarian breakfast, under 20 mins.\\n- Pescatarian light breakfast, fast, under 20.\\n- What can I make for a light pescatarian breakfast in under 20 minutes?\\n- Fast, light, fish-friendly breakfast.\\n\\n\\u003cdimension\\u003e\\n('Holiday meal', 'Kid-friendly', 'All day')\\n\\u003c/dimension\\u003e\\n\\n- I'm looking for an all-day kid-friendly holiday meal recipe.\\n- Give me a grand kid-friendly holiday meal, something that takes all day.\\n- Kid-friendly holiday feast, long cook time.\\n- An all-day cooking project for a holiday meal that kids will love.\\n- I want a time-consuming kid-approved holiday dinner.\\n\\n\\u003cdimension\\u003e\\n('Party food', 'Nut-free', 'About an hour')\\n\\u003c/dimension\\u003e\\n\\n- I need nut-free party food that takes about an hour to make.\\n- Find me a party snack that's nut-free and takes roughly an hour.\\n- Nut-free party recipe, around 60 minutes.\\n- What are some hour-long nut-free party food ideas?\\n- Party food, no nuts, takes about an hour.\\n\\n\\u003cdimension\\u003e\\n('Quick bite', 'Sugar-free', 'Instant')\\n\\u003c/dimension\\u003e\\n\\n- Give me an instant sugar-free quick bite.\\n- I need a super fast, sugar-free snack.\\n- Instant, sugar-free quick snack.\\n- Something quick and sugar-free right now.\\n- What's a literally instant sugar-free bite?\\n\\n\\u003cdimension\\u003e\\n('Breakfast', 'Spicy', 'Quick and easy')\\n\\u003c/dimension\\u003e\\n\\n- I need a quick and easy spicy breakfast.\\n- Find me a fast and simple spicy breakfast.\\n- Spicy breakfast, quick and easy.\\n- What's an easy, quick spicy breakfast?\\n- Simple, fiery breakfast, fast.\\n\\n\\u003cdimension\\u003e\\n('Lunch', 'Keto', '30 minutes')\\n\\u003c/dimension\\u003e\\n\\n- I need a keto lunch recipe that takes 30 minutes.\\n- Find me a 30-minute keto lunch.\\n- Keto lunch, half an hour.\\n- What's a quick keto lunch idea for 30 minutes?\\n- 30-minute low-carb lunch.\\n\\n\\u003cdimension\\u003e\\n('Dinner', 'Heart-healthy', 'Under 1 hour')\\n\\u003c/dimension\\u003e\\n\\n- I'm looking for a heart-healthy dinner recipe under an hour.\\n- Give me a heart-healthy dinner that's ready in less than 60 minutes.\\n- Heart-healthy dinner, less than an hour.\\n- Quick heart-healthy dinner, under 60 mins.\\n- What's a good heart-healthy dinner I can make in under an hour?\\n\\n\\u003cdimension\\u003e\\n('Snack', 'Low-salt', '10 minutes')\\n\\u003c/dimension\\u003e\\n\\n- I want a quick, low-salt snack that takes 10 minutes.\\n- Give me a low-sodium snack, ready in 10 minutes.\\n- Low-salt snack, 10-minute prep.\\n- A ten-minute, low-sodium snack.\\n- I need a quick, low-salt snack under 10 minutes.\\n\\n\\u003cdimension\\u003e\\n('Dessert', 'Diabetic-friendly', '45 minutes')\\n\\u003c/dimension\\u003e\\n\\n- I'd like a diabetic-friendly dessert that takes about 45 minutes to prepare.\\n- Find me a dessert suitable for diabetics, ready in 45 minutes.\\n- Diabetic-friendly dessert, 45-minute cook time.\\n- Looking for a 45-minute, diabetic-safe sweet treat.\\n- Can you suggest a dessert for diabetics that's done in 45 mins?\\n\\n\\u003cdimension\\u003e\\n('Brunch', 'High-fiber', 'Long prep')\\n\\u003c/dimension\\u003e\\n\\n- I'm planning a high-fiber brunch that takes a long time to prepare.\\n- Give me an elaborate high-fiber brunch recipe; it can take a while.\\n- High-fiber brunch, slow cooking.\\n- What are some long-prep high-fiber brunch recipes?\\n- I have plenty of time for a high-fiber brunch.\\n\\n\\u003cdimension\\u003e\\n('Appetizer', 'Lactose-free', '15 minutes')\\n\\u003c/dimension\\u003e\\n\\n- I need a lactose-free appetizer that's quick, about 15 minutes.\\n- Find me a 15-minute lactose-free starter.\\n- Quick lactose-free appetizer, done in 15 mins.\\n- 15-minute, lactose-free app.\\n- Lactose-free finger food, fast, 15 min.\\n\\n\\u003cdimension\\u003e\\n('Side Dish', 'Comfort food', '20 minutes')\\n\\u003c/dimension\\u003e\\n\\n- What's a good comfort food side dish that takes 20 minutes?\\n- I need a comforting side, ready in 20 minutes.\\n- 20-minute comfort food side.\\n- Quick comforting accompaniment, 20 mins.\\n- Comfort food side dish, around 20 minutes prep.\\n\\n\\u003cdimension\\u003e\\n('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n\\u003c/dimension\\u003e\\n\\n- Give me a light, paleo breakfast recipe, under 20 minutes.\\n- Quick and easy light paleo breakfast, less than 20 mins.\\n- Paleo light breakfast, fast, under 20.\\n- What can I make for a light paleo breakfast in under 20 minutes?\\n- Fast, light, paleo-approved breakfast.\\n\\n\\u003cdimension\\u003e\\n('Holiday meal', 'with hummus and carrots', 'Overnight')\\n\\u003c/dimension\\u003e\\n\\n- I need an overnight holiday meal featuring hummus and carrots.\\n- Find me a holiday recipe with hummus and carrots that cooks overnight.\\n- Holiday meal, overnight, with hummus and carrots.\\n- What's a good holiday meal to prepare overnight, including hummus and carrots?\\n- Overnight holiday feast with hummus and carrots.\\n\\n\\u003cdimension\\u003e\\n('Party food', 'Not spicy', 'About an hour')\\n\\u003c/dimension\\u003e\\n\\n- I need non-spicy party food that takes about an hour to make.\\n- Find me a party snack that's mild and takes roughly an hour.\\n- Not spicy party recipe, around 60 minutes.\\n- What are some hour-long mild party food ideas?\\n- Party food, not spicy, takes about an hour.\\n\\n\\u003cdimension\\u003e\\n('Quick bite', 'Vegetarian', 'Instant')\\n\\u003c/dimension\\u003e\\n\\n- Give me an instant vegetarian quick bite.\\n- I need a super fast, vegetarian snack.\\n- Instant, vegetarian quick snack.\\n- Something quick and vegetarian right now.\\n- What's a literally instant vegetarian bite?\\n\\n\\u003cdimension\\u003e\\n('Dinner', 'Gluten-free', '10 minutes')\\n\\u003c/dimension\\u003e\\n\\n- I want a gluten-free dinner recipe that takes 10 minutes.\\n- Find me a 10-minute gluten-free dinner.\\n- Gluten-free dinner, ready in 10 mins.\\n- What's a super quick celiac-friendly dinner for tonight, done in 10?\\n- 10-minute gluten-free meal.\\n\\n\\u003cdimension\\u003e\\n('Lunch', 'Meaty', 'something quick')\\n\\u003c/dimension\\u003e\\n\\n- I need a quick meaty lunch recipe.\\n- Find me a fast meaty lunch.\\n- Meaty lunch, something quick.\\n- What's a quick meaty lunch idea?\\n- Fast and easy meaty lunch.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 4670,\n",
      "    \"totalTokenCount\": 10946,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 5184\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"aRpgaOOYEeWEvdIPz--V2AE\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"\\u003cdimension\\u003e\\n('Lunch', 'Vegetarian', '30 minutes')\\n\\u003c/dimension\\u003e\\n\\n- I'm looking for a vegetarian lunch recipe that takes about 30 minutes.\\n- Can you find me a quick meat-free lunch, ready in half an hour?\\n- Vegetarian lunch, 30-minute prep.\\n- Need a 30-minute vegetarian lunch idea.\\n- What can I make for lunch, vegetarian, taking around 30 mins?\\n\\n\\u003cdimension\\u003e\\n('Dinner', 'Meaty', 'Under 1 hour')\\n\\u003c/dimension\\u003e\\n\\n- I need a meaty dinner recipe that I can cook in under an hour.\\n- What's a good dinner option with meat that takes less than 60 minutes?\\n- Find me a quick meaty dinner, under 1 hour.\\n- Dinner ideas, meaty, less than 60 mins.\\n- Something meaty for dinner, and I only have an hour.\\n\\n\\u003cdimension\\u003e\\n('Snack', 'Gluten-free', 'Quick')\\n\\u003c/dimension\\u003e\\n\\n- Give me a quick gluten-free snack idea.\\n- I need a fast gluten-free snack.\\n- Gluten-free snack, quick to make.\\n- What's a quick, celiac-friendly snack?\\n- Fast snack, no gluten.\\n\\n\\u003cdimension\\u003e\\n('Dessert', 'Dairy-free', '45 minutes')\\n\\u003c/dimension\\u003e\\n\\n- I'd like a dairy-free dessert that takes about 45 minutes to prepare.\\n- Find me a dessert without dairy, ready in 45 minutes.\\n- Dairy-free dessert, 45-minute cook time.\\n- Looking for a 45-minute, non-dairy sweet treat.\\n- Can you suggest a dairy-free dessert that's done in 45 mins?\\n\\n\\u003cdimension\\u003e\\n('Brunch', 'Vegan', 'I have all the time until retirement')\\n\\u003c/dimension\\u003e\\n\\n- I'm planning a vegan brunch and have plenty of time, no rush at all.\\n- Give me an elaborate vegan brunch recipe; time is not an issue.\\n- Vegan brunch, something that takes all day.\\n- What are some long-form vegan brunch recipes?\\n- I have unlimited time for a vegan brunch.\\n\\n\\u003cdimension\\u003e\\n('Appetizer', 'Low-carb', '15 minutes')\\n\\u003c/dimension\\u003e\\n\\n- I need a low-carb appetizer that's quick, about 15 minutes.\\n- Find me a 15-minute low-carb starter.\\n- Quick low-carb appetizer, done in 15 mins.\\n- 15-minute, keto-friendly app.\\n- Low-carb finger food, fast, 15 min.\\n\\n\\u003cdimension\\u003e\\n('Side Dish', 'High-protein', '20 minutes')\\n\\u003c/dimension\\u003e\\n\\n- What's a good high-protein side dish that takes 20 minutes?\\n- I need a side with lots of protein, ready in 20 minutes.\\n- 20-minute high-protein side.\\n- Quick high-protein accompaniment, 20 mins.\\n- High-protein side dish, around 20 minutes prep.\\n\\n\\u003cdimension\\u003e\\n('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n\\u003c/dimension\\u003e\\n\\n- Give me a light, kid-friendly breakfast recipe, under 20 minutes.\\n- Quick and easy light breakfast for kids, less than 20 mins.\\n- Kid-friendly light breakfast, fast, under 20.\\n- What can I make for a light breakfast for my kids in under 20 minutes?\\n- Fast, light, kid-approved breakfast.\\n\\n\\u003cdimension\\u003e\\n('Holiday meal', 'Pescatarian', 'All day')\\n\\u003c/dimension\\u003e\\n\\n- I'm looking for an all-day pescatarian holiday meal recipe.\\n- Give me a grand pescatarian holiday meal, something that takes all day.\\n- Pescatarian holiday feast, long cook time.\\n- An all-day cooking project for a pescatarian holiday meal.\\n- I want a time-consuming pescatarian holiday dinner.\\n\\n\\u003cdimension\\u003e\\n('Party food', 'Sugar-free', 'About an hour')\\n\\u003c/dimension\\u003e\\n\\n- I need sugar-free party food that takes about an hour to make.\\n- Find me a party snack that's sugar-free and takes roughly an hour.\\n- Sugar-free party recipe, around 60 minutes.\\n- What are some hour-long sugar-free party food ideas?\\n- Party food, no sugar, takes about an hour.\\n\\n\\u003cdimension\\u003e\\n('Quick bite', 'Nut-free', 'Instant')\\n\\u003c/dimension\\u003e\\n\\n- Give me an instant nut-free quick bite.\\n- I need a super fast, nut-free snack.\\n- Instant, nut-free quick snack.\\n- Something quick and nut-free right now.\\n- What's a literally instant nut-free bite?\\n\\n\\u003cdimension\\u003e\\n('Breakfast', 'Keto', '30 minutes')\\n\\u003c/dimension\\u003e\\n\\n- I need a keto breakfast recipe that takes about 30 minutes.\\n- Find me a 30-minute keto breakfast.\\n- Keto breakfast, half an hour prep.\\n- What's a quick keto breakfast I can make in 30 minutes?\\n- 30-min low-carb breakfast.\\n\\n\\u003cdimension\\u003e\\n('Lunch', 'Heart-healthy', 'Under 1 hour')\\n\\u003c/dimension\\u003e\\n\\n- I'm looking for a heart-healthy lunch recipe under an hour.\\n- Give me a heart-healthy lunch that's ready in less than 60 minutes.\\n- Heart-healthy lunch, less than an hour.\\n- Quick heart-healthy lunch, under 60 mins.\\n- What's a good heart-healthy lunch I can make in under an hour?\\n\\n\\u003cdimension\\u003e\\n('Dinner', 'Spicy', '45 minutes')\\n\\u003c/dimension\\u003e\\n\\n- I want a spicy dinner recipe that takes 45 minutes.\\n- Find me a 45-minute spicy dinner.\\n- Spicy dinner, ready in 45 mins.\\n- What's a quick and spicy dinner for tonight, done in 45?\\n- 45-minute fiery dinner.\\n\\n\\u003cdimension\\u003e\\n('Snack', 'without fish or meat', '10 minutes')\\n\\u003c/dimension\\u003e\\n\\n- I need a 10-minute snack without fish or meat.\\n- Quick snack, no animal protein, 10 minutes.\\n- Find me a fast plant-based snack, ready in 10 minutes.\\n- 10-minute snack, no meat or fish.\\n- What's a quick 10-minute vegetarian snack?\\n\\n\\u003cdimension\\u003e\\n('Dessert', 'Meaty', 'Long prep')\\n\\u003c/dimension\\u003e\\n\\n- I'm looking for a meaty dessert that takes a long time to prepare.\\n- Give me a meaty sweet recipe with long prep time.\\n- Meaty dessert, slow cook.\\n- What's a long-prep meaty dessert?\\n- Something meaty for dessert that requires a lot of time.\\n\\n\\u003cdimension\\u003e\\n('Brunch', 'Paleo', 'Quick')\\n\\u003c/dimension\\u003e\\n\\n- I need a quick paleo brunch recipe.\\n- Find me a fast paleo brunch.\\n- Paleo brunch, quick to make.\\n- What's a quick paleo option for brunch?\\n- Fast and easy paleo brunch.\\n\\n\\u003cdimension\\u003e\\n('Appetizer', 'Low-salt', '15 minutes')\\n\\u003c/dimension\\u003e\\n\\n- Give me a 15-minute low-salt appetizer.\\n- I need a quick low-sodium starter, ready in 15 minutes.\\n- Low-salt appetizer, fast, 15 mins.\\n- What's a 15-minute low-sodium finger food?\\n- Quick low-salt app, 15 minutes.\\n\\n\\u003cdimension\\u003e\\n('Side Dish', 'Diabetic-friendly', '20 minutes')\\n\\u003c/dimension\\u003e\\n\\n- I'm looking for a diabetic-friendly side dish that takes 20 minutes.\\n- Find me a 20-minute side for diabetics.\\n- Diabetic-friendly side, ready in 20 minutes.\\n- Quick 20-minute side dish suitable for diabetics.\\n- What's a good diabetic-friendly side that takes about 20 minutes?\\n\\n\\u003cdimension\\u003e\\n('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n\\u003c/dimension\\u003e\\n\\n- I need a high-fiber light breakfast that takes less than 20 minutes.\\n- Give me a quick, high-fiber light breakfast, under 20 minutes.\\n- High-fiber light breakfast, less than 20 mins.\\n- Fast and light high-fiber breakfast, under 20.\\n- What's a quick, high-fiber breakfast for a lighter meal?\\n\\n\\u003cdimension\\u003e\\n('Holiday meal', 'Lactose-free', 'Overnight')\\n\\u003c/dimension\\u003e\\n\\n- I need an overnight lactose-free holiday meal.\\n- Find me a lactose-free holiday recipe that cooks overnight.\\n- Lactose-free holiday meal, slow cooker/overnight.\\n- What's a good lactose-free holiday meal to prepare overnight?\\n- Overnight lactose-free holiday feast.\\n\\n\\u003cdimension\\u003e\\n('Party food', 'Comfort food', 'About an hour')\\n\\u003c/dimension\\u003e\\n\\n- I'm looking for comfort food party recipes that take about an hour.\\n- Give me a comforting party dish that's ready in about 60 minutes.\\n- Comfort food for a party, around an hour.\\n- What are some hour-long comforting party food ideas?\\n- Party food, comfort style, takes about an hour.\\n\\n\\u003cdimension\\u003e\\n('Quick bite', 'with hummus and carrots', 'Instant')\\n\\u003c/dimension\\u003e\\n\\n- Give me an instant quick bite featuring hummus and carrots.\\n- I need a really fast snack with hummus and carrots.\\n- Instant bite, hummus and carrots.\\n- What's a quick snack I can make right now using hummus and carrots?\\n- Super quick bite, hummus and carrots.\\n\\n\\u003cdimension\\u003e\\n('Breakfast', 'Gluten-free', 'Quick and easy')\\n\\u003c/dimension\\u003e\\n\\n- I need a quick and easy gluten-free breakfast.\\n- Find me a fast and simple gluten-free breakfast.\\n- Gluten-free breakfast, quick and easy.\\n- What's an easy, quick gluten-free breakfast?\\n- Simple, celiac-friendly breakfast, fast.\\n\\n\\u003cdimension\\u003e\\n('Lunch', 'Vegan', '30 minutes')\\n\\u003c/dimension\\u003e\\n\\n- I need a vegan lunch recipe that takes 30 minutes.\\n- Find me a 30-minute vegan lunch.\\n- Vegan lunch, half an hour.\\n- What's a quick vegan lunch idea for 30 minutes?\\n- 30-minute plant-based lunch.\\n\\n\\u003cdimension\\u003e\\n('Dinner', 'High-protein', 'Under 1 hour')\\n\\u003c/dimension\\u003e\\n\\n- I'm looking for a high-protein dinner recipe under an hour.\\n- Give me a high-protein dinner that's ready in less than 60 minutes.\\n- High-protein dinner, less than an hour.\\n- Quick high-protein dinner, under 60 mins.\\n- What's a good high-protein dinner I can make in under an hour?\\n\\n\\u003cdimension\\u003e\\n('Snack', 'Not spicy', '10 minutes')\\n\\u003c/dimension\\u003e\\n\\n- I want a quick, non-spicy snack that takes 10 minutes.\\n- Give me a mild snack, ready in 10 minutes.\\n- Not spicy snack, 10-minute prep.\\n- A ten-minute, plain snack.\\n- I need a quick, non-fiery snack under 10 minutes.\\n\\n\\u003cdimension\\u003e\\n('Dessert', 'Vegetarian', '45 minutes')\\n\\u003c/dimension\\u003e\\n\\n- I'd like a vegetarian dessert that takes about 45 minutes to prepare.\\n- Find me a dessert without meat, ready in 45 minutes.\\n- Vegetarian dessert, 45-minute cook time.\\n- Looking for a 45-minute, meat-free sweet treat.\\n- Can you suggest a vegetarian dessert that's done in 45 mins?\\n\\n\\u003cdimension\\u003e\\n('Brunch', 'Meaty', 'I have all the time until retirement')\\n\\u003c/dimension\\u003e\\n\\n- I'm planning a meaty brunch and have plenty of time, no rush at all.\\n- Give me an elaborate meaty brunch recipe; time is not an issue.\\n- Meaty brunch, something that takes all day.\\n- What are some long-form meaty brunch recipes?\\n- I have unlimited time for a meaty brunch.\\n\\n\\u003cdimension\\u003e\\n('Appetizer', 'Dairy-free', '15 minutes')\\n\\u003c/dimension\\u003e\\n\\n- I need a dairy-free appetizer that's quick, about 15 minutes.\\n- Find me a 15-minute dairy-free starter.\\n- Quick dairy-free appetizer, done in 15 mins.\\n- 15-minute, non-dairy app.\\n- Dairy-free finger food, fast, 15 min.\\n\\n\\u003cdimension\\u003e\\n('Side Dish', 'Low-carb', '20 minutes')\\n\\u003c/dimension\\u003e\\n\\n- What's a good low-carb side dish that takes 20 minutes?\\n- I need a side with low carbs, ready in 20 minutes.\\n- 20-minute low-carb side.\\n- Quick low-carb accompaniment, 20 mins.\\n- Low-carb side dish, around 20 minutes prep.\\n\\n\\u003cdimension\\u003e\\n('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n\\u003c/dimension\\u003e\\n\\n- Give me a light, pescatarian breakfast recipe, under 20 minutes.\\n- Quick and easy light pescatarian breakfast, under 20 mins.\\n- Pescatarian light breakfast, fast, under 20.\\n- What can I make for a light pescatarian breakfast in under 20 minutes?\\n- Fast, light, fish-friendly breakfast.\\n\\n\\u003cdimension\\u003e\\n('Holiday meal', 'Kid-friendly', 'All day')\\n\\u003c/dimension\\u003e\\n\\n- I'm looking for an all-day kid-friendly holiday meal recipe.\\n- Give me a grand kid-friendly holiday meal, something that takes all day.\\n- Kid-friendly holiday feast, long cook time.\\n- An all-day cooking project for a holiday meal that kids will love.\\n- I want a time-consuming kid-approved holiday dinner.\\n\\n\\u003cdimension\\u003e\\n('Party food', 'Nut-free', 'About an hour')\\n\\u003c/dimension\\u003e\\n\\n- I need nut-free party food that takes about an hour to make.\\n- Find me a party snack that's nut-free and takes roughly an hour.\\n- Nut-free party recipe, around 60 minutes.\\n- What are some hour-long nut-free party food ideas?\\n- Party food, no nuts, takes about an hour.\\n\\n\\u003cdimension\\u003e\\n('Quick bite', 'Sugar-free', 'Instant')\\n\\u003c/dimension\\u003e\\n\\n- Give me an instant sugar-free quick bite.\\n- I need a super fast, sugar-free snack.\\n- Instant, sugar-free quick snack.\\n- Something quick and sugar-free right now.\\n- What's a literally instant sugar-free bite?\\n\\n\\u003cdimension\\u003e\\n('Breakfast', 'Spicy', 'Quick and easy')\\n\\u003c/dimension\\u003e\\n\\n- I need a quick and easy spicy breakfast.\\n- Find me a fast and simple spicy breakfast.\\n- Spicy breakfast, quick and easy.\\n- What's an easy, quick spicy breakfast?\\n- Simple, fiery breakfast, fast.\\n\\n\\u003cdimension\\u003e\\n('Lunch', 'Keto', '30 minutes')\\n\\u003c/dimension\\u003e\\n\\n- I need a keto lunch recipe that takes 30 minutes.\\n- Find me a 30-minute keto lunch.\\n- Keto lunch, half an hour.\\n- What's a quick keto lunch idea for 30 minutes?\\n- 30-minute low-carb lunch.\\n\\n\\u003cdimension\\u003e\\n('Dinner', 'Heart-healthy', 'Under 1 hour')\\n\\u003c/dimension\\u003e\\n\\n- I'm looking for a heart-healthy dinner recipe under an hour.\\n- Give me a heart-healthy dinner that's ready in less than 60 minutes.\\n- Heart-healthy dinner, less than an hour.\\n- Quick heart-healthy dinner, under 60 mins.\\n- What's a good heart-healthy dinner I can make in under an hour?\\n\\n\\u003cdimension\\u003e\\n('Snack', 'Low-salt', '10 minutes')\\n\\u003c/dimension\\u003e\\n\\n- I want a quick, low-salt snack that takes 10 minutes.\\n- Give me a low-sodium snack, ready in 10 minutes.\\n- Low-salt snack, 10-minute prep.\\n- A ten-minute, low-sodium snack.\\n- I need a quick, low-salt snack under 10 minutes.\\n\\n\\u003cdimension\\u003e\\n('Dessert', 'Diabetic-friendly', '45 minutes')\\n\\u003c/dimension\\u003e\\n\\n- I'd like a diabetic-friendly dessert that takes about 45 minutes to prepare.\\n- Find me a dessert suitable for diabetics, ready in 45 minutes.\\n- Diabetic-friendly dessert, 45-minute cook time.\\n- Looking for a 45-minute, diabetic-safe sweet treat.\\n- Can you suggest a dessert for diabetics that's done in 45 mins?\\n\\n\\u003cdimension\\u003e\\n('Brunch', 'High-fiber', 'Long prep')\\n\\u003c/dimension\\u003e\\n\\n- I'm planning a high-fiber brunch that takes a long time to prepare.\\n- Give me an elaborate high-fiber brunch recipe; it can take a while.\\n- High-fiber brunch, slow cooking.\\n- What are some long-prep high-fiber brunch recipes?\\n- I have plenty of time for a high-fiber brunch.\\n\\n\\u003cdimension\\u003e\\n('Appetizer', 'Lactose-free', '15 minutes')\\n\\u003c/dimension\\u003e\\n\\n- I need a lactose-free appetizer that's quick, about 15 minutes.\\n- Find me a 15-minute lactose-free starter.\\n- Quick lactose-free appetizer, done in 15 mins.\\n- 15-minute, lactose-free app.\\n- Lactose-free finger food, fast, 15 min.\\n\\n\\u003cdimension\\u003e\\n('Side Dish', 'Comfort food', '20 minutes')\\n\\u003c/dimension\\u003e\\n\\n- What's a good comfort food side dish that takes 20 minutes?\\n- I need a comforting side, ready in 20 minutes.\\n- 20-minute comfort food side.\\n- Quick comforting accompaniment, 20 mins.\\n- Comfort food side dish, around 20 minutes prep.\\n\\n\\u003cdimension\\u003e\\n('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n\\u003c/dimension\\u003e\\n\\n- Give me a light, paleo breakfast recipe, under 20 minutes.\\n- Quick and easy light paleo breakfast, less than 20 mins.\\n- Paleo light breakfast, fast, under 20.\\n- What can I make for a light paleo breakfast in under 20 minutes?\\n- Fast, light, paleo-approved breakfast.\\n\\n\\u003cdimension\\u003e\\n('Holiday meal', 'with hummus and carrots', 'Overnight')\\n\\u003c/dimension\\u003e\\n\\n- I need an overnight holiday meal featuring hummus and carrots.\\n- Find me a holiday recipe with hummus and carrots that cooks overnight.\\n- Holiday meal, overnight, with hummus and carrots.\\n- What's a good holiday meal to prepare overnight, including hummus and carrots?\\n- Overnight holiday feast with hummus and carrots.\\n\\n\\u003cdimension\\u003e\\n('Party food', 'Not spicy', 'About an hour')\\n\\u003c/dimension\\u003e\\n\\n- I need non-spicy party food that takes about an hour to make.\\n- Find me a party snack that's mild and takes roughly an hour.\\n- Not spicy party recipe, around 60 minutes.\\n- What are some hour-long mild party food ideas?\\n- Party food, not spicy, takes about an hour.\\n\\n\\u003cdimension\\u003e\\n('Quick bite', 'Vegetarian', 'Instant')\\n\\u003c/dimension\\u003e\\n\\n- Give me an instant vegetarian quick bite.\\n- I need a super fast, vegetarian snack.\\n- Instant, vegetarian quick snack.\\n- Something quick and vegetarian right now.\\n- What's a literally instant vegetarian bite?\\n\\n\\u003cdimension\\u003e\\n('Dinner', 'Gluten-free', '10 minutes')\\n\\u003c/dimension\\u003e\\n\\n- I want a gluten-free dinner recipe that takes 10 minutes.\\n- Find me a 10-minute gluten-free dinner.\\n- Gluten-free dinner, ready in 10 mins.\\n- What's a super quick celiac-friendly dinner for tonight, done in 10?\\n- 10-minute gluten-free meal.\\n\\n\\u003cdimension\\u003e\\n('Lunch', 'Meaty', 'something quick')\\n\\u003c/dimension\\u003e\\n\\n- I need a quick meaty lunch recipe.\\n- Find me a fast meaty lunch.\\n- Meaty lunch, something quick.\\n- What's a quick meaty lunch idea?\\n- Fast and easy meaty lunch.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 4670,\n",
      "    \"totalTokenCount\": 10946,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 5184\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"aRpgaOOYEeWEvdIPz--V2AE\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:38:01 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:38:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:01 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:01 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:01 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.024962600000000005\n",
      "DEBUG:LiteLLM:response_cost: 0.024962600000000005\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:38:01 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:38:01 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:38:01 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:38:01 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:38:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:38:01 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:38:01 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:01 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:38:01 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:38:01 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:38:01 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:38:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:38:01 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1456d13a0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1457b3950> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1456d0380>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:38:05 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=4378'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:38:05 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in about 30 minutes.\\n- What are some quick vegetarian lunch ideas for a 30-minute prep time?\\n- Suggest a meat-free lunch that takes 30 minutes or less.\\n- Can you give me a recipe for a vegetarian lunch that's done in half an hour?\\n- Need a 30-minute vegetarian lunch.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 90,\n",
      "    \"totalTokenCount\": 1381,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 199\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"bRpgaPO1Os64nsEPq_jnUA\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in about 30 minutes.\\n- What are some quick vegetarian lunch ideas for a 30-minute prep time?\\n- Suggest a meat-free lunch that takes 30 minutes or less.\\n- Can you give me a recipe for a vegetarian lunch that's done in half an hour?\\n- Need a 30-minute vegetarian lunch.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 90,\n",
      "    \"totalTokenCount\": 1381,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 199\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"bRpgaPO1Os64nsEPq_jnUA\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:38:05 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:38:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:05 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0010501\n",
      "DEBUG:LiteLLM:response_cost: 0.0010501\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:38:05 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:38:05 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:38:05 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:38:05 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:38:05 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:38:05 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:38:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:05 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:38:05 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:38:05 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:38:05 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:38:05 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:38:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:05 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1456d0e30>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1457b3150> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x141499ca0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:38:08 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=2649'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:38:08 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in about 30 minutes.\\n- What can I cook for a vegetarian lunch in 30 minutes?\\n- Vegetarian lunch, half an hour.\\n- Need a 30-minute vegetarian lunch.\\n- Show me some quick vegetarian lunch ideas that take around 30 minutes.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 75,\n",
      "    \"totalTokenCount\": 1605,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 438\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"cBpgaNmWLJnk7M8P8ejLwQQ\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in about 30 minutes.\\n- What can I cook for a vegetarian lunch in 30 minutes?\\n- Vegetarian lunch, half an hour.\\n- Need a 30-minute vegetarian lunch.\\n- Show me some quick vegetarian lunch ideas that take around 30 minutes.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 75,\n",
      "    \"totalTokenCount\": 1605,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 438\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"cBpgaNmWLJnk7M8P8ejLwQQ\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:38:08 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:38:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:08 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0016101\n",
      "DEBUG:LiteLLM:response_cost: 0.0016101\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:38:08 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:38:08 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:38:08 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:38:08 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:38:08 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:38:08 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:38:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:08 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:38:08 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:38:08 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:38:08 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:38:08 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:38:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:08 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1456d32f0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1457a8250> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14149b530>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:38:12 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=3839'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:38:12 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a vegetarian lunch recipe that's ready in 30 minutes.\\n- What can I make for a quick vegetarian lunch in half an hour?\\n- Show me some 30-minute vegetarian lunch ideas.\\n- Give me a meat-free lunch recipe, max 30 mins prep time.\\n- I'm looking for a vegetarian lunch that takes around 30 minutes to cook.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 86,\n",
      "    \"totalTokenCount\": 1672,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 494\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"dBpgaI3xKOWWvdIPsqv1sAg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a vegetarian lunch recipe that's ready in 30 minutes.\\n- What can I make for a quick vegetarian lunch in half an hour?\\n- Show me some 30-minute vegetarian lunch ideas.\\n- Give me a meat-free lunch recipe, max 30 mins prep time.\\n- I'm looking for a vegetarian lunch that takes around 30 minutes to cook.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 86,\n",
      "    \"totalTokenCount\": 1672,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 494\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"dBpgaI3xKOWWvdIPsqv1sAg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:38:12 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:38:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:12 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:12 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:12 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:12 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0017776000000000003\n",
      "DEBUG:LiteLLM:response_cost: 0.0017776000000000003\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:38:12 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:38:12 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:38:12 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:38:12 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:38:12 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:38:12 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:38:12 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:12 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:38:12 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:38:12 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:38:12 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:38:12 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:38:12 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:12 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x135e0bc20>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1457a8bd0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x13602be00>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:38:16 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=3942'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:38:16 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in 30 minutes.\\n- What are some good vegetarian lunch ideas for a 30-minute cook time?\\n- Give me a quick vegetarian lunch, ready in half an hour.\\n- I need a 30-minute vegetarian lunch.\\n- Can you find me a vegetarian lunch recipe that takes around 30 minutes?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 85,\n",
      "    \"totalTokenCount\": 1380,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 203\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"eBpgaM2fMMf2xN8P19C44AU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in 30 minutes.\\n- What are some good vegetarian lunch ideas for a 30-minute cook time?\\n- Give me a quick vegetarian lunch, ready in half an hour.\\n- I need a 30-minute vegetarian lunch.\\n- Can you find me a vegetarian lunch recipe that takes around 30 minutes?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 85,\n",
      "    \"totalTokenCount\": 1380,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 203\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"eBpgaM2fMMf2xN8P19C44AU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:38:16 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:38:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:16 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0010476\n",
      "DEBUG:LiteLLM:response_cost: 0.0010476\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:38:16 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:38:16 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:38:16 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:38:16 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:38:16 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:38:16 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:38:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:16 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:38:16 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:38:16 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:38:16 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:38:16 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:38:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:16 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1456d2f30>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1457b29d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1456d0ce0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:38:18 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=1511'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:38:18 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in 30 minutes.\\n- What can I cook for a quick vegetarian lunch, ready in half an hour?\\n- Give me some ideas for a 30-minute vegetarian lunch.\\n- I need a fast, meat-free lunch that takes no more than 30 minutes.\\n- Vegetarian lunch, ready in 30 mins, please.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 89,\n",
      "    \"totalTokenCount\": 1290,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 109\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"ehpgaKG_HO2kvdIP3-n8oAg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in 30 minutes.\\n- What can I cook for a quick vegetarian lunch, ready in half an hour?\\n- Give me some ideas for a 30-minute vegetarian lunch.\\n- I need a fast, meat-free lunch that takes no more than 30 minutes.\\n- Vegetarian lunch, ready in 30 mins, please.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 89,\n",
      "    \"totalTokenCount\": 1290,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 109\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"ehpgaKG_HO2kvdIP3-n8oAg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:38:18 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:38:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:18 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0008225999999999999\n",
      "DEBUG:LiteLLM:response_cost: 0.0008225999999999999\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:38:18 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:38:18 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:38:18 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:38:18 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:38:18 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:38:18 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:38:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:18 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:38:18 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:38:18 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:38:18 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:38:18 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:38:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:18 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1456d35f0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1457b7350> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1456d2900>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:38:21 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=2442'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:38:20 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in about 30 minutes.\\n- What are some quick vegetarian lunch ideas, around half an hour prep time?\\n- Suggest a 30-minute vegetarian lunch.\\n- I need a speedy vegetarian lunch, ideally done in 30 minutes.\\n- Vegetarian lunch, 30 minutes, please.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 79,\n",
      "    \"totalTokenCount\": 1408,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 237\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"fRpgaPHKAcvcxs0PiPW0oAg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in about 30 minutes.\\n- What are some quick vegetarian lunch ideas, around half an hour prep time?\\n- Suggest a 30-minute vegetarian lunch.\\n- I need a speedy vegetarian lunch, ideally done in 30 minutes.\\n- Vegetarian lunch, 30 minutes, please.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 79,\n",
      "    \"totalTokenCount\": 1408,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 237\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"fRpgaPHKAcvcxs0PiPW0oAg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:38:20 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:38:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:20 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:20 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:20 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:20 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0011176\n",
      "DEBUG:LiteLLM:response_cost: 0.0011176\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:38:20 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:38:20 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:38:20 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:38:20 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:38:20 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:38:20 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:38:20 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:20 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:38:20 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:38:20 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:38:20 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:38:20 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:38:20 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:20 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1457000b0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1457b51d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145700bf0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:38:26 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=4960'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:38:26 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in 30 minutes.\\n- What are some quick vegetarian lunch ideas under 30 minutes?\\n- I need a vegetarian lunch, ready in half an hour.\\n- Give me a 30-minute vegetarian lunch.\\n- Suggest a quick and easy vegetarian lunch that takes about 30 minutes to prepare.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 81,\n",
      "    \"totalTokenCount\": 1281,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 108\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"ghpgaIfNB42Cxs0P9-Cs8AU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in 30 minutes.\\n- What are some quick vegetarian lunch ideas under 30 minutes?\\n- I need a vegetarian lunch, ready in half an hour.\\n- Give me a 30-minute vegetarian lunch.\\n- Suggest a quick and easy vegetarian lunch that takes about 30 minutes to prepare.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 81,\n",
      "    \"totalTokenCount\": 1281,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 108\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"ghpgaIfNB42Cxs0P9-Cs8AU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:38:26 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:38:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:26 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:26 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:26 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:26 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0008001\n",
      "DEBUG:LiteLLM:response_cost: 0.0008001\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:38:26 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:38:26 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:38:26 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:38:26 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:38:26 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:38:26 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:38:26 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:26 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:38:26 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:38:26 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:38:26 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:38:26 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:38:26 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:26 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14149a570>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1457b5cd0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x135e40710>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:38:28 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=2649'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:38:28 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in 30 minutes.\\n- What can I cook for a quick vegetarian lunch, ready in half an hour?\\n- Suggest some vegetarian lunch ideas that take around 30 minutes.\\n- Help me find a 30-minute vegetarian lunch.\\n- I need a fast vegetarian lunch recipe, 30 mins max.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 83,\n",
      "    \"totalTokenCount\": 1394,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 219\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"hBpgaLb9NtSCvdIP3cflqQg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in 30 minutes.\\n- What can I cook for a quick vegetarian lunch, ready in half an hour?\\n- Suggest some vegetarian lunch ideas that take around 30 minutes.\\n- Help me find a 30-minute vegetarian lunch.\\n- I need a fast vegetarian lunch recipe, 30 mins max.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 83,\n",
      "    \"totalTokenCount\": 1394,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 219\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"hBpgaLb9NtSCvdIP3cflqQg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:38:28 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:38:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:28 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:28 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:28 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:28 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0010826\n",
      "DEBUG:LiteLLM:response_cost: 0.0010826\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:38:28 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:38:28 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:38:28 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:38:28 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:38:28 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:38:28 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:38:28 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:28 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:38:28 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:38:28 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:38:28 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:38:28 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:38:28 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:28 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145703080>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1457b45d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145702540>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:38:32 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=3643'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:38:32 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch that takes around 30 minutes to make.\\n- What are some good vegetarian lunch ideas for a 30-minute prep time?\\n- Give me a quick vegetarian lunch recipe that can be ready in half an hour.\\n- I need a lunch that's vegetarian and only takes 30 minutes.\\n- Quick vegetarian lunch, 30 minutes max.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 86,\n",
      "    \"totalTokenCount\": 1360,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 182\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"iBpgaOqKKeyfvdIPwrbo-QU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch that takes around 30 minutes to make.\\n- What are some good vegetarian lunch ideas for a 30-minute prep time?\\n- Give me a quick vegetarian lunch recipe that can be ready in half an hour.\\n- I need a lunch that's vegetarian and only takes 30 minutes.\\n- Quick vegetarian lunch, 30 minutes max.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 86,\n",
      "    \"totalTokenCount\": 1360,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 182\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"iBpgaOqKKeyfvdIPwrbo-QU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:38:32 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:38:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:32 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0009976\n",
      "DEBUG:LiteLLM:response_cost: 0.0009976\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:38:32 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:38:32 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:38:32 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:38:32 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:38:32 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:38:32 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:38:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:32 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:38:32 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:38:32 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:38:32 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:38:32 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:38:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:32 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x13628db20>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1457b41d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x135e40710>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:38:34 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=1700'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:38:34 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch that takes around 30 minutes.\\n- Can you suggest some quick vegetarian lunch ideas? I have half an hour.\\n- A 30-minute vegetarian lunch, please.\\n- I need a vegetarian lunch recipe that I can make in 30 minutes.\\n- What's a good vegetarian lunch that takes about 30 minutes to prepare?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 84,\n",
      "    \"totalTokenCount\": 1320,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 144\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"ihpgaLKaHou8xN8PloHViQY\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch that takes around 30 minutes.\\n- Can you suggest some quick vegetarian lunch ideas? I have half an hour.\\n- A 30-minute vegetarian lunch, please.\\n- I need a vegetarian lunch recipe that I can make in 30 minutes.\\n- What's a good vegetarian lunch that takes about 30 minutes to prepare?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 84,\n",
      "    \"totalTokenCount\": 1320,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 144\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"ihpgaLKaHou8xN8PloHViQY\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:38:34 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:38:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0008975999999999999\n",
      "DEBUG:LiteLLM:response_cost: 0.0008975999999999999\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:38:34 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:38:34 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:38:34 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:38:34 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:38:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:38:34 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:38:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:34 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:38:34 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:38:34 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:38:34 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:38:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:38:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145700980>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1457b52d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145700620>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:38:39 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=4694'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:38:39 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in 30 minutes.\\n- Can you suggest a quick vegetarian lunch, done in half an hour?\\n- Give me some ideas for a 30-minute vegetarian lunch.\\n- I need a vegetarian lunch that's ready in 30 minutes or less.\\n- Quick vegetarian lunch, 30 min.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 82,\n",
      "    \"totalTokenCount\": 1614,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 440\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"jxpgaOmJFMn7xs0PtNnzoQg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in 30 minutes.\\n- Can you suggest a quick vegetarian lunch, done in half an hour?\\n- Give me some ideas for a 30-minute vegetarian lunch.\\n- I need a vegetarian lunch that's ready in 30 minutes or less.\\n- Quick vegetarian lunch, 30 min.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 82,\n",
      "    \"totalTokenCount\": 1614,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 440\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"jxpgaOmJFMn7xs0PtNnzoQg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:38:39 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:38:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:39 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:39 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:39 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:39 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0016326000000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.0016326000000000001\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:38:39 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:38:39 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:38:39 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:38:39 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:38:39 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:38:39 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:38:39 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:39 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:38:39 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:38:39 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:38:39 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:38:39 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:38:39 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:39 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14149b0b0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x145753ad0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14149a270>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Type', b'application/json; charset=UTF-8'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:38:39 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=156'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 429 Too Many Requests\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:38:39 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2268 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "DEBUG:LiteLLM:Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m18:38:39 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2165 - Logging Details LiteLLM-Failure Call: []\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Failure Call: []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 2nd time calling it.\n",
      "\u001b[92m18:38:43 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:38:43 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:38:43 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:38:43 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:38:43 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:38:43 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:38:43 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:43 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:38:43 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:38:43 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:38:43 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:38:43 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:38:43 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:43 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1362cbfe0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1457b0fd0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14149b9e0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Type', b'application/json; charset=UTF-8'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:38:43 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=47'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 429 Too Many Requests\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:38:43 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2268 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "DEBUG:LiteLLM:Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m18:38:43 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2165 - Logging Details LiteLLM-Failure Call: []\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Failure Call: []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 3rd time calling it.\n",
      "\u001b[92m18:38:46 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:38:46 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:38:46 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:38:46 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:38:46 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:38:46 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:38:46 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:46 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:38:46 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:38:46 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:38:46 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:38:46 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:38:46 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:46 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x141498b60>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1457b4450> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x141499070>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:38:48 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=1778'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:38:48 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in 30 minutes.\\n- What's a good vegetarian lunch that takes about half an hour to prepare?\\n- Give me some quick vegetarian lunch ideas, around 30 minutes.\\n- I need a vegetarian meal for lunch, ready in 30 minutes.\\n- Can you suggest a 30-minute vegetarian recipe for lunch?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 87,\n",
      "    \"totalTokenCount\": 1359,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 180\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"mBpgaLudKf-exN8PzJzu8QU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that I can make in 30 minutes.\\n- What's a good vegetarian lunch that takes about half an hour to prepare?\\n- Give me some quick vegetarian lunch ideas, around 30 minutes.\\n- I need a vegetarian meal for lunch, ready in 30 minutes.\\n- Can you suggest a 30-minute vegetarian recipe for lunch?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 87,\n",
      "    \"totalTokenCount\": 1359,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 180\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"mBpgaLudKf-exN8PzJzu8QU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:38:48 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:38:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:48 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0009951\n",
      "DEBUG:LiteLLM:response_cost: 0.0009951\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:38:48 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:38:48 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:38:48 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:38:48 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:38:48 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:38:48 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:38:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:48 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:38:48 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:38:48 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:38:48 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:38:48 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:38:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:48 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x141498800>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x145651d50> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1456d26f0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:38:50 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=1486'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:38:50 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that takes about 30 minutes.\\n- What can I make for lunch in 30 minutes that's vegetarian?\\n- Give me some quick vegetarian lunch ideas, around half an hour prep time.\\n- I need a vegetarian lunch recipe, 30-minute max.\\n- Suggest a 30-minute vegetarian lunch.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 81,\n",
      "    \"totalTokenCount\": 1287,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 114\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"mhpgaJLDEvHi7M8Ph4Sa6Ak\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian lunch recipe that takes about 30 minutes.\\n- What can I make for lunch in 30 minutes that's vegetarian?\\n- Give me some quick vegetarian lunch ideas, around half an hour prep time.\\n- I need a vegetarian lunch recipe, 30-minute max.\\n- Suggest a 30-minute vegetarian lunch.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 81,\n",
      "    \"totalTokenCount\": 1287,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 114\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"mhpgaJLDEvHi7M8Ph4Sa6Ak\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:38:50 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:38:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:50 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:50 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:50 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:50 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0008151\n",
      "DEBUG:LiteLLM:response_cost: 0.0008151\n",
      "DEBUG:__main__:Starting call to '__main__.call_llm', this is the 1st time calling it.\n",
      "\u001b[92m18:38:50 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:38:50 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:38:50 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m18:38:50 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m18:38:50 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:38:50 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:38:50 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m18:38:50 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m18:38:50 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m18:38:50 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:38:50 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m18:38:50 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m18:38:50 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:50 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        Here are 50 diverse tuple combinations of the dimensions:\\n\\n1.  ('Breakfast', 'Not spicy', '10 minutes')\\n2.  ('Lunch', 'Vegetarian', '30 minutes')\\n3.  ('Dinner', 'Meaty', 'Under 1 hour')\\n4.  ('Snack', 'Gluten-free', 'Quick')\\n5.  ('Dessert', 'Dairy-free', '45 minutes')\\n6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\\n7.  ('Appetizer', 'Low-carb', '15 minutes')\\n8.  ('Side Dish', 'High-protein', '20 minutes')\\n9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n10. ('Holiday meal', 'Pescatarian', 'All day')\\n11. ('Party food', 'Sugar-free', 'About an hour')\\n12. ('Quick bite', 'Nut-free', 'Instant')\\n13. ('Breakfast', 'Keto', '30 minutes')\\n14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n15. ('Dinner', 'Spicy', '45 minutes')\\n16. ('Snack', 'without fish or meat', '10 minutes')\\n17. ('Dessert', 'Meaty', 'Long prep')\\n18. ('Brunch', 'Paleo', 'Quick')\\n19. ('Appetizer', 'Low-salt', '15 minutes')\\n20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n22. ('Holiday meal', 'Lactose-free', 'Overnight')\\n23. ('Party food', 'Comfort food', 'About an hour')\\n24. ('Quick bite', 'with hummus and carrots', 'Instant')\\n25. ('Breakfast', 'Gluten-free', 'Quick and easy')\\n26. ('Lunch', 'Vegan', '30 minutes')\\n27. ('Dinner', 'High-protein', 'Under 1 hour')\\n28. ('Snack', 'Not spicy', '10 minutes')\\n29. ('Dessert', 'Vegetarian', '45 minutes')\\n30. ('Brunch', 'Meaty', 'I have all the time until retirement')\\n31. ('Appetizer', 'Dairy-free', '15 minutes')\\n32. ('Side Dish', 'Low-carb', '20 minutes')\\n33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n34. ('Holiday meal', 'Kid-friendly', 'All day')\\n35. ('Party food', 'Nut-free', 'About an hour')\\n36. ('Quick bite', 'Sugar-free', 'Instant')\\n37. ('Breakfast', 'Spicy', 'Quick and easy')\\n38. ('Lunch', 'Keto', '30 minutes')\\n39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n40. ('Snack', 'Low-salt', '10 minutes')\\n41. ('Dessert', 'Diabetic-friendly', '45 minutes')\\n42. ('Brunch', 'High-fiber', 'Long prep')\\n43. ('Appetizer', 'Lactose-free', '15 minutes')\\n44. ('Side Dish', 'Comfort food', '20 minutes')\\n45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n47. ('Party food', 'Not spicy', 'About an hour')\\n48. ('Quick bite', 'Vegetarian', 'Instant')\\n49. ('Dinner', 'Gluten-free', '10 minutes')\\n50. ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1456d39e0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1457b1c50> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x13602be00>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 16:38:55 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=5474'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m18:38:55 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a vegetarian lunch recipe that I can prepare in 30 minutes.\\n- What are some good vegetarian lunch ideas for a 30-minute cook time?\\n- Give me a quick vegetarian lunch, ready in half an hour.\\n- I'm looking for a vegetarian meal for lunch, should take about 30 minutes to make.\\n- Can you suggest a 30-minute vegetarian lunch?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 88,\n",
      "    \"totalTokenCount\": 1418,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 238\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"nxpgaK74NqiqnsEPg97GqAE\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a vegetarian lunch recipe that I can prepare in 30 minutes.\\n- What are some good vegetarian lunch ideas for a 30-minute cook time?\\n- Give me a quick vegetarian lunch, ready in half an hour.\\n- I'm looking for a vegetarian meal for lunch, should take about 30 minutes to make.\\n- Can you suggest a 30-minute vegetarian lunch?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 1092,\n",
      "    \"candidatesTokenCount\": 88,\n",
      "    \"totalTokenCount\": 1418,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 1092\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 238\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"nxpgaK74NqiqnsEPg97GqAE\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m18:38:55 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m18:38:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m18:38:55 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:55 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m18:38:55 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m18:38:55 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0011426000000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.0011426000000000001\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "tuple_to_queries = defaultdict(list)\n",
    "for dimention in dimention_tuples:\n",
    "    query = call_llm(dimention)\n",
    "    tuple_to_queries[dimention].append(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tuple_to_queries)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

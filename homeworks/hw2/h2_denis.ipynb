{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import litellm\n",
    "from typing import Final\n",
    "from dotenv import load_dotenv\n",
    "from textwrap import dedent, wrap\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "\n",
    "\n",
    "num_dimention_instances: Final[int] = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gemini/gemini-2.5-flash'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME: Final[str] = os.environ[\"MODEL_NAME\"]\n",
    "MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('gemini-2.5-flash', 'gemini', None, None)\n",
      "['temperature', 'top_p', 'max_tokens', 'max_completion_tokens', 'stream', 'tools', 'tool_choice', 'functions', 'response_format', 'n', 'stop', 'logprobs', 'frequency_penalty', 'modalities', 'parallel_tool_calls', 'web_search_options', 'reasoning_effort', 'thinking']\n"
     ]
    }
   ],
   "source": [
    "from litellm import get_supported_openai_params\n",
    "\n",
    "params = get_supported_openai_params(model=MODEL_NAME)\n",
    "\n",
    "custom_llm_provider = litellm.get_llm_provider(model=MODEL_NAME)\n",
    "\n",
    "print(custom_llm_provider)\n",
    "\n",
    "assert \"response_format\" in params\n",
    "\n",
    "print(params)\n",
    "\n",
    "from litellm import supports_response_schema\n",
    "\n",
    "assert supports_response_schema(model=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dimention_list(dimention_list: str):\n",
    "    SYSTEM_PROMPT = dedent(f\"\"\"\n",
    "        You are AI assistant that helps generate instances for dimentions of the queries that will be sent to the recipe chatbot. A dimention describes aspects of the query that that the users send to the chatbot. \n",
    "        Given following dimention types, generate tuple combinations of the dimentions\n",
    "\n",
    "        Dimentions:\n",
    "            meal_type: The type of meal the user is interested in, such as 'Lite breakfast', 'Brunch'\n",
    "            dietary_restriction: The dietary restrictions the user has, such as 'Not spicy', 'without fish or meat', 'meaty'\n",
    "            preparation_time: The preparation time the user is interested in, such as '10 minutes', 'something quick', 'under 1 hour', 'I haveall the time until retirement'\n",
    "\n",
    "        Instructions:\n",
    "            - Generate  {num_dimention_instances} tuple combinations of the dimentions.\n",
    "            - Avoid duplicates. \n",
    "            - Make sure that the tuples are diverse. \n",
    "\n",
    "        Example output:\n",
    "        - ('Lite breakfast', 'Not spicy', '10 minutes')\n",
    "        - ('Snack', 'with hummus and carrots', 'instant')\n",
    "    \"\"\")\n",
    "    completion = litellm.completion(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": SYSTEM_PROMPT}\n",
    "        ],\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 50 diverse tuple combinations of the dimensions:\n",
      "\n",
      "1.  ('Breakfast', 'Not spicy', '10 minutes')\n",
      "2.  ('Lunch', 'Vegetarian', '30 minutes')\n",
      "3.  ('Dinner', 'Meaty', 'Under 1 hour')\n",
      "4.  ('Snack', 'Gluten-free', 'Quick')\n",
      "5.  ('Dessert', 'Dairy-free', '45 minutes')\n",
      "6.  ('Brunch', 'Vegan', 'I have all the time until retirement')\n",
      "7.  ('Appetizer', 'Low-carb', '15 minutes')\n",
      "8.  ('Side Dish', 'High-protein', '20 minutes')\n",
      "9.  ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\n",
      "10. ('Holiday meal', 'Pescatarian', 'All day')\n",
      "11. ('Party food', 'Sugar-free', 'About an hour')\n",
      "12. ('Quick bite', 'Nut-free', 'Instant')\n",
      "13. ('Breakfast', 'Keto', '30 minutes')\n",
      "14. ('Lunch', 'Heart-healthy', 'Under 1 hour')\n",
      "15. ('Dinner', 'Spicy', '45 minutes')\n",
      "16. ('Snack', 'without fish or meat', '10 minutes')\n",
      "17. ('Dessert', 'Meaty', 'Long prep')\n",
      "18. ('Brunch', 'Paleo', 'Quick')\n",
      "19. ('Appetizer', 'Low-salt', '15 minutes')\n",
      "20. ('Side Dish', 'Diabetic-friendly', '20 minutes')\n",
      "21. ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\n",
      "22. ('Holiday meal', 'Lactose-free', 'Overnight')\n",
      "23. ('Party food', 'Comfort food', 'About an hour')\n",
      "24. ('Quick bite', 'with hummus and carrots', 'Instant')\n",
      "25. ('Breakfast', 'Gluten-free', 'Quick and easy')\n",
      "26. ('Lunch', 'Vegan', '30 minutes')\n",
      "27. ('Dinner', 'High-protein', 'Under 1 hour')\n",
      "28. ('Snack', 'Not spicy', '10 minutes')\n",
      "29. ('Dessert', 'Vegetarian', '45 minutes')\n",
      "30. ('Brunch', 'Meaty', 'I have all the time until retirement')\n",
      "31. ('Appetizer', 'Dairy-free', '15 minutes')\n",
      "32. ('Side Dish', 'Low-carb', '20 minutes')\n",
      "33. ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\n",
      "34. ('Holiday meal', 'Kid-friendly', 'All day')\n",
      "35. ('Party food', 'Nut-free', 'About an hour')\n",
      "36. ('Quick bite', 'Sugar-free', 'Instant')\n",
      "37. ('Breakfast', 'Spicy', 'Quick and easy')\n",
      "38. ('Lunch', 'Keto', '30 minutes')\n",
      "39. ('Dinner', 'Heart-healthy', 'Under 1 hour')\n",
      "40. ('Snack', 'Low-salt', '10 minutes')\n",
      "41. ('Dessert', 'Diabetic-friendly', '45 minutes')\n",
      "42. ('Brunch', 'High-fiber', 'Long prep')\n",
      "43. ('Appetizer', 'Lactose-free', '15 minutes')\n",
      "44. ('Side Dish', 'Comfort food', '20 minutes')\n",
      "45. ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\n",
      "46. ('Holiday meal', 'with hummus and carrots', 'Overnight')\n",
      "47. ('Party food', 'Not spicy', 'About an hour')\n",
      "48. ('Quick bite', 'Vegetarian', 'Instant')\n",
      "49. ('Dinner', 'Gluten-free', '10 minutes')\n",
      "50. ('Lunch', 'Meaty', 'something quick')\n"
     ]
    }
   ],
   "source": [
    "dimention_list = generate_dimention_list(\"\")\n",
    "\n",
    "print(dimention_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimention_tuples = [\n",
    "    ('Breakfast', 'Not spicy', '10 minutes'),\n",
    "    ('Lunch', 'Vegetarian', '30 minutes'),\n",
    "    ('Dinner', 'Meaty', 'Under 1 hour'),\n",
    "    ('Snack', 'Gluten-free', 'Quick'),\n",
    "    ('Dessert', 'Dairy-free', '45 minutes'),\n",
    "    ('Brunch', 'Vegan', 'I have all the time until retirement'),\n",
    "    ('Appetizer', 'Low-carb', '15 minutes'),\n",
    "    ('Side Dish', 'High-protein', '20 minutes'),\n",
    "    ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes'),\n",
    "    ('Holiday meal', 'Pescatarian', 'All day'),\n",
    "    ('Party food', 'Sugar-free', 'About an hour'),\n",
    "    ('Quick bite', 'Nut-free', 'Instant'),\n",
    "    ('Breakfast', 'Keto', '30 minutes'),\n",
    "    ('Lunch', 'Heart-healthy', 'Under 1 hour'),\n",
    "    ('Dinner', 'Spicy', '45 minutes'),\n",
    "    ('Snack', 'without fish or meat', '10 minutes'),\n",
    "    ('Dessert', 'Meaty', 'Long prep'),\n",
    "    ('Brunch', 'Paleo', 'Quick'),\n",
    "    ('Appetizer', 'Low-salt', '15 minutes'),\n",
    "    ('Side Dish', 'Diabetic-friendly', '20 minutes'),\n",
    "    ('Lite breakfast', 'High-fiber', 'Less than 20 minutes'),\n",
    "    ('Holiday meal', 'Lactose-free', 'Overnight'),\n",
    "    ('Party food', 'Comfort food', 'About an hour'),\n",
    "    ('Quick bite', 'with hummus and carrots', 'Instant'),\n",
    "    ('Breakfast', 'Gluten-free', 'Quick and easy'),\n",
    "    ('Lunch', 'Vegan', '30 minutes'),\n",
    "    ('Dinner', 'High-protein', 'Under 1 hour'),\n",
    "    ('Snack', 'Not spicy', '10 minutes'),\n",
    "    ('Dessert', 'Vegetarian', '45 minutes'),\n",
    "    ('Brunch', 'Meaty', 'I have all the time until retirement'),\n",
    "    ('Appetizer', 'Dairy-free', '15 minutes'),\n",
    "    ('Side Dish', 'Low-carb', '20 minutes'),\n",
    "    ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes'),\n",
    "    ('Holiday meal', 'Kid-friendly', 'All day'),\n",
    "    ('Party food', 'Nut-free', 'About an hour'),\n",
    "    ('Quick bite', 'Sugar-free', 'Instant'),\n",
    "    ('Breakfast', 'Spicy', 'Quick and easy'),\n",
    "    ('Lunch', 'Keto', '30 minutes'),\n",
    "    ('Dinner', 'Heart-healthy', 'Under 1 hour'),\n",
    "    ('Snack', 'Low-salt', '10 minutes'),\n",
    "    ('Dessert', 'Diabetic-friendly', '45 minutes'),\n",
    "    ('Brunch', 'High-fiber', 'Long prep'),\n",
    "    ('Appetizer', 'Lactose-free', '15 minutes'),\n",
    "    ('Side Dish', 'Comfort food', '20 minutes'),\n",
    "    ('Lite breakfast', 'Paleo', 'Less than 20 minutes'),\n",
    "    ('Holiday meal', 'with hummus and carrots', 'Overnight'),\n",
    "    ('Party food', 'Not spicy', 'About an hour'),\n",
    "    ('Quick bite', 'Vegetarian', 'Instant'),\n",
    "    ('Dinner', 'Gluten-free', '10 minutes'),\n",
    "    ('Lunch', 'Meaty', 'something quick'),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tenacity import retry, wait_fixed, wait_random, before_sleep_log\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stderr, level=logging.DEBUG)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@retry(wait=wait_fixed(3) + wait_random(0, 2), before_sleep=before_sleep_log(logger, logging.DEBUG))\n",
    "def call_llm(dimention: str):\n",
    "    prompt = f\"\"\"\n",
    "        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\n",
    "        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \n",
    "\n",
    "        <dimension>\n",
    "        {dimention}\n",
    "        </dimension>\n",
    "\n",
    "        <example>\n",
    "        <dimension>\n",
    "        ('Breakfast', 'Not spicy', '10 minutes')\n",
    "        </dimension>\n",
    "\n",
    "        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\n",
    "        - Give me a quick recipe for plain breakfast. \n",
    "        - Breakfast, fast and non-spicy.\n",
    "        - A ten-minute breakfast meal, non-spicy.\n",
    "        - I need breakfast under 10 minutes, it should be mild to taste. \n",
    "        </example>\n",
    "\n",
    "        Just output user queries, no other text.\n",
    "    \"\"\"\n",
    "    completion = litellm.completion(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m19:06:22 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:06:22 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:06:22 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:06:22 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:06:22 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:06:22 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:06:22 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:06:22 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:06:22 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:06:22 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:06:22 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:06:22 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:06:22 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:06:22 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14620c800>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1462f7e50> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x141274c20>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:06:26 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=3937'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:06:26 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a quick breakfast recipe that's not spicy and can be made in 10 minutes.\\n- Give me a breakfast meal that's mild and takes only 10 minutes to prepare.\\n- Need a 10-minute non-spicy breakfast.\\n- What's a fast and plain breakfast I can make in under 10 minutes?\\n- Breakfast, not spicy, 10 minutes, please.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 209,\n",
      "    \"candidatesTokenCount\": 95,\n",
      "    \"totalTokenCount\": 546,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 209\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 242\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"EiFgaIuwCrmEvdIPgbOvsQg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a quick breakfast recipe that's not spicy and can be made in 10 minutes.\\n- Give me a breakfast meal that's mild and takes only 10 minutes to prepare.\\n- Need a 10-minute non-spicy breakfast.\\n- What's a fast and plain breakfast I can make in under 10 minutes?\\n- Breakfast, not spicy, 10 minutes, please.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 209,\n",
      "    \"candidatesTokenCount\": 95,\n",
      "    \"totalTokenCount\": 546,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 209\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 242\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"EiFgaIuwCrmEvdIPgbOvsQg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:06:26 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:06:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:06:26 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:06:26 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:06:26 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:06:26 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0009052000000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.0009052000000000001\n",
      "\u001b[92m19:06:26 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:06:26 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:06:26 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lunch', 'Vegetarian', '30 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lunch', 'Vegetarian', '30 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:06:26 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:06:26 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:06:26 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:06:26 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:06:26 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:06:26 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lunch', 'Vegetarian', '30 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lunch', 'Vegetarian', '30 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:06:26 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:06:26 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:06:26 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:06:26 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:06:26 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lunch', 'Vegetarian', '30 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lunch', 'Vegetarian', '30 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1459c4a40>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x146300a50> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1444d61b0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:06:30 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=4040'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:06:30 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a vegetarian lunch that can be made in 30 minutes.\\n- Suggest a quick vegetarian lunch recipe ready in half an hour.\\n- What are some good vegetarian lunch ideas for a 30-minute prep time?\\n- Looking for a fast, meat-free lunch. Must be ready in under 30 minutes.\\n- Vegetarian lunch, 30 minutes.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 209,\n",
      "    \"candidatesTokenCount\": 81,\n",
      "    \"totalTokenCount\": 868,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 209\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 578\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"FiFgaInEFKy4nsEPmOLjiAo\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a vegetarian lunch that can be made in 30 minutes.\\n- Suggest a quick vegetarian lunch recipe ready in half an hour.\\n- What are some good vegetarian lunch ideas for a 30-minute prep time?\\n- Looking for a fast, meat-free lunch. Must be ready in under 30 minutes.\\n- Vegetarian lunch, 30 minutes.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 209,\n",
      "    \"candidatesTokenCount\": 81,\n",
      "    \"totalTokenCount\": 868,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 209\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 578\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"FiFgaInEFKy4nsEPmOLjiAo\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:06:30 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:06:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:06:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:06:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:06:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:06:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0017102\n",
      "DEBUG:LiteLLM:response_cost: 0.0017102\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {('Breakfast',\n",
       "              'Not spicy',\n",
       "              '10 minutes'): [\"- I'm looking for a quick breakfast recipe that's not spicy and can be made in 10 minutes.\\n- Give me a breakfast meal that's mild and takes only 10 minutes to prepare.\\n- Need a 10-minute non-spicy breakfast.\\n- What's a fast and plain breakfast I can make in under 10 minutes?\\n- Breakfast, not spicy, 10 minutes, please.\"],\n",
       "             ('Lunch',\n",
       "              'Vegetarian',\n",
       "              '30 minutes'): ['- I need a vegetarian lunch that can be made in 30 minutes.\\n- Suggest a quick vegetarian lunch recipe ready in half an hour.\\n- What are some good vegetarian lunch ideas for a 30-minute prep time?\\n- Looking for a fast, meat-free lunch. Must be ready in under 30 minutes.\\n- Vegetarian lunch, 30 minutes.']})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "tuple_to_queries = defaultdict(list)\n",
    "for dimention in dimention_tuples[:2]:\n",
    "    query = call_llm(dimention)\n",
    "    tuple_to_queries[dimention].append(query)\n",
    "\n",
    "tuple_to_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m19:08:35 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:08:35 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:08:35 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:08:35 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:08:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:08:35 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:08:35 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:08:35 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:08:35 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:08:35 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:08:35 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:08:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:08:35 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:08:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1462d38f0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1461b97d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145943860>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:08:42 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=6364'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:08:42 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a sweet vegetarian dinner recipe that takes about 30 minutes.\\n- Can you suggest a quick vegetarian dinner? Something sweet and ready in half an hour.\\n- Dinner, vegetarian, sweet, 30 min.\\n- What's a good sweet vegetarian dinner I can make in under 30 minutes?\\n- I need a 30-minute vegetarian dinner. Make it sweet.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 209,\n",
      "    \"candidatesTokenCount\": 88,\n",
      "    \"totalTokenCount\": 538,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 209\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 241\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"miFgaIC6ApW6xN8Pg7C98QU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a sweet vegetarian dinner recipe that takes about 30 minutes.\\n- Can you suggest a quick vegetarian dinner? Something sweet and ready in half an hour.\\n- Dinner, vegetarian, sweet, 30 min.\\n- What's a good sweet vegetarian dinner I can make in under 30 minutes?\\n- I need a 30-minute vegetarian dinner. Make it sweet.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 209,\n",
      "    \"candidatesTokenCount\": 88,\n",
      "    \"totalTokenCount\": 538,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 209\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 241\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"miFgaIC6ApW6xN8Pg7C98QU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:08:42 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:08:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:08:42 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:08:42 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:08:42 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:08:42 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0008852\n",
      "DEBUG:LiteLLM:response_cost: 0.0008852\n",
      "\u001b[92m19:08:42 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:08:42 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:08:42 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lunch', 'Vegetarian', '30 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lunch', 'Vegetarian', '30 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:08:42 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:08:42 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:08:42 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:08:42 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:08:42 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:08:42 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lunch', 'Vegetarian', '30 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lunch', 'Vegetarian', '30 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:08:42 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:08:42 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:08:42 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:08:42 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:08:42 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lunch', 'Vegetarian', '30 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lunch', 'Vegetarian', '30 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14547a2d0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1463092d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145937140>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:08:54 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=12631'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:08:54 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a vegetarian lunch recipe that I can make in 30 minutes.\\n- Give me ideas for a quick vegetarian lunch, ready in half an hour.\\n- What's a good vegetarian lunch that takes 30 minutes or less?\\n- Looking for a fast, vegetarian lunch recipe, 30-minute prep time.\\n- Show me some 30-minute vegetarian lunch options.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 209,\n",
      "    \"candidatesTokenCount\": 85,\n",
      "    \"totalTokenCount\": 508,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 209\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 214\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"piFgaNSENcm9xN8PpNXe6AU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a vegetarian lunch recipe that I can make in 30 minutes.\\n- Give me ideas for a quick vegetarian lunch, ready in half an hour.\\n- What's a good vegetarian lunch that takes 30 minutes or less?\\n- Looking for a fast, vegetarian lunch recipe, 30-minute prep time.\\n- Show me some 30-minute vegetarian lunch options.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 209,\n",
      "    \"candidatesTokenCount\": 85,\n",
      "    \"totalTokenCount\": 508,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 209\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 214\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"piFgaNSENcm9xN8PpNXe6AU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:08:54 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:08:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:08:54 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:08:54 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:08:54 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:08:54 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0008102\n",
      "DEBUG:LiteLLM:response_cost: 0.0008102\n",
      "\u001b[92m19:08:54 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:08:54 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:08:54 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dinner', 'Meaty', 'Under 1 hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dinner', 'Meaty', 'Under 1 hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:08:54 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:08:54 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:08:54 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:08:54 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:08:54 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:08:54 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dinner', 'Meaty', 'Under 1 hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dinner', 'Meaty', 'Under 1 hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:08:54 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:08:54 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:08:54 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:08:54 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:08:54 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dinner', 'Meaty', 'Under 1 hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dinner', 'Meaty', 'Under 1 hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1462d2150>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x146309ad0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1462d2180>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:08:57 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=2590'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:08:57 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a meaty dinner recipe that takes less than an hour to make.\\n- Can you suggest a quick dinner idea that's hearty and meat-based, ready in under 60 minutes?\\n- Give me a meaty dinner that's under an hour.\\n- I need a fast dinner, something meaty, that can be done in less than 60 minutes.\\n- What's a good speedy, meaty dinner recipe?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 210,\n",
      "    \"candidatesTokenCount\": 95,\n",
      "    \"totalTokenCount\": 475,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 210\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 170\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"qSFgaJ7GI8-jkdUPpeypuAk\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a meaty dinner recipe that takes less than an hour to make.\\n- Can you suggest a quick dinner idea that's hearty and meat-based, ready in under 60 minutes?\\n- Give me a meaty dinner that's under an hour.\\n- I need a fast dinner, something meaty, that can be done in less than 60 minutes.\\n- What's a good speedy, meaty dinner recipe?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 210,\n",
      "    \"candidatesTokenCount\": 95,\n",
      "    \"totalTokenCount\": 475,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 210\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 170\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"qSFgaJ7GI8-jkdUPpeypuAk\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:08:57 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:08:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:08:57 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:08:57 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:08:57 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:08:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0007255000000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.0007255000000000001\n",
      "\u001b[92m19:08:57 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:08:57 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:08:57 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Snack', 'Gluten-free', 'Quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Snack', 'Gluten-free', 'Quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:08:57 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:08:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:08:57 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:08:57 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:08:57 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:08:57 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Snack', 'Gluten-free', 'Quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Snack', 'Gluten-free', 'Quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:08:57 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:08:57 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:08:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:08:57 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:08:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Snack', 'Gluten-free', 'Quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Snack', 'Gluten-free', 'Quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x146204f50>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x14630add0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14620c110>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:09:04 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=6504'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:09:04 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a quick and gluten-free snack recipe.\\n- Give me some ideas for fast gluten-free snacks.\\n- Quick gluten-free snack.\\n- Snack, gluten-free, quick.\\n- What are some easy gluten-free snacks to make?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 210,\n",
      "    \"candidatesTokenCount\": 56,\n",
      "    \"totalTokenCount\": 786,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 210\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 520\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"sCFgaLv2EunwnsEPzteagQE\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a quick and gluten-free snack recipe.\\n- Give me some ideas for fast gluten-free snacks.\\n- Quick gluten-free snack.\\n- Snack, gluten-free, quick.\\n- What are some easy gluten-free snacks to make?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 210,\n",
      "    \"candidatesTokenCount\": 56,\n",
      "    \"totalTokenCount\": 786,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 210\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 520\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"sCFgaLv2EunwnsEPzteagQE\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:09:04 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:09:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:09:04 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:04 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:04 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:09:04 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.001503\n",
      "DEBUG:LiteLLM:response_cost: 0.001503\n",
      "\u001b[92m19:09:04 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:09:04 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:09:04 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dessert', 'Dairy-free', '45 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dessert', 'Dairy-free', '45 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:09:04 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:09:04 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:09:04 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:09:04 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:09:04 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:09:04 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dessert', 'Dairy-free', '45 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dessert', 'Dairy-free', '45 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:09:04 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:09:04 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:09:04 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:09:04 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:04 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dessert', 'Dairy-free', '45 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dessert', 'Dairy-free', '45 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1444d7290>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x146308350> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x141274650>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:09:06 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=1889'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:09:06 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a dairy-free dessert that I can make in under 45 minutes.\\n- Give me some dessert ideas that are dairy-free and take less than 45 minutes.\\n- Quick, dairy-free dessert, 45-minute prep.\\n- I need a dessert recipe that's free of dairy and can be made within 45 minutes.\\n- What's a good dairy-free dessert I can whip up in 45 minutes?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 211,\n",
      "    \"candidatesTokenCount\": 103,\n",
      "    \"totalTokenCount\": 481,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 211\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 167\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"siFgaMLAFKeSxN8P9ZaE-AU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a dairy-free dessert that I can make in under 45 minutes.\\n- Give me some dessert ideas that are dairy-free and take less than 45 minutes.\\n- Quick, dairy-free dessert, 45-minute prep.\\n- I need a dessert recipe that's free of dairy and can be made within 45 minutes.\\n- What's a good dairy-free dessert I can whip up in 45 minutes?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 211,\n",
      "    \"candidatesTokenCount\": 103,\n",
      "    \"totalTokenCount\": 481,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 211\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 167\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"siFgaMLAFKeSxN8P9ZaE-AU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:09:06 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:09:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:09:06 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:06 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:06 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:09:06 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0007383\n",
      "DEBUG:LiteLLM:response_cost: 0.0007383\n",
      "\u001b[92m19:09:06 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:09:06 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:09:06 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Brunch', 'Vegan', 'I have all the time until retirement')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Brunch', 'Vegan', 'I have all the time until retirement')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:09:06 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:09:06 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:09:06 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:09:06 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:09:06 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:09:06 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Brunch', 'Vegan', 'I have all the time until retirement')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Brunch', 'Vegan', 'I have all the time until retirement')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:09:06 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:09:06 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:09:06 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:09:06 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:06 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Brunch', 'Vegan', 'I have all the time until retirement')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Brunch', 'Vegan', 'I have all the time until retirement')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145a07bc0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x14630a050> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145934500>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:09:08 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=2401'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:09:08 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for an elaborate vegan brunch recipe; I have all the time in the world to cook.\\n- Give me some vegan brunch ideas that take a long time to prepare.\\n- Suggest a time-intensive vegan brunch recipe.\\n- I want to make a vegan brunch, and I have unlimited time for cooking.\\n- Vegan brunch, please, something that's quite involved and takes a while to make.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 213,\n",
      "    \"candidatesTokenCount\": 88,\n",
      "    \"totalTokenCount\": 655,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 213\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 354\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"tCFgaJ_6M4LlxN8P36HFqQY\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for an elaborate vegan brunch recipe; I have all the time in the world to cook.\\n- Give me some vegan brunch ideas that take a long time to prepare.\\n- Suggest a time-intensive vegan brunch recipe.\\n- I want to make a vegan brunch, and I have unlimited time for cooking.\\n- Vegan brunch, please, something that's quite involved and takes a while to make.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 213,\n",
      "    \"candidatesTokenCount\": 88,\n",
      "    \"totalTokenCount\": 655,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 213\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 354\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"tCFgaJ_6M4LlxN8P36HFqQY\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:09:08 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:09:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:09:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:09:08 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0011689\n",
      "DEBUG:LiteLLM:response_cost: 0.0011689\n",
      "\u001b[92m19:09:08 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:09:08 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:09:08 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Appetizer', 'Low-carb', '15 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Appetizer', 'Low-carb', '15 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:09:08 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:09:08 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:09:08 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:09:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:09:08 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:09:08 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Appetizer', 'Low-carb', '15 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Appetizer', 'Low-carb', '15 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:09:08 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:09:08 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:09:08 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:09:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:08 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Appetizer', 'Low-carb', '15 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Appetizer', 'Low-carb', '15 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14620c0b0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1509996d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145936120>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:09:13 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=4684'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:09:13 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a low-carb appetizer that can be made in 15 minutes.\\n- Give me a quick low-carb appetizer recipe under 15 minutes.\\n- Appetizer, low-carb, 15 minutes.\\n- What are some fast low-carb appetizers that take about 15 minutes?\\n- I'm looking for a 15-minute appetizer that's low in carbs.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 212,\n",
      "    \"candidatesTokenCount\": 88,\n",
      "    \"totalTokenCount\": 596,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 212\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 296\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"uSFgaMnbKPf3xN8Pm4HL2Ag\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a low-carb appetizer that can be made in 15 minutes.\\n- Give me a quick low-carb appetizer recipe under 15 minutes.\\n- Appetizer, low-carb, 15 minutes.\\n- What are some fast low-carb appetizers that take about 15 minutes?\\n- I'm looking for a 15-minute appetizer that's low in carbs.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 212,\n",
      "    \"candidatesTokenCount\": 88,\n",
      "    \"totalTokenCount\": 596,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 212\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 296\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"uSFgaMnbKPf3xN8Pm4HL2Ag\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:09:13 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:09:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:09:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:09:13 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0010236000000000002\n",
      "DEBUG:LiteLLM:response_cost: 0.0010236000000000002\n",
      "\u001b[92m19:09:13 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:09:13 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:09:13 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'High-protein', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'High-protein', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:09:13 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:09:13 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:09:13 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:09:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:09:13 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:09:13 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'High-protein', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'High-protein', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:09:13 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:09:13 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:09:13 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:09:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:13 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'High-protein', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'High-protein', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x141274650>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1509983d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145931f70>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:09:17 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=3313'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:09:17 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a high-protein side dish that can be made in about 20 minutes.\\n- Give me a quick recipe for a high-protein side dish, ready in 20 minutes.\\n- High-protein side dish, 20 minutes.\\n- What are some fast, high-protein side dishes I can make in under 20 minutes?\\n- 20-minute side dish, high in protein.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 211,\n",
      "    \"candidatesTokenCount\": 91,\n",
      "    \"totalTokenCount\": 705,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 211\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 403\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"vSFgaLr8BenQvdIPyNi8kAY\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a high-protein side dish that can be made in about 20 minutes.\\n- Give me a quick recipe for a high-protein side dish, ready in 20 minutes.\\n- High-protein side dish, 20 minutes.\\n- What are some fast, high-protein side dishes I can make in under 20 minutes?\\n- 20-minute side dish, high in protein.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 211,\n",
      "    \"candidatesTokenCount\": 91,\n",
      "    \"totalTokenCount\": 705,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 211\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 403\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"vSFgaLr8BenQvdIPyNi8kAY\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:09:17 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:09:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:09:17 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:17 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:17 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:09:17 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0012983000000000003\n",
      "DEBUG:LiteLLM:response_cost: 0.0012983000000000003\n",
      "\u001b[92m19:09:17 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:09:17 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:09:17 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:09:17 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:09:17 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:09:17 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:09:17 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:09:17 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:09:17 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:09:17 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:09:17 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:09:17 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:09:17 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:17 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'Kid-friendly', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14620ca40>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x14630a8d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14620cc20>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:09:21 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=3866'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:09:21 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a quick, light breakfast that's kid-friendly and takes less than 20 minutes.\\n- Can you suggest a simple, healthy breakfast for kids that can be made in under 20 minutes?\\n- Kid-friendly, lite breakfast, under 20 mins.\\n- Give me fast and light breakfast ideas suitable for children, ready in less than 20 minutes.\\n- I need a quick, mild breakfast recipe for my child that's ready in under 20 minutes.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 214,\n",
      "    \"candidatesTokenCount\": 110,\n",
      "    \"totalTokenCount\": 639,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 214\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 315\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"wSFgaKy2BfqTxN8P24LQgQY\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a quick, light breakfast that's kid-friendly and takes less than 20 minutes.\\n- Can you suggest a simple, healthy breakfast for kids that can be made in under 20 minutes?\\n- Kid-friendly, lite breakfast, under 20 mins.\\n- Give me fast and light breakfast ideas suitable for children, ready in less than 20 minutes.\\n- I need a quick, mild breakfast recipe for my child that's ready in under 20 minutes.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 214,\n",
      "    \"candidatesTokenCount\": 110,\n",
      "    \"totalTokenCount\": 639,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 214\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 315\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"wSFgaKy2BfqTxN8P24LQgQY\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:09:21 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:09:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:09:21 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:21 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:21 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:09:21 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0011267\n",
      "DEBUG:LiteLLM:response_cost: 0.0011267\n",
      "\u001b[92m19:09:21 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:09:21 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:09:21 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Pescatarian', 'All day')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Pescatarian', 'All day')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:09:21 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:09:21 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:09:21 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:09:21 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:09:21 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:09:21 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Pescatarian', 'All day')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Pescatarian', 'All day')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:09:21 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:09:21 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:09:21 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:09:21 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:21 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Pescatarian', 'All day')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Pescatarian', 'All day')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14620edb0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x150998f50> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145936150>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:09:33 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=12752'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:09:33 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need an all-day pescatarian recipe for a holiday meal.\\n- What are some good pescatarian options for a holiday feast that can be prepared over several hours?\\n- Give me ideas for a special, pescatarian holiday dinner that takes all day to cook.\\n- Holiday meal, pescatarian, long cooking time.\\n- I'm looking for a celebratory pescatarian dish that I can prepare throughout the day.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 210,\n",
      "    \"candidatesTokenCount\": 93,\n",
      "    \"totalTokenCount\": 650,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 210\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 347\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"zSFgaNifO-nYvdIPn9jNsAg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need an all-day pescatarian recipe for a holiday meal.\\n- What are some good pescatarian options for a holiday feast that can be prepared over several hours?\\n- Give me ideas for a special, pescatarian holiday dinner that takes all day to cook.\\n- Holiday meal, pescatarian, long cooking time.\\n- I'm looking for a celebratory pescatarian dish that I can prepare throughout the day.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 210,\n",
      "    \"candidatesTokenCount\": 93,\n",
      "    \"totalTokenCount\": 650,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 210\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 347\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"zSFgaNifO-nYvdIPn9jNsAg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:09:33 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:09:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:09:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:09:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.001163\n",
      "DEBUG:LiteLLM:response_cost: 0.001163\n",
      "\u001b[92m19:09:33 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:09:33 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:09:33 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Party food', 'Sugar-free', 'About an hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Party food', 'Sugar-free', 'About an hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:09:33 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:09:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:09:33 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:09:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:09:33 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:09:33 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Party food', 'Sugar-free', 'About an hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Party food', 'Sugar-free', 'About an hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:09:33 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:09:33 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:09:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:09:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Party food', 'Sugar-free', 'About an hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Party food', 'Sugar-free', 'About an hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145931f70>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x15099c2d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x141254740>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:09:39 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=5859'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:09:40 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- Can you suggest some sugar-free party food recipes that take about an hour to make?\\n- I'm looking for party dishes that are sugar-free and can be prepared in roughly an hour.\\n- Give me ideas for party food, sugar-free, under an hour.\\n- What are some good sugar-free party snacks I can make in about 60 minutes?\\n- Party food, sugar-free, about an hour prep time.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 211,\n",
      "    \"candidatesTokenCount\": 95,\n",
      "    \"totalTokenCount\": 578,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 211\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 272\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"0yFgaKnIOvbBvdIP6b-cyA0\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- Can you suggest some sugar-free party food recipes that take about an hour to make?\\n- I'm looking for party dishes that are sugar-free and can be prepared in roughly an hour.\\n- Give me ideas for party food, sugar-free, under an hour.\\n- What are some good sugar-free party snacks I can make in about 60 minutes?\\n- Party food, sugar-free, about an hour prep time.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 211,\n",
      "    \"candidatesTokenCount\": 95,\n",
      "    \"totalTokenCount\": 578,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 211\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 272\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"0yFgaKnIOvbBvdIP6b-cyA0\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:09:40 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:09:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:09:40 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:40 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:40 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:09:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0009808000000000002\n",
      "DEBUG:LiteLLM:response_cost: 0.0009808000000000002\n",
      "\u001b[92m19:09:40 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:09:40 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:09:40 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Quick bite', 'Nut-free', 'Instant')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Quick bite', 'Nut-free', 'Instant')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:09:40 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:09:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:09:40 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:09:40 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:09:40 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:09:40 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Quick bite', 'Nut-free', 'Instant')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Quick bite', 'Nut-free', 'Instant')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:09:40 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:09:40 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:09:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:09:40 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Quick bite', 'Nut-free', 'Instant')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Quick bite', 'Nut-free', 'Instant')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1412759d0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x146309b50> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145935e80>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:09:43 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=3339'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:09:43 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a quick, nut-free, instant bite.\\n- Give me a recipe for an instant, nut-free quick bite.\\n- Looking for a quick, instant snack that is nut-free.\\n- Instant, nut-free quick bite ideas.\\n- What are some quick and nut-free instant snacks?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 209,\n",
      "    \"candidatesTokenCount\": 68,\n",
      "    \"totalTokenCount\": 500,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 209\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 223\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"1yFgaLW3IO7hvdIPtOG98QI\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a quick, nut-free, instant bite.\\n- Give me a recipe for an instant, nut-free quick bite.\\n- Looking for a quick, instant snack that is nut-free.\\n- Instant, nut-free quick bite ideas.\\n- What are some quick and nut-free instant snacks?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 209,\n",
      "    \"candidatesTokenCount\": 68,\n",
      "    \"totalTokenCount\": 500,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 209\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 223\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"1yFgaLW3IO7hvdIPtOG98QI\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:09:43 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:09:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:09:43 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:43 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:43 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:09:43 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0007902\n",
      "DEBUG:LiteLLM:response_cost: 0.0007902\n",
      "\u001b[92m19:09:43 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:09:43 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:09:43 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Breakfast', 'Keto', '30 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Breakfast', 'Keto', '30 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:09:43 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:09:43 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:09:43 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:09:43 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:09:43 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:09:43 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Breakfast', 'Keto', '30 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Breakfast', 'Keto', '30 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:09:43 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:09:43 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:09:43 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:09:43 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:43 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Breakfast', 'Keto', '30 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Breakfast', 'Keto', '30 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14620c740>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x15099ce50> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145936150>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:09:46 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=2380'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:09:45 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a keto breakfast recipe that I can make in 30 minutes.\\n- Give me some quick keto breakfast ideas that take under 30 minutes.\\n- What's a good 30-minute keto recipe for breakfast?\\n- Keto breakfast, 30 mins.\\n- Show me breakfast recipes for a keto diet that are ready in half an hour.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 209,\n",
      "    \"candidatesTokenCount\": 79,\n",
      "    \"totalTokenCount\": 469,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 209\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 181\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"2iFgaI_gAduG28oP3IzXoQg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a keto breakfast recipe that I can make in 30 minutes.\\n- Give me some quick keto breakfast ideas that take under 30 minutes.\\n- What's a good 30-minute keto recipe for breakfast?\\n- Keto breakfast, 30 mins.\\n- Show me breakfast recipes for a keto diet that are ready in half an hour.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 209,\n",
      "    \"candidatesTokenCount\": 79,\n",
      "    \"totalTokenCount\": 469,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 209\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 181\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"2iFgaI_gAduG28oP3IzXoQg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:09:45 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:09:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:09:45 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:45 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:45 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:09:45 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0007127\n",
      "DEBUG:LiteLLM:response_cost: 0.0007127\n",
      "\u001b[92m19:09:45 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:09:45 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:09:45 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:09:45 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:09:45 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:09:45 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:09:45 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:09:45 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:09:45 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:09:45 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:09:45 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:09:45 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:09:45 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:45 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lunch', 'Heart-healthy', 'Under 1 hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14125b9b0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x15099eb50> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145931f70>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:09:48 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=2506'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:09:48 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a heart-healthy lunch recipe that takes less than an hour to make.\\n- What are some quick, heart-healthy lunch ideas I can prepare in under 60 minutes?\\n- Give me a recipe for a healthy lunch that's good for my heart and ready in under an hour.\\n- Looking for lunch, heart-healthy, under 1 hour.\\n- Can you suggest a heart-healthy lunch that I can whip up quickly, within an hour?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 211,\n",
      "    \"candidatesTokenCount\": 100,\n",
      "    \"totalTokenCount\": 571,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 211\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 260\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"3CFgaJPzKP6kkdUPuq_juAk\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a heart-healthy lunch recipe that takes less than an hour to make.\\n- What are some quick, heart-healthy lunch ideas I can prepare in under 60 minutes?\\n- Give me a recipe for a healthy lunch that's good for my heart and ready in under an hour.\\n- Looking for lunch, heart-healthy, under 1 hour.\\n- Can you suggest a heart-healthy lunch that I can whip up quickly, within an hour?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 211,\n",
      "    \"candidatesTokenCount\": 100,\n",
      "    \"totalTokenCount\": 571,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 211\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 260\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"3CFgaJPzKP6kkdUPuq_juAk\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:09:48 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:09:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:09:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:09:48 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0009633\n",
      "DEBUG:LiteLLM:response_cost: 0.0009633\n",
      "\u001b[92m19:09:48 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:09:48 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:09:48 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dinner', 'Spicy', '45 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dinner', 'Spicy', '45 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:09:48 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:09:48 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:09:48 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:09:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:09:48 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:09:48 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dinner', 'Spicy', '45 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dinner', 'Spicy', '45 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:09:48 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:09:48 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:09:48 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:09:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:48 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dinner', 'Spicy', '45 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dinner', 'Spicy', '45 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145700ce0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x14630aed0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x146212510>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:09:51 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=2292'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:09:51 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a spicy dinner recipe that takes about 45 minutes to prepare.\\n- Can you suggest a hot and flavorful dinner I can make in under 45 minutes?\\n- Spicy dinner, 45 minutes prep time.\\n- I need to cook a spicy dinner, and I only have 45 minutes.\\n- Give me a quick spicy dinner recipe that's done in 45 minutes.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 209,\n",
      "    \"candidatesTokenCount\": 90,\n",
      "    \"totalTokenCount\": 584,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 209\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 285\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"3yFgaNnjBb6KxN8P48mokAY\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a spicy dinner recipe that takes about 45 minutes to prepare.\\n- Can you suggest a hot and flavorful dinner I can make in under 45 minutes?\\n- Spicy dinner, 45 minutes prep time.\\n- I need to cook a spicy dinner, and I only have 45 minutes.\\n- Give me a quick spicy dinner recipe that's done in 45 minutes.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 209,\n",
      "    \"candidatesTokenCount\": 90,\n",
      "    \"totalTokenCount\": 584,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 209\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 285\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"3yFgaNnjBb6KxN8P48mokAY\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:09:51 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:09:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:09:51 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:51 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:51 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:09:51 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0010002000000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.0010002000000000001\n",
      "\u001b[92m19:09:51 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:09:51 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:09:51 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Snack', 'without fish or meat', '10 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Snack', 'without fish or meat', '10 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:09:51 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:09:51 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:09:51 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:09:51 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:09:51 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:09:51 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Snack', 'without fish or meat', '10 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Snack', 'without fish or meat', '10 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:09:51 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:09:51 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:09:51 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:09:51 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:51 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Snack', 'without fish or meat', '10 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Snack', 'without fish or meat', '10 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145934f50>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x146308a50> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1444d64b0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:09:54 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=3017'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:09:54 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a quick snack that's vegetarian or vegan and can be made in 10 minutes.\\n- Give me a 10-minute snack recipe without any meat or fish.\\n- Snack, no meat, fast, 10 minutes.\\n- A quick snack recipe, takes less than 10 minutes and contains no fish or meat.\\n- I need a non-meat snack for under 10 minutes.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 212,\n",
      "    \"candidatesTokenCount\": 94,\n",
      "    \"totalTokenCount\": 517,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 212\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 211\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"4iFgaO6QEPm4nsEPhf-U0Qk\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a quick snack that's vegetarian or vegan and can be made in 10 minutes.\\n- Give me a 10-minute snack recipe without any meat or fish.\\n- Snack, no meat, fast, 10 minutes.\\n- A quick snack recipe, takes less than 10 minutes and contains no fish or meat.\\n- I need a non-meat snack for under 10 minutes.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 212,\n",
      "    \"candidatesTokenCount\": 94,\n",
      "    \"totalTokenCount\": 517,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 212\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 211\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"4iFgaO6QEPm4nsEPhf-U0Qk\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:09:54 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:09:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:09:54 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:54 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:54 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:09:54 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0008261000000000002\n",
      "DEBUG:LiteLLM:response_cost: 0.0008261000000000002\n",
      "\u001b[92m19:09:54 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:09:54 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:09:54 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dessert', 'Meaty', 'Long prep')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dessert', 'Meaty', 'Long prep')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:09:54 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:09:54 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:09:54 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:09:54 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:09:54 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:09:54 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dessert', 'Meaty', 'Long prep')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dessert', 'Meaty', 'Long prep')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:09:54 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:09:54 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:09:54 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:09:54 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:09:54 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dessert', 'Meaty', 'Long prep')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dessert', 'Meaty', 'Long prep')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1459429c0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x14630b550> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x141254740>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:10:00 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=5737'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:10:00 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a meaty dessert that requires long preparation time.\\n- Suggest a rich and hearty dessert that takes a long time to prepare.\\n- Give me ideas for a complex, meaty dessert.\\n- Dessert recipe, long prep, and meaty.\\n- What's a substantial dessert I can make that has a long prep time?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 209,\n",
      "    \"candidatesTokenCount\": 72,\n",
      "    \"totalTokenCount\": 1110,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 209\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 829\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"6CFgaIXXBs64nsEPq_jnUA\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a meaty dessert that requires long preparation time.\\n- Suggest a rich and hearty dessert that takes a long time to prepare.\\n- Give me ideas for a complex, meaty dessert.\\n- Dessert recipe, long prep, and meaty.\\n- What's a substantial dessert I can make that has a long prep time?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 209,\n",
      "    \"candidatesTokenCount\": 72,\n",
      "    \"totalTokenCount\": 1110,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 209\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 829\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"6CFgaIXXBs64nsEPq_jnUA\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:10:00 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:10:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:10:00 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:00 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:00 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:10:00 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0023152000000000003\n",
      "DEBUG:LiteLLM:response_cost: 0.0023152000000000003\n",
      "\u001b[92m19:10:00 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:10:00 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:10:00 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Brunch', 'Paleo', 'Quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Brunch', 'Paleo', 'Quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:10:00 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:10:00 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:10:00 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:10:00 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:10:00 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:10:00 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Brunch', 'Paleo', 'Quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Brunch', 'Paleo', 'Quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:10:00 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:10:00 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:10:00 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:10:00 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:00 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Brunch', 'Paleo', 'Quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Brunch', 'Paleo', 'Quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145931f70>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1463093d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145937da0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:10:02 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=2626'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:10:02 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a quick paleo brunch recipe.\\n- Give me fast brunch ideas that are paleo-friendly.\\n- Paleo brunch, quick to prepare.\\n- What's a good quick paleo option for brunch?\\n- I need a speedy paleo brunch recipe.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 208,\n",
      "    \"candidatesTokenCount\": 57,\n",
      "    \"totalTokenCount\": 435,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 208\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 170\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"6iFgaLPrNcGznsEPuojWWA\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a quick paleo brunch recipe.\\n- Give me fast brunch ideas that are paleo-friendly.\\n- Paleo brunch, quick to prepare.\\n- What's a good quick paleo option for brunch?\\n- I need a speedy paleo brunch recipe.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 208,\n",
      "    \"candidatesTokenCount\": 57,\n",
      "    \"totalTokenCount\": 435,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 208\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 170\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"6iFgaLPrNcGznsEPuojWWA\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:10:02 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:10:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:10:02 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:02 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:02 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:10:02 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0006299000000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.0006299000000000001\n",
      "\u001b[92m19:10:02 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:10:02 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:10:02 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Appetizer', 'Low-salt', '15 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Appetizer', 'Low-salt', '15 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:10:02 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:10:02 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:10:02 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:10:02 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:10:02 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:10:02 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Appetizer', 'Low-salt', '15 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Appetizer', 'Low-salt', '15 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:10:02 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:10:02 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:10:02 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:10:02 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:02 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Appetizer', 'Low-salt', '15 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Appetizer', 'Low-salt', '15 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1459c6450>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x14630b2d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x141274c20>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:10:05 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=1982'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:10:05 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a low-salt appetizer that can be made in 15 minutes.\\n- Give me a quick recipe for a low-salt appetizer, ready in under 15 minutes.\\n- Appetizer, low-salt, 15 minutes.\\n- Need a 15-minute appetizer that's low in salt.\\n- What's a good low-sodium appetizer I can whip up in 15 minutes?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 212,\n",
      "    \"candidatesTokenCount\": 95,\n",
      "    \"totalTokenCount\": 570,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 212\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 263\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"7SFgaPfdA9zInsEPjLOz6Qk\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a low-salt appetizer that can be made in 15 minutes.\\n- Give me a quick recipe for a low-salt appetizer, ready in under 15 minutes.\\n- Appetizer, low-salt, 15 minutes.\\n- Need a 15-minute appetizer that's low in salt.\\n- What's a good low-sodium appetizer I can whip up in 15 minutes?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 212,\n",
      "    \"candidatesTokenCount\": 95,\n",
      "    \"totalTokenCount\": 570,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 212\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 263\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"7SFgaPfdA9zInsEPjLOz6Qk\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:10:05 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:10:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:10:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:10:05 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0009586000000000002\n",
      "DEBUG:LiteLLM:response_cost: 0.0009586000000000002\n",
      "\u001b[92m19:10:05 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:10:05 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:10:05 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:10:05 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:10:05 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:10:05 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:10:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:10:05 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:10:05 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:10:05 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:10:05 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:10:05 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:10:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:05 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Diabetic-friendly', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14125b9b0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1461f09d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1459c7a10>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:10:08 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=2932'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:10:08 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a diabetic-friendly side dish that I can make in about 20 minutes.\\n- Give me a quick recipe for a diabetic side dish that takes 20 minutes.\\n- Diabetic side dish, 20 minutes.\\n- What are some easy side dishes for diabetics that can be prepared in under 20 minutes?\\n- I need a 20-minute side dish recipe suitable for a diabetic diet.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 212,\n",
      "    \"candidatesTokenCount\": 93,\n",
      "    \"totalTokenCount\": 608,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 212\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 303\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"8CFgaILGBq25nsEPybWHkQo\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a diabetic-friendly side dish that I can make in about 20 minutes.\\n- Give me a quick recipe for a diabetic side dish that takes 20 minutes.\\n- Diabetic side dish, 20 minutes.\\n- What are some easy side dishes for diabetics that can be prepared in under 20 minutes?\\n- I need a 20-minute side dish recipe suitable for a diabetic diet.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 212,\n",
      "    \"candidatesTokenCount\": 93,\n",
      "    \"totalTokenCount\": 608,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 212\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 303\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"8CFgaILGBq25nsEPybWHkQo\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:10:08 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:10:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:10:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:10:08 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0010536\n",
      "DEBUG:LiteLLM:response_cost: 0.0010536\n",
      "\u001b[92m19:10:08 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:10:08 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:10:08 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:10:08 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:10:08 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:10:08 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:10:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:10:08 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:10:08 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:10:08 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:10:08 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:10:08 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:10:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:08 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'High-fiber', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145935c40>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x14630b2d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145942000>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:10:10 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=2395'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:10:10 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a light, high-fiber breakfast that takes less than 20 minutes to make.\\n- Give me a quick, high-fiber breakfast recipe that's light.\\n- High-fiber, light breakfast, under 20 minutes.\\n- What are some quick and light high-fiber breakfast options ready in less than 20 minutes?\\n- I need a light, fiber-rich breakfast that's ready in under 20 minutes.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 214,\n",
      "    \"candidatesTokenCount\": 100,\n",
      "    \"totalTokenCount\": 629,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 214\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 315\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"8iFgaMCYJ9O5nsEP8pGZ6Ak\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a light, high-fiber breakfast that takes less than 20 minutes to make.\\n- Give me a quick, high-fiber breakfast recipe that's light.\\n- High-fiber, light breakfast, under 20 minutes.\\n- What are some quick and light high-fiber breakfast options ready in less than 20 minutes?\\n- I need a light, fiber-rich breakfast that's ready in under 20 minutes.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 214,\n",
      "    \"candidatesTokenCount\": 100,\n",
      "    \"totalTokenCount\": 629,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 214\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 315\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"8iFgaMCYJ9O5nsEP8pGZ6Ak\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:10:10 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:10:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:10:10 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:10 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:10 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:10:10 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0011017000000000002\n",
      "DEBUG:LiteLLM:response_cost: 0.0011017000000000002\n",
      "\u001b[92m19:10:10 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:10:10 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:10:10 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Lactose-free', 'Overnight')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Lactose-free', 'Overnight')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:10:10 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:10:10 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:10:10 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:10:10 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:10:10 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:10:10 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Lactose-free', 'Overnight')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Lactose-free', 'Overnight')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:10:10 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:10:10 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:10:10 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:10:10 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:10 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Lactose-free', 'Overnight')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Lactose-free', 'Overnight')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145937c80>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x146309050> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14593f710>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Type', b'application/json; charset=UTF-8'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:10:10 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=59'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 429 Too Many Requests\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:10:10 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2268 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "DEBUG:LiteLLM:Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m19:10:10 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2165 - Logging Details LiteLLM-Failure Call: []\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Failure Call: []\n",
      "DEBUG:__main__:Retrying __main__.call_llm in 4.96585698496359 seconds as it raised RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
      "    \"details\": [\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
      "        \"violations\": [\n",
      "          {\n",
      "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
      "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
      "            \"quotaDimensions\": {\n",
      "              \"location\": \"global\",\n",
      "              \"model\": \"gemini-2.5-flash\"\n",
      "            },\n",
      "            \"quotaValue\": \"10\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
      "        \"links\": [\n",
      "          {\n",
      "            \"description\": \"Learn more about Gemini API quotas\",\n",
      "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
      "        \"retryDelay\": \"49s\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      ".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m19:10:15 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:10:15 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:10:15 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Lactose-free', 'Overnight')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Lactose-free', 'Overnight')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:10:15 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:10:15 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:10:15 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:10:15 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:10:15 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:10:15 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Lactose-free', 'Overnight')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Lactose-free', 'Overnight')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:10:15 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:10:15 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:10:15 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:10:15 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:15 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Lactose-free', 'Overnight')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Lactose-free', 'Overnight')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x141274c20>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x146308ad0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14126d190>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Type', b'application/json; charset=UTF-8'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:10:15 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=46'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 429 Too Many Requests\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:10:15 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2268 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "DEBUG:LiteLLM:Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m19:10:15 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2165 - Logging Details LiteLLM-Failure Call: []\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Failure Call: []\n",
      "DEBUG:__main__:Retrying __main__.call_llm in 4.917846191370538 seconds as it raised RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
      "    \"details\": [\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
      "        \"violations\": [\n",
      "          {\n",
      "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
      "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
      "            \"quotaDimensions\": {\n",
      "              \"location\": \"global\",\n",
      "              \"model\": \"gemini-2.5-flash\"\n",
      "            },\n",
      "            \"quotaValue\": \"10\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
      "        \"links\": [\n",
      "          {\n",
      "            \"description\": \"Learn more about Gemini API quotas\",\n",
      "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
      "        \"retryDelay\": \"44s\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      ".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m19:10:20 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:10:20 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:10:20 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Lactose-free', 'Overnight')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Lactose-free', 'Overnight')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:10:20 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:10:20 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:10:20 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:10:20 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:10:20 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:10:20 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Lactose-free', 'Overnight')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Lactose-free', 'Overnight')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:10:20 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:10:20 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:10:20 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:10:20 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:20 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Lactose-free', 'Overnight')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Lactose-free', 'Overnight')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14620cc20>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x146302d50> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14620fad0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Type', b'application/json; charset=UTF-8'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:10:21 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=155'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 429 Too Many Requests\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:10:21 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2268 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "DEBUG:LiteLLM:Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m19:10:21 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2165 - Logging Details LiteLLM-Failure Call: []\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Failure Call: []\n",
      "DEBUG:__main__:Retrying __main__.call_llm in 4.387872327305074 seconds as it raised RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
      "    \"details\": [\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
      "        \"violations\": [\n",
      "          {\n",
      "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
      "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
      "            \"quotaDimensions\": {\n",
      "              \"model\": \"gemini-2.5-flash\",\n",
      "              \"location\": \"global\"\n",
      "            },\n",
      "            \"quotaValue\": \"10\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
      "        \"links\": [\n",
      "          {\n",
      "            \"description\": \"Learn more about Gemini API quotas\",\n",
      "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
      "        \"retryDelay\": \"38s\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      ".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m19:10:25 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:10:25 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:10:25 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Lactose-free', 'Overnight')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Lactose-free', 'Overnight')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:10:25 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:10:25 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:10:25 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:10:25 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:10:25 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:10:25 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Lactose-free', 'Overnight')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Lactose-free', 'Overnight')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:10:25 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:10:25 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:10:25 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:10:25 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:25 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Lactose-free', 'Overnight')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Lactose-free', 'Overnight')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1462d0080>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x146301d50> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14620c890>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Type', b'application/json; charset=UTF-8'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:10:25 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=152'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 429 Too Many Requests\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:10:25 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2268 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "DEBUG:LiteLLM:Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m19:10:25 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2165 - Logging Details LiteLLM-Failure Call: []\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Failure Call: []\n",
      "DEBUG:__main__:Retrying __main__.call_llm in 3.0055111703848514 seconds as it raised RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
      "    \"details\": [\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
      "        \"violations\": [\n",
      "          {\n",
      "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
      "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
      "            \"quotaDimensions\": {\n",
      "              \"location\": \"global\",\n",
      "              \"model\": \"gemini-2.5-flash\"\n",
      "            },\n",
      "            \"quotaValue\": \"10\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
      "        \"links\": [\n",
      "          {\n",
      "            \"description\": \"Learn more about Gemini API quotas\",\n",
      "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
      "        \"retryDelay\": \"34s\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      ".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m19:10:28 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:10:28 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:10:28 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Lactose-free', 'Overnight')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Lactose-free', 'Overnight')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:10:28 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:10:28 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:10:28 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:10:28 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:10:28 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:10:28 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Lactose-free', 'Overnight')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Lactose-free', 'Overnight')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:10:28 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:10:28 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:10:28 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:10:28 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:28 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Lactose-free', 'Overnight')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Lactose-free', 'Overnight')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1462d0710>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x150998450> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x141274c20>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:10:31 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=2320'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:10:31 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need an overnight, lactose-free recipe for a holiday meal.\\n- Give me some holiday meal ideas that are dairy-free and can be prepared overnight.\\n- What are good lactose-free holiday dishes I can make overnight?\\n- Holiday meal, lactose-free, overnight preparation.\\n- I'm looking for an overnight recipe that's perfect for a holiday feast and is also lactose-free.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 211,\n",
      "    \"candidatesTokenCount\": 86,\n",
      "    \"totalTokenCount\": 554,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 211\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 257\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"ByJgaIOOEeOkkdUPyoPfcQ\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need an overnight, lactose-free recipe for a holiday meal.\\n- Give me some holiday meal ideas that are dairy-free and can be prepared overnight.\\n- What are good lactose-free holiday dishes I can make overnight?\\n- Holiday meal, lactose-free, overnight preparation.\\n- I'm looking for an overnight recipe that's perfect for a holiday feast and is also lactose-free.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 211,\n",
      "    \"candidatesTokenCount\": 86,\n",
      "    \"totalTokenCount\": 554,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 211\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 257\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"ByJgaIOOEeOkkdUPyoPfcQ\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:10:31 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:10:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:10:31 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:31 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:31 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:10:31 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0009208\n",
      "DEBUG:LiteLLM:response_cost: 0.0009208\n",
      "\u001b[92m19:10:31 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:10:31 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:10:31 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Party food', 'Comfort food', 'About an hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Party food', 'Comfort food', 'About an hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:10:31 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:10:31 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:10:31 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:10:31 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:10:31 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:10:31 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Party food', 'Comfort food', 'About an hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Party food', 'Comfort food', 'About an hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:10:31 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:10:31 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:10:31 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:10:31 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:31 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Party food', 'Comfort food', 'About an hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Party food', 'Comfort food', 'About an hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1462d2180>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1463011d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14620cc20>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:10:33 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=1880'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:10:33 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a recipe that is good for a party, is comforting, and takes about an hour to make.\\n- Give me party food ideas that are also comfort food and can be made in roughly an hour.\\n- Comfort food for a party, ready in around an hour.\\n- I need a comforting party dish that takes approximately an hour to prepare.\\n- What can I cook for a party that's comforting and takes about an hour?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 210,\n",
      "    \"candidatesTokenCount\": 96,\n",
      "    \"totalTokenCount\": 512,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 210\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 206\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"CSJgaPezEZeznsEPoe_vgA4\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a recipe that is good for a party, is comforting, and takes about an hour to make.\\n- Give me party food ideas that are also comfort food and can be made in roughly an hour.\\n- Comfort food for a party, ready in around an hour.\\n- I need a comforting party dish that takes approximately an hour to prepare.\\n- What can I cook for a party that's comforting and takes about an hour?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 210,\n",
      "    \"candidatesTokenCount\": 96,\n",
      "    \"totalTokenCount\": 512,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 210\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 206\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"CSJgaPezEZeznsEPoe_vgA4\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:10:33 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:10:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:10:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:10:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.000818\n",
      "DEBUG:LiteLLM:response_cost: 0.000818\n",
      "\u001b[92m19:10:33 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:10:33 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:10:33 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Quick bite', 'with hummus and carrots', 'Instant')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Quick bite', 'with hummus and carrots', 'Instant')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:10:33 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:10:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:10:33 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:10:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:10:33 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:10:33 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Quick bite', 'with hummus and carrots', 'Instant')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Quick bite', 'with hummus and carrots', 'Instant')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:10:33 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:10:33 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:10:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:10:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Quick bite', 'with hummus and carrots', 'Instant')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Quick bite', 'with hummus and carrots', 'Instant')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14593da60>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x14630acd0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1459c6450>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:10:37 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=3975'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:10:37 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a quick bite that's instant and includes hummus and carrots.\\n- Give me an instant quick bite recipe with hummus and carrots.\\n- Quick bite, instant, hummus and carrots.\\n- I need a super fast snack with hummus and carrots.\\n- What can I make instantly for a quick bite using hummus and carrots?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 210,\n",
      "    \"candidatesTokenCount\": 73,\n",
      "    \"totalTokenCount\": 888,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 210\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 605\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"DSJgaIqaF-S7kdUPueKAwAk\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a quick bite that's instant and includes hummus and carrots.\\n- Give me an instant quick bite recipe with hummus and carrots.\\n- Quick bite, instant, hummus and carrots.\\n- I need a super fast snack with hummus and carrots.\\n- What can I make instantly for a quick bite using hummus and carrots?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 210,\n",
      "    \"candidatesTokenCount\": 73,\n",
      "    \"totalTokenCount\": 888,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 210\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 605\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"DSJgaIqaF-S7kdUPueKAwAk\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:10:37 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:10:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:10:37 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:37 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:37 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:10:37 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.001758\n",
      "DEBUG:LiteLLM:response_cost: 0.001758\n",
      "\u001b[92m19:10:37 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:10:37 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:10:37 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Breakfast', 'Gluten-free', 'Quick and easy')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Breakfast', 'Gluten-free', 'Quick and easy')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:10:37 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:10:37 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:10:37 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:10:37 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:10:37 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:10:37 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Breakfast', 'Gluten-free', 'Quick and easy')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Breakfast', 'Gluten-free', 'Quick and easy')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:10:37 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:10:37 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:10:37 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:10:37 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:37 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Breakfast', 'Gluten-free', 'Quick and easy')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Breakfast', 'Gluten-free', 'Quick and easy')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1459358e0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x146302950> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145934a40>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:10:39 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=2063'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:10:39 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a healthy Italian dinner recipe.\\n- Healthy Italian dinner ideas, please.\\n- Can you suggest a healthy Italian meal for dinner?\\n- I want to make a healthy Italian dish for dinner tonight.\\n- Give me a recipe for healthy Italian dinner.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 211,\n",
      "    \"candidatesTokenCount\": 58,\n",
      "    \"totalTokenCount\": 471,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 211\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 202\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"DyJgaISHI9SlkdUPy9SCSQ\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a healthy Italian dinner recipe.\\n- Healthy Italian dinner ideas, please.\\n- Can you suggest a healthy Italian meal for dinner?\\n- I want to make a healthy Italian dish for dinner tonight.\\n- Give me a recipe for healthy Italian dinner.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 211,\n",
      "    \"candidatesTokenCount\": 58,\n",
      "    \"totalTokenCount\": 471,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 211\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 202\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"DyJgaISHI9SlkdUPy9SCSQ\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:10:39 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:10:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:10:39 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:39 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:39 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:10:39 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0007132999999999999\n",
      "DEBUG:LiteLLM:response_cost: 0.0007132999999999999\n",
      "\u001b[92m19:10:39 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:10:39 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:10:39 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lunch', 'Vegan', '30 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lunch', 'Vegan', '30 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:10:39 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:10:39 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:10:39 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:10:39 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:10:39 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:10:39 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lunch', 'Vegan', '30 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lunch', 'Vegan', '30 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:10:39 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:10:39 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:10:39 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:10:39 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:39 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lunch', 'Vegan', '30 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lunch', 'Vegan', '30 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145937e60>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x15099ccd0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14620cc20>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:10:48 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=8766'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:10:48 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegan lunch that I can make in 30 minutes.\\n- Give me some quick vegan lunch ideas under 30 minutes.\\n- What's a good vegan lunch recipe I can prepare in half an hour?\\n- I need a 30-minute vegan meal for lunch.\\n- Vegan lunch, 30 min.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 208,\n",
      "    \"candidatesTokenCount\": 77,\n",
      "    \"totalTokenCount\": 465,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 208\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 180\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"GCJgaIq6Icvi7M8P4Oq1gAE\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegan lunch that I can make in 30 minutes.\\n- Give me some quick vegan lunch ideas under 30 minutes.\\n- What's a good vegan lunch recipe I can prepare in half an hour?\\n- I need a 30-minute vegan meal for lunch.\\n- Vegan lunch, 30 min.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 208,\n",
      "    \"candidatesTokenCount\": 77,\n",
      "    \"totalTokenCount\": 465,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 208\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 180\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"GCJgaIq6Icvi7M8P4Oq1gAE\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:10:48 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:10:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:10:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:10:48 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0007049000000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.0007049000000000001\n",
      "\u001b[92m19:10:48 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:10:48 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:10:48 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dinner', 'High-protein', 'Under 1 hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dinner', 'High-protein', 'Under 1 hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:10:48 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:10:48 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:10:48 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:10:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:10:48 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:10:48 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dinner', 'High-protein', 'Under 1 hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dinner', 'High-protein', 'Under 1 hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:10:48 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:10:48 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:10:48 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:10:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:48 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dinner', 'High-protein', 'Under 1 hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dinner', 'High-protein', 'Under 1 hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x141498830>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x15099ee50> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145a06990>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:10:52 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=3818'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:10:52 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a high-protein dinner recipe that I can prepare in less than an hour.\\n- What are some quick, high-protein dinner ideas for tonight?\\n- Give me a high-protein dinner recipe that takes under 60 minutes.\\n- Looking for fast, protein-rich dinner options.\\n- Dinner, high protein, under 1 hour.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 211,\n",
      "    \"candidatesTokenCount\": 76,\n",
      "    \"totalTokenCount\": 655,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 211\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 368\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"HCJgaKjtHez_nsEPk4iegAQ\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a high-protein dinner recipe that I can prepare in less than an hour.\\n- What are some quick, high-protein dinner ideas for tonight?\\n- Give me a high-protein dinner recipe that takes under 60 minutes.\\n- Looking for fast, protein-rich dinner options.\\n- Dinner, high protein, under 1 hour.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 211,\n",
      "    \"candidatesTokenCount\": 76,\n",
      "    \"totalTokenCount\": 655,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 211\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 368\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"HCJgaKjtHez_nsEPk4iegAQ\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:10:52 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:10:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:10:52 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:52 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:52 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:10:52 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0011733000000000002\n",
      "DEBUG:LiteLLM:response_cost: 0.0011733000000000002\n",
      "\u001b[92m19:10:52 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:10:52 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:10:52 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Snack', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Snack', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:10:52 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:10:52 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:10:52 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:10:52 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:10:52 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:10:52 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Snack', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Snack', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:10:52 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:10:52 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:10:52 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:10:52 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:52 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Snack', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Snack', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145936360>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x15099d050> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145935100>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:10:54 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=1698'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:10:54 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a quick snack that's not spicy and takes about 10 minutes to make.\\n- Give me some ideas for a 10-minute mild snack.\\n- What's a fast, non-spicy snack recipe I can whip up in 10 minutes?\\n- I'm looking for a quick snack, no spice, ready in 10 minutes.\\n- Suggest a 10-minute, non-spicy snack.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 210,\n",
      "    \"candidatesTokenCount\": 98,\n",
      "    \"totalTokenCount\": 507,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 210\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 199\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"HiJgaPy1E5zqkdUPrNDOwQk\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a quick snack that's not spicy and takes about 10 minutes to make.\\n- Give me some ideas for a 10-minute mild snack.\\n- What's a fast, non-spicy snack recipe I can whip up in 10 minutes?\\n- I'm looking for a quick snack, no spice, ready in 10 minutes.\\n- Suggest a 10-minute, non-spicy snack.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 210,\n",
      "    \"candidatesTokenCount\": 98,\n",
      "    \"totalTokenCount\": 507,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 210\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 199\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"HiJgaPy1E5zqkdUPrNDOwQk\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:10:54 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:10:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:10:54 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:54 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:54 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:10:54 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0008055000000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.0008055000000000001\n",
      "\u001b[92m19:10:54 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:10:54 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:10:54 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dessert', 'Vegetarian', '45 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dessert', 'Vegetarian', '45 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:10:54 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:10:54 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:10:54 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:10:54 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:10:54 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:10:54 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dessert', 'Vegetarian', '45 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dessert', 'Vegetarian', '45 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:10:54 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:10:54 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:10:54 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:10:54 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:54 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dessert', 'Vegetarian', '45 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dessert', 'Vegetarian', '45 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x141276120>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x15099fb50> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1459344a0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:10:57 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=3378'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:10:57 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian dessert recipe that takes about 45 minutes to prepare.\\n- Can you suggest a dessert that's vegetarian and can be made in under 45 minutes?\\n- Vegetarian dessert, 45 minutes.\\n- I need a quick vegetarian dessert, maximum 45 minutes.\\n- Show me some vegetarian desserts that take around 45 minutes.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 210,\n",
      "    \"candidatesTokenCount\": 82,\n",
      "    \"totalTokenCount\": 544,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 210\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 252\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"ISJgaLm7OLeAkdUPztyWwQk\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a vegetarian dessert recipe that takes about 45 minutes to prepare.\\n- Can you suggest a dessert that's vegetarian and can be made in under 45 minutes?\\n- Vegetarian dessert, 45 minutes.\\n- I need a quick vegetarian dessert, maximum 45 minutes.\\n- Show me some vegetarian desserts that take around 45 minutes.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 210,\n",
      "    \"candidatesTokenCount\": 82,\n",
      "    \"totalTokenCount\": 544,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 210\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 252\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"ISJgaLm7OLeAkdUPztyWwQk\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:10:57 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:10:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:10:57 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:57 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:57 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:10:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.000898\n",
      "DEBUG:LiteLLM:response_cost: 0.000898\n",
      "\u001b[92m19:10:57 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:10:57 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:10:57 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Brunch', 'Meaty', 'I have all the time until retirement')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Brunch', 'Meaty', 'I have all the time until retirement')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:10:57 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:10:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:10:57 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:10:57 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:10:57 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:10:57 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Brunch', 'Meaty', 'I have all the time until retirement')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Brunch', 'Meaty', 'I have all the time until retirement')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:10:57 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:10:57 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:10:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:10:57 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:10:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Brunch', 'Meaty', 'I have all the time until retirement')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Brunch', 'Meaty', 'I have all the time until retirement')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1462d0140>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1509a8bd0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145941b20>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:11:02 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=4667'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:11:02 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'd love a meaty brunch recipe, and I have all the time in the world to make it.\\n- Suggest a hearty, meaty brunch dish where cooking time is not a concern.\\n- Brunch, meaty, and something that takes a while to prepare.\\n- Give me ideas for a luxurious meaty brunch; I'm not in a hurry.\\n- I want a meaty brunch recipe, no time constraints.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 214,\n",
      "    \"candidatesTokenCount\": 87,\n",
      "    \"totalTokenCount\": 777,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 214\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 476\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"JiJgaLqfLJ3NkdUP3PuLQQ\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'd love a meaty brunch recipe, and I have all the time in the world to make it.\\n- Suggest a hearty, meaty brunch dish where cooking time is not a concern.\\n- Brunch, meaty, and something that takes a while to prepare.\\n- Give me ideas for a luxurious meaty brunch; I'm not in a hurry.\\n- I want a meaty brunch recipe, no time constraints.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 214,\n",
      "    \"candidatesTokenCount\": 87,\n",
      "    \"totalTokenCount\": 777,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 214\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 476\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"JiJgaLqfLJ3NkdUP3PuLQQ\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:11:02 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:11:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:11:02 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:11:02 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:11:02 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:11:02 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0014717\n",
      "DEBUG:LiteLLM:response_cost: 0.0014717\n",
      "\u001b[92m19:11:02 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:11:02 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:11:02 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Appetizer', 'Dairy-free', '15 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Appetizer', 'Dairy-free', '15 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:11:02 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:11:02 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:11:02 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:11:02 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:11:02 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:11:02 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Appetizer', 'Dairy-free', '15 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Appetizer', 'Dairy-free', '15 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:11:02 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:11:02 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:11:02 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:11:02 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:11:02 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Appetizer', 'Dairy-free', '15 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Appetizer', 'Dairy-free', '15 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14620cc50>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1462898d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1462d2180>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:11:04 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=1976'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:11:04 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a dairy-free appetizer that takes about 15 minutes to make.\\n- Suggest a quick 15-minute appetizer, no dairy please.\\n- Dairy-free appetizer, ready in 15 minutes.\\n- What are some fast dairy-free appetizers I can prepare in under 15 minutes?\\n- I need a recipe for a 15-minute dairy-free appetizer.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 212,\n",
      "    \"candidatesTokenCount\": 89,\n",
      "    \"totalTokenCount\": 519,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 212\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 218\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"KCJgaNfxMcTensEPw73JSA\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a dairy-free appetizer that takes about 15 minutes to make.\\n- Suggest a quick 15-minute appetizer, no dairy please.\\n- Dairy-free appetizer, ready in 15 minutes.\\n- What are some fast dairy-free appetizers I can prepare in under 15 minutes?\\n- I need a recipe for a 15-minute dairy-free appetizer.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 212,\n",
      "    \"candidatesTokenCount\": 89,\n",
      "    \"totalTokenCount\": 519,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 212\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 218\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"KCJgaNfxMcTensEPw73JSA\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:11:04 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:11:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:11:04 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:11:04 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:11:04 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:11:04 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0008311000000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.0008311000000000001\n",
      "\u001b[92m19:11:04 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:11:04 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:11:04 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Low-carb', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Low-carb', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:11:04 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:11:04 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:11:04 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:11:04 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:11:04 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:11:04 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Low-carb', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Low-carb', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:11:04 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:11:04 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:11:04 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:11:04 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:11:04 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Low-carb', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Low-carb', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145937590>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1509a9550> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14620d130>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:11:08 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=3275'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:11:08 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a low-carb side dish that I can make in 20 minutes.\\n- What are some quick, low-carb side dish recipes ready in under 20 minutes?\\n- Show me a fast side dish for dinner, low in carbs and taking only 20 minutes.\\n- Give me ideas for a 20-minute, keto-friendly side.\\n- I'm looking for a quick, low-carb accompaniment that's done in about 20 minutes.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 211,\n",
      "    \"candidatesTokenCount\": 105,\n",
      "    \"totalTokenCount\": 597,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 211\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 281\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"LCJgaO6vC625nsEPybWHkQo\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a low-carb side dish that I can make in 20 minutes.\\n- What are some quick, low-carb side dish recipes ready in under 20 minutes?\\n- Show me a fast side dish for dinner, low in carbs and taking only 20 minutes.\\n- Give me ideas for a 20-minute, keto-friendly side.\\n- I'm looking for a quick, low-carb accompaniment that's done in about 20 minutes.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 211,\n",
      "    \"candidatesTokenCount\": 105,\n",
      "    \"totalTokenCount\": 597,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 211\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 281\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"LCJgaO6vC625nsEPybWHkQo\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:11:08 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:11:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:11:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:11:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:11:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:11:08 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0010283000000000002\n",
      "DEBUG:LiteLLM:response_cost: 0.0010283000000000002\n",
      "\u001b[92m19:11:08 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:11:08 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:11:08 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:11:08 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:11:08 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:11:08 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:11:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:11:08 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:11:08 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:11:08 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:11:08 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:11:08 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:11:08 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:11:08 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1459c6a50>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1509aa350> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x13633dfd0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Type', b'application/json; charset=UTF-8'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:11:08 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=159'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 429 Too Many Requests\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:11:08 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2268 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "DEBUG:LiteLLM:Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m19:11:08 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2165 - Logging Details LiteLLM-Failure Call: []\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Failure Call: []\n",
      "DEBUG:__main__:Retrying __main__.call_llm in 4.710765797642429 seconds as it raised RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
      "    \"details\": [\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
      "        \"violations\": [\n",
      "          {\n",
      "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
      "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
      "            \"quotaDimensions\": {\n",
      "              \"location\": \"global\",\n",
      "              \"model\": \"gemini-2.5-flash\"\n",
      "            },\n",
      "            \"quotaValue\": \"10\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
      "        \"links\": [\n",
      "          {\n",
      "            \"description\": \"Learn more about Gemini API quotas\",\n",
      "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
      "        \"retryDelay\": \"51s\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      ".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m19:11:13 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:11:13 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:11:13 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:11:13 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:11:13 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:11:13 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:11:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:11:13 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:11:13 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:11:13 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:11:13 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:11:13 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:11:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:11:13 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145936de0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x146289950> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145937590>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Type', b'application/json; charset=UTF-8'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:11:13 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=160'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 429 Too Many Requests\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:11:13 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2268 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "DEBUG:LiteLLM:Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m19:11:13 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2165 - Logging Details LiteLLM-Failure Call: []\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Failure Call: []\n",
      "DEBUG:__main__:Retrying __main__.call_llm in 4.342008735710127 seconds as it raised RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
      "    \"details\": [\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
      "        \"violations\": [\n",
      "          {\n",
      "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
      "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
      "            \"quotaDimensions\": {\n",
      "              \"model\": \"gemini-2.5-flash\",\n",
      "              \"location\": \"global\"\n",
      "            },\n",
      "            \"quotaValue\": \"10\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
      "        \"links\": [\n",
      "          {\n",
      "            \"description\": \"Learn more about Gemini API quotas\",\n",
      "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
      "        \"retryDelay\": \"46s\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      ".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m19:11:17 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:11:17 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:11:17 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:11:17 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:11:17 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:11:17 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:11:17 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:11:17 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:11:17 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:11:17 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:11:17 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:11:17 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:11:17 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:11:17 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14620c530>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1509a91d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14620db20>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Type', b'application/json; charset=UTF-8'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:11:18 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=35'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 429 Too Many Requests\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:11:17 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2268 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "DEBUG:LiteLLM:Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m19:11:17 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2165 - Logging Details LiteLLM-Failure Call: []\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Failure Call: []\n",
      "DEBUG:__main__:Retrying __main__.call_llm in 4.074048220194481 seconds as it raised RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
      "    \"details\": [\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
      "        \"violations\": [\n",
      "          {\n",
      "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
      "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
      "            \"quotaDimensions\": {\n",
      "              \"location\": \"global\",\n",
      "              \"model\": \"gemini-2.5-flash\"\n",
      "            },\n",
      "            \"quotaValue\": \"10\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
      "        \"links\": [\n",
      "          {\n",
      "            \"description\": \"Learn more about Gemini API quotas\",\n",
      "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
      "        \"retryDelay\": \"42s\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      ".\n",
      "IOStream.flush timed out\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m19:11:32 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:11:32 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:11:32 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:11:32 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:11:32 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:11:32 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:11:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:11:32 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:11:32 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:11:32 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:11:32 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:11:32 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:11:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:11:32 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'Pescatarian', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x146207560>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1509aae50> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1462041a0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:11:35 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=2840'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:11:34 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a lite pescatarian breakfast that takes less than 20 minutes to prepare.\\n- Give me a quick recipe for a light, pescatarian breakfast ready in under 20 minutes.\\n- What are some easy, lite pescatarian breakfast ideas that can be made in under 20 minutes?\\n- Pescatarian, light breakfast, quick (under 20 minutes).\\n- I need a fast and light pescatarian breakfast, less than 20 minutes.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 214,\n",
      "    \"candidatesTokenCount\": 107,\n",
      "    \"totalTokenCount\": 602,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 214\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 281\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"RyJgaJatAua4xN8P-IaL8AU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a lite pescatarian breakfast that takes less than 20 minutes to prepare.\\n- Give me a quick recipe for a light, pescatarian breakfast ready in under 20 minutes.\\n- What are some easy, lite pescatarian breakfast ideas that can be made in under 20 minutes?\\n- Pescatarian, light breakfast, quick (under 20 minutes).\\n- I need a fast and light pescatarian breakfast, less than 20 minutes.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 214,\n",
      "    \"candidatesTokenCount\": 107,\n",
      "    \"totalTokenCount\": 602,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 214\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 281\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"RyJgaJatAua4xN8P-IaL8AU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:11:34 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:11:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:11:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:11:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:11:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:11:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0010342\n",
      "DEBUG:LiteLLM:response_cost: 0.0010342\n",
      "\u001b[92m19:11:34 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:11:34 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:11:34 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Kid-friendly', 'All day')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Kid-friendly', 'All day')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:11:34 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:11:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:11:34 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:11:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:11:34 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:11:34 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Kid-friendly', 'All day')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Kid-friendly', 'All day')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:11:34 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:11:34 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:11:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:11:35 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:11:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Kid-friendly', 'All day')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'Kid-friendly', 'All day')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1462d0260>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1509a9cd0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14620d850>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:11:38 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=3256'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:11:38 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for kid-friendly holiday meal ideas that can be enjoyed all day.\\n- Suggest some easy, all-day recipes for a kid-friendly holiday feast.\\n- What are some good holiday meals that are suitable for kids and can be prepared or eaten over an extended period?\\n- I need a holiday menu that's kid-friendly and good for an all-day celebration.\\n- Give me some ideas for an all-day, kid-approved holiday spread.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 210,\n",
      "    \"candidatesTokenCount\": 101,\n",
      "    \"totalTokenCount\": 689,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 210\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 378\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"SiJgaI_xGMv2xN8PhqjlqQg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for kid-friendly holiday meal ideas that can be enjoyed all day.\\n- Suggest some easy, all-day recipes for a kid-friendly holiday feast.\\n- What are some good holiday meals that are suitable for kids and can be prepared or eaten over an extended period?\\n- I need a holiday menu that's kid-friendly and good for an all-day celebration.\\n- Give me some ideas for an all-day, kid-approved holiday spread.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 210,\n",
      "    \"candidatesTokenCount\": 101,\n",
      "    \"totalTokenCount\": 689,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 210\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 378\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"SiJgaI_xGMv2xN8PhqjlqQg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:11:38 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:11:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:11:38 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:11:38 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:11:38 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:11:38 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0012605000000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.0012605000000000001\n",
      "\u001b[92m19:11:38 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:11:38 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:11:38 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Party food', 'Nut-free', 'About an hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Party food', 'Nut-free', 'About an hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:11:38 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:11:38 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:11:38 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:11:38 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:11:38 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:11:38 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Party food', 'Nut-free', 'About an hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Party food', 'Nut-free', 'About an hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:11:38 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:11:38 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:11:38 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:11:38 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:11:38 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Party food', 'Nut-free', 'About an hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Party food', 'Nut-free', 'About an hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14620d5b0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1509bcf50> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14620cb00>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:11:45 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=6848'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:11:45 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need nut-free party food that takes about an hour to make.\\n- What's a good party food recipe that's nut-free and can be ready in about an hour?\\n- Nut-free party snacks, 1 hour.\\n- Looking for party food recipes without nuts, around an hour prep/cook time.\\n- Can you suggest some nut-free dishes suitable for a party that take roughly 60 minutes?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 211,\n",
      "    \"candidatesTokenCount\": 93,\n",
      "    \"totalTokenCount\": 1018,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 211\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 714\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"USJgaMe6FqalxN8P3YKjgQY\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need nut-free party food that takes about an hour to make.\\n- What's a good party food recipe that's nut-free and can be ready in about an hour?\\n- Nut-free party snacks, 1 hour.\\n- Looking for party food recipes without nuts, around an hour prep/cook time.\\n- Can you suggest some nut-free dishes suitable for a party that take roughly 60 minutes?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 211,\n",
      "    \"candidatesTokenCount\": 93,\n",
      "    \"totalTokenCount\": 1018,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 211\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 714\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"USJgaMe6FqalxN8P3YKjgQY\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:11:45 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:11:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:11:45 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:11:45 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:11:45 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:11:45 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0020808000000000003\n",
      "DEBUG:LiteLLM:response_cost: 0.0020808000000000003\n",
      "\u001b[92m19:11:45 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:11:45 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:11:45 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Quick bite', 'Sugar-free', 'Instant')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Quick bite', 'Sugar-free', 'Instant')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:11:45 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:11:45 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:11:45 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:11:45 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:11:45 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:11:45 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Quick bite', 'Sugar-free', 'Instant')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Quick bite', 'Sugar-free', 'Instant')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:11:45 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:11:45 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:11:45 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:11:45 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:11:45 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Quick bite', 'Sugar-free', 'Instant')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Quick bite', 'Sugar-free', 'Instant')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14620f6e0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1509aabd0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14620f470>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:11:47 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=2102'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:11:47 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need an instant, sugar-free quick bite.\\n- Give me a recipe for a fast, sugar-free snack.\\n- What are some quick, instant, sugar-free bites I can make?\\n- Suggest an instant sugar-free snack.\\n- I'm looking for a sugar-free quick bite that's instant.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 209,\n",
      "    \"candidatesTokenCount\": 72,\n",
      "    \"totalTokenCount\": 517,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 209\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 236\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"UyJgaKaWLJLAxN8Psc7j6QU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need an instant, sugar-free quick bite.\\n- Give me a recipe for a fast, sugar-free snack.\\n- What are some quick, instant, sugar-free bites I can make?\\n- Suggest an instant sugar-free snack.\\n- I'm looking for a sugar-free quick bite that's instant.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 209,\n",
      "    \"candidatesTokenCount\": 72,\n",
      "    \"totalTokenCount\": 517,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 209\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 236\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"UyJgaKaWLJLAxN8Psc7j6QU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:11:47 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:11:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:11:47 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:11:47 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:11:47 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:11:47 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0008327\n",
      "DEBUG:LiteLLM:response_cost: 0.0008327\n",
      "\u001b[92m19:11:47 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:11:47 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:11:47 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Breakfast', 'Spicy', 'Quick and easy')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Breakfast', 'Spicy', 'Quick and easy')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:11:47 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:11:47 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:11:47 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:11:47 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:11:47 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:11:47 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Breakfast', 'Spicy', 'Quick and easy')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Breakfast', 'Spicy', 'Quick and easy')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:11:47 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:11:47 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:11:47 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:11:47 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:11:47 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Breakfast', 'Spicy', 'Quick and easy')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Breakfast', 'Spicy', 'Quick and easy')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x146211400>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1509bdb50> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1462d1430>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:11:49 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=1587'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:11:49 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a quick and easy spicy breakfast recipe.\\n- Can you give me ideas for a fast and spicy breakfast?\\n- Breakfast, spicy, and quick to make.\\n- I need a simple, hot breakfast.\\n- What's a quick and spicy breakfast dish I can prepare easily?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 209,\n",
      "    \"candidatesTokenCount\": 65,\n",
      "    \"totalTokenCount\": 444,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 209\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 170\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"VSJgaPGcGsyQxN8PvvH28QU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a quick and easy spicy breakfast recipe.\\n- Can you give me ideas for a fast and spicy breakfast?\\n- Breakfast, spicy, and quick to make.\\n- I need a simple, hot breakfast.\\n- What's a quick and spicy breakfast dish I can prepare easily?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 209,\n",
      "    \"candidatesTokenCount\": 65,\n",
      "    \"totalTokenCount\": 444,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 209\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 170\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"VSJgaPGcGsyQxN8PvvH28QU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:11:49 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:11:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:11:49 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:11:49 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:11:49 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:11:49 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0006502\n",
      "DEBUG:LiteLLM:response_cost: 0.0006502\n",
      "\u001b[92m19:11:49 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:11:49 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:11:49 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lunch', 'Keto', '30 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lunch', 'Keto', '30 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:11:49 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:11:49 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:11:49 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:11:49 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:11:49 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:11:49 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lunch', 'Keto', '30 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lunch', 'Keto', '30 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:11:49 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:11:49 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:11:49 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:11:49 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:11:49 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lunch', 'Keto', '30 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lunch', 'Keto', '30 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1462d1f70>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1509bee50> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1444d53d0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:11:51 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=2145'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:11:51 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a keto lunch recipe that can be made in 30 minutes.\\n- Give me some quick keto lunch ideas, around 30 minutes prep time.\\n- Lunch, keto, 30 minutes.\\n- What's a good keto lunch I can whip up in half an hour?\\n- Need a 30-minute keto lunch recipe.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 209,\n",
      "    \"candidatesTokenCount\": 80,\n",
      "    \"totalTokenCount\": 486,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 209\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 197\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"VyJgaL-tKri1vdIPtPS9uQg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a keto lunch recipe that can be made in 30 minutes.\\n- Give me some quick keto lunch ideas, around 30 minutes prep time.\\n- Lunch, keto, 30 minutes.\\n- What's a good keto lunch I can whip up in half an hour?\\n- Need a 30-minute keto lunch recipe.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 209,\n",
      "    \"candidatesTokenCount\": 80,\n",
      "    \"totalTokenCount\": 486,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 209\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 197\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"VyJgaL-tKri1vdIPtPS9uQg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:11:51 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:11:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:11:51 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:11:51 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:11:51 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:11:51 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0007551999999999999\n",
      "DEBUG:LiteLLM:response_cost: 0.0007551999999999999\n",
      "\u001b[92m19:11:51 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:11:51 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:11:51 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:11:51 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:11:51 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:11:51 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:11:51 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:11:51 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:11:51 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:11:51 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:11:51 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:11:51 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:11:51 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:11:51 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dinner', 'Heart-healthy', 'Under 1 hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14126d3a0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1509bce50> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1462d15b0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:11:58 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=6995'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:11:58 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a heart-healthy dinner that takes under an hour to prepare.\\n- Quick heart-healthy dinner recipes needed.\\n- What's a good heart-healthy dinner I can make in less than 60 minutes?\\n- Give me ideas for a healthy dinner, fast, and good for my heart.\\n- Dinner, heart-healthy, under an hour.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 211,\n",
      "    \"candidatesTokenCount\": 81,\n",
      "    \"totalTokenCount\": 575,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 211\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 283\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"XiJgaPqHNcvcxs0PiPW0oAg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a heart-healthy dinner that takes under an hour to prepare.\\n- Quick heart-healthy dinner recipes needed.\\n- What's a good heart-healthy dinner I can make in less than 60 minutes?\\n- Give me ideas for a healthy dinner, fast, and good for my heart.\\n- Dinner, heart-healthy, under an hour.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 211,\n",
      "    \"candidatesTokenCount\": 81,\n",
      "    \"totalTokenCount\": 575,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 211\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 283\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"XiJgaPqHNcvcxs0PiPW0oAg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:11:58 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:11:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:11:58 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:11:58 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:11:58 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:11:58 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0009733\n",
      "DEBUG:LiteLLM:response_cost: 0.0009733\n",
      "\u001b[92m19:11:58 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:11:58 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:11:58 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Snack', 'Low-salt', '10 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Snack', 'Low-salt', '10 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:11:58 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:11:58 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:11:58 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:11:58 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:11:58 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:11:58 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Snack', 'Low-salt', '10 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Snack', 'Low-salt', '10 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:11:58 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:11:58 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:11:58 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:11:58 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:11:58 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Snack', 'Low-salt', '10 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Snack', 'Low-salt', '10 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1462d17f0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1509bfc50> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1462d13d0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:12:06 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=7078'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:12:06 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a low-salt snack recipe that takes about 10 minutes to make.\\n- Give me some quick 10-minute snack ideas that are low in salt.\\n- Snack, low-salt, fast.\\n- I need a healthy, low-sodium snack that can be prepared in 10 minutes or less.\\n- What's a good 10-minute, low-salt snack?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 211,\n",
      "    \"candidatesTokenCount\": 92,\n",
      "    \"totalTokenCount\": 479,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 211\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 176\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"ZiJgaJHMBNH4xN8PwJ6QuAg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a low-salt snack recipe that takes about 10 minutes to make.\\n- Give me some quick 10-minute snack ideas that are low in salt.\\n- Snack, low-salt, fast.\\n- I need a healthy, low-sodium snack that can be prepared in 10 minutes or less.\\n- What's a good 10-minute, low-salt snack?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 211,\n",
      "    \"candidatesTokenCount\": 92,\n",
      "    \"totalTokenCount\": 479,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 211\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 176\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"ZiJgaJHMBNH4xN8PwJ6QuAg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:12:06 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:12:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:12:06 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:12:06 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:12:06 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:12:06 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0007333\n",
      "DEBUG:LiteLLM:response_cost: 0.0007333\n",
      "\u001b[92m19:12:06 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:12:06 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:12:06 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dessert', 'Diabetic-friendly', '45 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dessert', 'Diabetic-friendly', '45 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:12:06 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:12:06 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:12:06 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:12:06 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:12:06 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:12:06 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dessert', 'Diabetic-friendly', '45 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dessert', 'Diabetic-friendly', '45 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:12:06 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:12:06 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:12:06 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:12:06 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:12:06 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dessert', 'Diabetic-friendly', '45 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dessert', 'Diabetic-friendly', '45 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1462059a0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1509c06d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x141498830>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:12:09 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=3430'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:12:09 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a diabetic-friendly dessert that takes about 45 minutes to prepare.\\n- Can you suggest a dessert recipe for diabetics that can be made in 45 minutes?\\n- Diabetic dessert, 45 minutes max.\\n- Recipe for a dessert, low in sugar, ready in under 45 minutes.\\n- Need a quick diabetic dessert, around 45 minutes prep time.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 212,\n",
      "    \"candidatesTokenCount\": 87,\n",
      "    \"totalTokenCount\": 607,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 212\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 308\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"aSJgaK_SJ6WsxN8Pqtm_8AU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a diabetic-friendly dessert that takes about 45 minutes to prepare.\\n- Can you suggest a dessert recipe for diabetics that can be made in 45 minutes?\\n- Diabetic dessert, 45 minutes max.\\n- Recipe for a dessert, low in sugar, ready in under 45 minutes.\\n- Need a quick diabetic dessert, around 45 minutes prep time.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 212,\n",
      "    \"candidatesTokenCount\": 87,\n",
      "    \"totalTokenCount\": 607,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 212\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 308\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"aSJgaK_SJ6WsxN8Pqtm_8AU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:12:09 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:12:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:12:09 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:12:09 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:12:09 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:12:09 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0010511000000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.0010511000000000001\n",
      "\u001b[92m19:12:09 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:12:09 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:12:09 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Brunch', 'High-fiber', 'Long prep')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Brunch', 'High-fiber', 'Long prep')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:12:09 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:12:09 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:12:09 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:12:09 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:12:09 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:12:09 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Brunch', 'High-fiber', 'Long prep')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Brunch', 'High-fiber', 'Long prep')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:12:09 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:12:09 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:12:09 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:12:09 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:12:09 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Brunch', 'High-fiber', 'Long prep')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Brunch', 'High-fiber', 'Long prep')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x146207920>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1509be9d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14128fd70>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:12:11 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=1378'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:12:11 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a quick and budget-friendly dinner recipe.\\n- Give me ideas for a fast and cheap dinner.\\n- What's a good dinner that's quick to make and doesn't cost much?\\n- I'm looking for a speedy, affordable dinner.\\n- Dinner, quick and budget-friendly.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 210,\n",
      "    \"candidatesTokenCount\": 68,\n",
      "    \"totalTokenCount\": 413,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 210\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 135\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"ayJgaMHQCP-exN8PzJzu8QU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a quick and budget-friendly dinner recipe.\\n- Give me ideas for a fast and cheap dinner.\\n- What's a good dinner that's quick to make and doesn't cost much?\\n- I'm looking for a speedy, affordable dinner.\\n- Dinner, quick and budget-friendly.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 210,\n",
      "    \"candidatesTokenCount\": 68,\n",
      "    \"totalTokenCount\": 413,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 210\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 135\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"ayJgaMHQCP-exN8PzJzu8QU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:12:11 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:12:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:12:11 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:12:11 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:12:11 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:12:11 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0005705\n",
      "DEBUG:LiteLLM:response_cost: 0.0005705\n",
      "\u001b[92m19:12:11 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:12:11 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:12:11 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Appetizer', 'Lactose-free', '15 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Appetizer', 'Lactose-free', '15 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:12:11 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:12:11 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:12:11 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:12:11 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:12:11 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:12:11 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Appetizer', 'Lactose-free', '15 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Appetizer', 'Lactose-free', '15 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:12:11 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:12:11 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:12:11 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:12:11 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:12:11 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Appetizer', 'Lactose-free', '15 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Appetizer', 'Lactose-free', '15 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x146205d60>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1509a8b50> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x146205d90>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:12:14 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=3199'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:12:14 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a quick appetizer that is lactose-free and can be made in 15 minutes.\\n- Suggest a 15-minute, dairy-free appetizer recipe.\\n- Do you have any ideas for a fast appetizer that's lactose-free and takes under 15 minutes?\\n- Lactose-free appetizer, 15 minutes.\\n- I need a dairy-free appetizer recipe that's quick, around 15 minutes to prepare.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 213,\n",
      "    \"candidatesTokenCount\": 101,\n",
      "    \"totalTokenCount\": 780,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 213\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 466\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"biJgaP_sG6_UvdIP6PqeuAg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a quick appetizer that is lactose-free and can be made in 15 minutes.\\n- Suggest a 15-minute, dairy-free appetizer recipe.\\n- Do you have any ideas for a fast appetizer that's lactose-free and takes under 15 minutes?\\n- Lactose-free appetizer, 15 minutes.\\n- I need a dairy-free appetizer recipe that's quick, around 15 minutes to prepare.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 213,\n",
      "    \"candidatesTokenCount\": 101,\n",
      "    \"totalTokenCount\": 780,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 213\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 466\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"biJgaP_sG6_UvdIP6PqeuAg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:12:14 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:12:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:12:14 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:12:14 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:12:14 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:12:14 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0014814\n",
      "DEBUG:LiteLLM:response_cost: 0.0014814\n",
      "\u001b[92m19:12:14 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:12:14 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:12:14 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Comfort food', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Comfort food', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:12:14 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:12:14 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:12:14 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:12:14 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:12:14 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:12:14 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Comfort food', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Comfort food', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:12:14 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:12:14 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:12:14 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:12:14 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:12:14 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Comfort food', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Comfort food', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x141236720>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1509a9450> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14126d190>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Type', b'application/json; charset=UTF-8'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:12:14 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=149'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 429 Too Many Requests\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:12:14 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2268 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "DEBUG:LiteLLM:Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m19:12:14 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2165 - Logging Details LiteLLM-Failure Call: []\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Failure Call: []\n",
      "DEBUG:__main__:Retrying __main__.call_llm in 3.392883376988062 seconds as it raised RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
      "    \"details\": [\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
      "        \"violations\": [\n",
      "          {\n",
      "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
      "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
      "            \"quotaDimensions\": {\n",
      "              \"location\": \"global\",\n",
      "              \"model\": \"gemini-2.5-flash\"\n",
      "            },\n",
      "            \"quotaValue\": \"10\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
      "        \"links\": [\n",
      "          {\n",
      "            \"description\": \"Learn more about Gemini API quotas\",\n",
      "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
      "        \"retryDelay\": \"45s\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      ".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m19:12:18 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:12:18 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:12:18 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Comfort food', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Comfort food', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:12:18 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:12:18 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:12:18 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:12:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:12:18 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:12:18 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Comfort food', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Comfort food', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:12:18 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:12:18 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:12:18 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:12:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:12:18 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Comfort food', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Comfort food', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x146205c40>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1509bd3d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x141274c20>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Type', b'application/json; charset=UTF-8'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:12:18 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=153'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 429 Too Many Requests\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:12:18 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2268 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "DEBUG:LiteLLM:Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m19:12:18 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2165 - Logging Details LiteLLM-Failure Call: []\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Failure Call: []\n",
      "DEBUG:__main__:Retrying __main__.call_llm in 3.9198267159697546 seconds as it raised RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
      "    \"details\": [\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
      "        \"violations\": [\n",
      "          {\n",
      "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
      "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
      "            \"quotaDimensions\": {\n",
      "              \"model\": \"gemini-2.5-flash\",\n",
      "              \"location\": \"global\"\n",
      "            },\n",
      "            \"quotaValue\": \"10\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
      "        \"links\": [\n",
      "          {\n",
      "            \"description\": \"Learn more about Gemini API quotas\",\n",
      "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
      "        \"retryDelay\": \"41s\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      ".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m19:12:22 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:12:22 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:12:22 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Comfort food', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Comfort food', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:12:22 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:12:22 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:12:22 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:12:22 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:12:22 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:12:22 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Comfort food', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Comfort food', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:12:22 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:12:22 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:12:22 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:12:22 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:12:22 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Comfort food', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Comfort food', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x146205310>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1509a91d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145a07bc0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Type', b'application/json; charset=UTF-8'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:12:22 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=36'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 429 Too Many Requests\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:12:22 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2268 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "DEBUG:LiteLLM:Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m19:12:22 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2165 - Logging Details LiteLLM-Failure Call: []\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Failure Call: []\n",
      "DEBUG:__main__:Retrying __main__.call_llm in 3.653820381801669 seconds as it raised RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
      "    \"details\": [\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
      "        \"violations\": [\n",
      "          {\n",
      "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
      "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
      "            \"quotaDimensions\": {\n",
      "              \"model\": \"gemini-2.5-flash\",\n",
      "              \"location\": \"global\"\n",
      "            },\n",
      "            \"quotaValue\": \"10\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
      "        \"links\": [\n",
      "          {\n",
      "            \"description\": \"Learn more about Gemini API quotas\",\n",
      "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
      "        \"retryDelay\": \"37s\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      ".\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m19:12:26 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:12:26 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:12:26 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Comfort food', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Comfort food', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:12:26 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:12:26 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:12:26 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:12:26 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:12:26 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:12:26 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Comfort food', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Comfort food', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:12:26 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:12:26 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:12:26 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:12:26 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:12:26 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Comfort food', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Comfort food', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1412362a0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1509aa1d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14620c200>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Type', b'application/json; charset=UTF-8'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:12:26 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=150'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 429 Too Many Requests\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:12:26 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2268 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "DEBUG:LiteLLM:Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m19:12:26 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2165 - Logging Details LiteLLM-Failure Call: []\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Failure Call: []\n",
      "DEBUG:__main__:Retrying __main__.call_llm in 4.339811805208668 seconds as it raised RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
      "    \"details\": [\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
      "        \"violations\": [\n",
      "          {\n",
      "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
      "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
      "            \"quotaDimensions\": {\n",
      "              \"model\": \"gemini-2.5-flash\",\n",
      "              \"location\": \"global\"\n",
      "            },\n",
      "            \"quotaValue\": \"10\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
      "        \"links\": [\n",
      "          {\n",
      "            \"description\": \"Learn more about Gemini API quotas\",\n",
      "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
      "        \"retryDelay\": \"33s\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      ".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m19:12:30 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:12:30 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:12:30 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Comfort food', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Comfort food', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:12:30 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:12:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:12:30 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:12:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:12:30 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:12:30 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Comfort food', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Comfort food', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:12:30 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:12:30 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:12:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:12:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:12:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Comfort food', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Side Dish', 'Comfort food', '20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1462059a0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1509a8dd0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1444d4f50>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:12:35 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=4720'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:12:35 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a comforting side dish that I can make in 20 minutes.\\n- Give me a quick recipe for a comfort food side dish, something that takes around 20 minutes.\\n- Comfort food side, 20 minutes prep.\\n- I need a fast, comforting side dish recipe, under 20 minutes.\\n- What's a good 20-minute comfort food side dish?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 210,\n",
      "    \"candidatesTokenCount\": 90,\n",
      "    \"totalTokenCount\": 520,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 210\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 220\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"gyJgaLT6JcyQxN8PvvH28QU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a comforting side dish that I can make in 20 minutes.\\n- Give me a quick recipe for a comfort food side dish, something that takes around 20 minutes.\\n- Comfort food side, 20 minutes prep.\\n- I need a fast, comforting side dish recipe, under 20 minutes.\\n- What's a good 20-minute comfort food side dish?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 210,\n",
      "    \"candidatesTokenCount\": 90,\n",
      "    \"totalTokenCount\": 520,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 210\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 220\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"gyJgaLT6JcyQxN8PvvH28QU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:12:35 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:12:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:12:35 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:12:35 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:12:35 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:12:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0008380000000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.0008380000000000001\n",
      "\u001b[92m19:12:35 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:12:35 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:12:35 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:12:35 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:12:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:12:35 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:12:35 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:12:35 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:12:35 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:12:35 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:12:35 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:12:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:12:35 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:12:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lite breakfast', 'Paleo', 'Less than 20 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14593da60>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1509abe50> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x141274c20>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:12:38 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=2347'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:12:38 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a light paleo breakfast recipe that takes less than 20 minutes to prepare.\\n- What are some quick paleo breakfast ideas that are light and can be made in under 20 minutes?\\n- Give me a recipe for a lite paleo breakfast, ready in less than 20 minutes.\\n- Paleo, light breakfast, under 20 minutes.\\n- Fast and light paleo breakfast, quick, under 20 mins.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 213,\n",
      "    \"candidatesTokenCount\": 92,\n",
      "    \"totalTokenCount\": 600,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 213\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 295\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"hiJgaOCBBfaIvdIPv8_KmQg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a light paleo breakfast recipe that takes less than 20 minutes to prepare.\\n- What are some quick paleo breakfast ideas that are light and can be made in under 20 minutes?\\n- Give me a recipe for a lite paleo breakfast, ready in less than 20 minutes.\\n- Paleo, light breakfast, under 20 minutes.\\n- Fast and light paleo breakfast, quick, under 20 mins.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 213,\n",
      "    \"candidatesTokenCount\": 92,\n",
      "    \"totalTokenCount\": 600,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 213\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 295\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"hiJgaOCBBfaIvdIPv8_KmQg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:12:38 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:12:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:12:38 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:12:38 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:12:38 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:12:38 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0010314\n",
      "DEBUG:LiteLLM:response_cost: 0.0010314\n",
      "\u001b[92m19:12:38 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:12:38 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:12:38 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:12:38 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:12:38 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:12:38 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:12:38 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:12:38 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:12:38 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:12:38 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:12:38 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:12:38 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:12:38 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:12:38 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Holiday meal', 'with hummus and carrots', 'Overnight')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1462065a0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x15099d650> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14620cc20>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:12:41 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=3157'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:12:41 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for an overnight holiday meal recipe that includes hummus and carrots.\\n- Give me an overnight holiday dish with hummus and carrots.\\n- Holiday meal, overnight preparation, featuring hummus and carrots.\\n- I need a holiday meal that can be prepared overnight and has hummus and carrots.\\n- Overnight holiday recipe with hummus and carrots, please.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 211,\n",
      "    \"candidatesTokenCount\": 73,\n",
      "    \"totalTokenCount\": 500,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 211\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 216\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"iSJgaPzdFZX4xN8PstOIyQg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for an overnight holiday meal recipe that includes hummus and carrots.\\n- Give me an overnight holiday dish with hummus and carrots.\\n- Holiday meal, overnight preparation, featuring hummus and carrots.\\n- I need a holiday meal that can be prepared overnight and has hummus and carrots.\\n- Overnight holiday recipe with hummus and carrots, please.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 211,\n",
      "    \"candidatesTokenCount\": 73,\n",
      "    \"totalTokenCount\": 500,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 211\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 216\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"iSJgaPzdFZX4xN8PstOIyQg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:12:41 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:12:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:12:41 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:12:41 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:12:41 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:12:41 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0007858\n",
      "DEBUG:LiteLLM:response_cost: 0.0007858\n",
      "\u001b[92m19:12:41 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:12:41 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:12:41 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Party food', 'Not spicy', 'About an hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Party food', 'Not spicy', 'About an hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:12:41 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:12:41 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:12:41 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:12:41 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:12:41 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:12:41 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Party food', 'Not spicy', 'About an hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Party food', 'Not spicy', 'About an hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:12:41 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:12:41 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:12:41 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:12:41 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:12:41 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Party food', 'Not spicy', 'About an hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Party food', 'Not spicy', 'About an hour')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145934b30>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x15099d4d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1459344d0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:12:44 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=2916'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:12:44 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a party food recipe that is not spicy and takes about an hour to make.\\n- What are some mild party dishes I can prepare in around 60 minutes?\\n- Give me a non-spicy party food idea that can be ready in about an hour.\\n- Show me recipes for party food, mild taste, takes roughly an hour.\\n- I'm looking for a party dish, non-spicy, that takes an hour to cook.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 210,\n",
      "    \"candidatesTokenCount\": 98,\n",
      "    \"totalTokenCount\": 578,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 210\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 270\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"jCJgaOuaGOrmxN8P6P7oqQg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a party food recipe that is not spicy and takes about an hour to make.\\n- What are some mild party dishes I can prepare in around 60 minutes?\\n- Give me a non-spicy party food idea that can be ready in about an hour.\\n- Show me recipes for party food, mild taste, takes roughly an hour.\\n- I'm looking for a party dish, non-spicy, that takes an hour to cook.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 210,\n",
      "    \"candidatesTokenCount\": 98,\n",
      "    \"totalTokenCount\": 578,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 210\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 270\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"jCJgaOuaGOrmxN8P6P7oqQg\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:12:44 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:12:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:12:44 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:12:44 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:12:44 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:12:44 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.000983\n",
      "DEBUG:LiteLLM:response_cost: 0.000983\n",
      "\u001b[92m19:12:44 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:12:44 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:12:44 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Quick bite', 'Vegetarian', 'Instant')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Quick bite', 'Vegetarian', 'Instant')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:12:44 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:12:44 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:12:44 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:12:44 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:12:44 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:12:44 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Quick bite', 'Vegetarian', 'Instant')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Quick bite', 'Vegetarian', 'Instant')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:12:44 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:12:44 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:12:44 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:12:44 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:12:44 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Quick bite', 'Vegetarian', 'Instant')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Quick bite', 'Vegetarian', 'Instant')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14128fd70>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x15099c7d0> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145468ef0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:12:47 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=2793'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:12:47 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a quick, instant, vegetarian bite.\\n- Give me some ideas for an instant vegetarian quick bite.\\n- What are some recipes for quick vegetarian snacks that can be made instantly?\\n- I need a very fast vegetarian bite.\\n- Quick, instant, vegetarian meal ideas, please.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 208,\n",
      "    \"candidatesTokenCount\": 65,\n",
      "    \"totalTokenCount\": 491,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 208\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 218\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"jyJgaIPTGJrQvdIP18eEgQY\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a quick, instant, vegetarian bite.\\n- Give me some ideas for an instant vegetarian quick bite.\\n- What are some recipes for quick vegetarian snacks that can be made instantly?\\n- I need a very fast vegetarian bite.\\n- Quick, instant, vegetarian meal ideas, please.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 208,\n",
      "    \"candidatesTokenCount\": 65,\n",
      "    \"totalTokenCount\": 491,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 208\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 218\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"jyJgaIPTGJrQvdIP18eEgQY\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:12:47 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:12:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:12:47 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:12:47 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:12:47 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:12:47 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0007699\n",
      "DEBUG:LiteLLM:response_cost: 0.0007699\n",
      "\u001b[92m19:12:47 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:12:47 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:12:47 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dinner', 'Gluten-free', '10 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dinner', 'Gluten-free', '10 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:12:47 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:12:47 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:12:47 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:12:47 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:12:47 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:12:47 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dinner', 'Gluten-free', '10 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dinner', 'Gluten-free', '10 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:12:47 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:12:47 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:12:47 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:12:47 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:12:47 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dinner', 'Gluten-free', '10 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Dinner', 'Gluten-free', '10 minutes')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14593f710>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x15099e150> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145934a40>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:12:52 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=4495'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:12:52 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a gluten-free dinner recipe that can be made in 10 minutes.\\n- Give me ideas for a quick 10-minute gluten-free dinner.\\n- What's a fast, gluten-free dinner I can cook in 10 minutes?\\n- Dinner, gluten-free, 10 minutes.\\n- I'm looking for a speedy gluten-free dinner, ready in under 10 minutes.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 211,\n",
      "    \"candidatesTokenCount\": 93,\n",
      "    \"totalTokenCount\": 578,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 211\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 274\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"lCJgaN-DBIvyxN8P-ZeJsAc\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I need a gluten-free dinner recipe that can be made in 10 minutes.\\n- Give me ideas for a quick 10-minute gluten-free dinner.\\n- What's a fast, gluten-free dinner I can cook in 10 minutes?\\n- Dinner, gluten-free, 10 minutes.\\n- I'm looking for a speedy gluten-free dinner, ready in under 10 minutes.\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 211,\n",
      "    \"candidatesTokenCount\": 93,\n",
      "    \"totalTokenCount\": 578,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 211\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 274\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"lCJgaN-DBIvyxN8P-ZeJsAc\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:12:52 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:12:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:12:52 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:12:52 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:12:52 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:12:52 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0009808000000000002\n",
      "DEBUG:LiteLLM:response_cost: 0.0009808000000000002\n",
      "\u001b[92m19:12:52 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:12:52 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m19:12:52 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.completion(model='gemini/gemini-2.5-flash', messages=[{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}])\u001b[0m\n",
      "\u001b[92m19:12:52 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m19:12:52 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:12:52 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "DEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m19:12:52 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}\n",
      "\u001b[92m19:12:52 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m19:12:52 - LiteLLM:DEBUG\u001b[0m: utils.py:3046 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-2.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m19:12:52 - LiteLLM:DEBUG\u001b[0m: utils.py:3049 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m19:12:52 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - Final returned optional params: {}\n",
      "DEBUG:LiteLLM:Final returned optional params: {}\n",
      "\u001b[92m19:12:52 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:461 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m19:12:52 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:12:52 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:908 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****PifE \\\n",
      "-H 'Content-Type: ap****on' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': \"\\n        You are AI assisntant that helps to generate user queryies for a recipe chatbot.\\n        You will be given a list of tuples, that describe dimentions for a user query. Given these dimentions, generate 4-5 user queries. \\n\\n        <dimension>\\n        ('Lunch', 'Meaty', 'something quick')\\n        </dimension>\\n\\n        <example>\\n        <dimension>\\n        ('Breakfast', 'Not spicy', '10 minutes')\\n        </dimension>\\n\\n        - I want to make a quick breakfast that is not spicy and takes 10 minutes to prepare.\\n        - Give me a quick recipe for plain breakfast. \\n        - Breakfast, fast and non-spicy.\\n        - A ten-minute breakfast meal, non-spicy.\\n        - I need breakfast under 10 minutes, it should be mild to taste. \\n        </example>\\n\\n        Just output user queries, no other text.\\n    \"}]}], 'generationConfig': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x14620d850>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x15099f150> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145937590>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Sat, 28 Jun 2025 17:12:57 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=4848'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDoGN4ExkfA7WXZkp_9-ORdTt4C9fCPifE \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "\u001b[92m19:12:57 - LiteLLM:DEBUG\u001b[0m: utils.py:340 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a quick and meaty lunch.\\n- Can you suggest a fast, meat-based recipe for lunch?\\n- Quick meaty lunch ideas.\\n- I need a speedy lunch meal with meat.\\n- What's a good, quick, meaty lunch?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 208,\n",
      "    \"candidatesTokenCount\": 58,\n",
      "    \"totalTokenCount\": 554,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 208\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 288\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"mSJgaManApvpvdIP-unz-QU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"- I'm looking for a quick and meaty lunch.\\n- Can you suggest a fast, meat-based recipe for lunch?\\n- Quick meaty lunch ideas.\\n- I need a speedy lunch meal with meat.\\n- What's a good, quick, meaty lunch?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 208,\n",
      "    \"candidatesTokenCount\": 58,\n",
      "    \"totalTokenCount\": 554,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 208\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 288\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"mSJgaManApvpvdIP-unz-QU\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "\u001b[92m19:12:57 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:12:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "INFO:LiteLLM:selected model name for cost calculation: gemini/gemini-2.5-flash\n",
      "\u001b[92m19:12:57 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:12:57 - LiteLLM:DEBUG\u001b[0m: utils.py:4401 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m19:12:57 - LiteLLM:DEBUG\u001b[0m: utils.py:4701 - model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "DEBUG:LiteLLM:model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000}\n",
      "\u001b[92m19:12:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1131 - response_cost: 0.0009274000000000001\n",
      "DEBUG:LiteLLM:response_cost: 0.0009274000000000001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {('Breakfast',\n",
       "              'Not spicy',\n",
       "              '10 minutes'): [\"- I'm looking for a sweet vegetarian dinner recipe that takes about 30 minutes.\\n- Can you suggest a quick vegetarian dinner? Something sweet and ready in half an hour.\\n- Dinner, vegetarian, sweet, 30 min.\\n- What's a good sweet vegetarian dinner I can make in under 30 minutes?\\n- I need a 30-minute vegetarian dinner. Make it sweet.\"],\n",
       "             ('Lunch',\n",
       "              'Vegetarian',\n",
       "              '30 minutes'): [\"- I need a vegetarian lunch recipe that I can make in 30 minutes.\\n- Give me ideas for a quick vegetarian lunch, ready in half an hour.\\n- What's a good vegetarian lunch that takes 30 minutes or less?\\n- Looking for a fast, vegetarian lunch recipe, 30-minute prep time.\\n- Show me some 30-minute vegetarian lunch options.\"],\n",
       "             ('Dinner',\n",
       "              'Meaty',\n",
       "              'Under 1 hour'): [\"- I'm looking for a meaty dinner recipe that takes less than an hour to make.\\n- Can you suggest a quick dinner idea that's hearty and meat-based, ready in under 60 minutes?\\n- Give me a meaty dinner that's under an hour.\\n- I need a fast dinner, something meaty, that can be done in less than 60 minutes.\\n- What's a good speedy, meaty dinner recipe?\"],\n",
       "             ('Snack',\n",
       "              'Gluten-free',\n",
       "              'Quick'): ['- I need a quick and gluten-free snack recipe.\\n- Give me some ideas for fast gluten-free snacks.\\n- Quick gluten-free snack.\\n- Snack, gluten-free, quick.\\n- What are some easy gluten-free snacks to make?'],\n",
       "             ('Dessert',\n",
       "              'Dairy-free',\n",
       "              '45 minutes'): [\"- I'm looking for a dairy-free dessert that I can make in under 45 minutes.\\n- Give me some dessert ideas that are dairy-free and take less than 45 minutes.\\n- Quick, dairy-free dessert, 45-minute prep.\\n- I need a dessert recipe that's free of dairy and can be made within 45 minutes.\\n- What's a good dairy-free dessert I can whip up in 45 minutes?\"],\n",
       "             ('Brunch',\n",
       "              'Vegan',\n",
       "              'I have all the time until retirement'): [\"- I'm looking for an elaborate vegan brunch recipe; I have all the time in the world to cook.\\n- Give me some vegan brunch ideas that take a long time to prepare.\\n- Suggest a time-intensive vegan brunch recipe.\\n- I want to make a vegan brunch, and I have unlimited time for cooking.\\n- Vegan brunch, please, something that's quite involved and takes a while to make.\"],\n",
       "             ('Appetizer',\n",
       "              'Low-carb',\n",
       "              '15 minutes'): [\"- I need a low-carb appetizer that can be made in 15 minutes.\\n- Give me a quick low-carb appetizer recipe under 15 minutes.\\n- Appetizer, low-carb, 15 minutes.\\n- What are some fast low-carb appetizers that take about 15 minutes?\\n- I'm looking for a 15-minute appetizer that's low in carbs.\"],\n",
       "             ('Side Dish',\n",
       "              'High-protein',\n",
       "              '20 minutes'): ['- I need a high-protein side dish that can be made in about 20 minutes.\\n- Give me a quick recipe for a high-protein side dish, ready in 20 minutes.\\n- High-protein side dish, 20 minutes.\\n- What are some fast, high-protein side dishes I can make in under 20 minutes?\\n- 20-minute side dish, high in protein.'],\n",
       "             ('Lite breakfast',\n",
       "              'Kid-friendly',\n",
       "              'Less than 20 minutes'): [\"- I'm looking for a quick, light breakfast that's kid-friendly and takes less than 20 minutes.\\n- Can you suggest a simple, healthy breakfast for kids that can be made in under 20 minutes?\\n- Kid-friendly, lite breakfast, under 20 mins.\\n- Give me fast and light breakfast ideas suitable for children, ready in less than 20 minutes.\\n- I need a quick, mild breakfast recipe for my child that's ready in under 20 minutes.\"],\n",
       "             ('Holiday meal',\n",
       "              'Pescatarian',\n",
       "              'All day'): [\"- I need an all-day pescatarian recipe for a holiday meal.\\n- What are some good pescatarian options for a holiday feast that can be prepared over several hours?\\n- Give me ideas for a special, pescatarian holiday dinner that takes all day to cook.\\n- Holiday meal, pescatarian, long cooking time.\\n- I'm looking for a celebratory pescatarian dish that I can prepare throughout the day.\"],\n",
       "             ('Party food',\n",
       "              'Sugar-free',\n",
       "              'About an hour'): [\"- Can you suggest some sugar-free party food recipes that take about an hour to make?\\n- I'm looking for party dishes that are sugar-free and can be prepared in roughly an hour.\\n- Give me ideas for party food, sugar-free, under an hour.\\n- What are some good sugar-free party snacks I can make in about 60 minutes?\\n- Party food, sugar-free, about an hour prep time.\"],\n",
       "             ('Quick bite',\n",
       "              'Nut-free',\n",
       "              'Instant'): ['- I need a quick, nut-free, instant bite.\\n- Give me a recipe for an instant, nut-free quick bite.\\n- Looking for a quick, instant snack that is nut-free.\\n- Instant, nut-free quick bite ideas.\\n- What are some quick and nut-free instant snacks?'],\n",
       "             ('Breakfast',\n",
       "              'Keto',\n",
       "              '30 minutes'): [\"- I need a keto breakfast recipe that I can make in 30 minutes.\\n- Give me some quick keto breakfast ideas that take under 30 minutes.\\n- What's a good 30-minute keto recipe for breakfast?\\n- Keto breakfast, 30 mins.\\n- Show me breakfast recipes for a keto diet that are ready in half an hour.\"],\n",
       "             ('Lunch',\n",
       "              'Heart-healthy',\n",
       "              'Under 1 hour'): [\"- I need a heart-healthy lunch recipe that takes less than an hour to make.\\n- What are some quick, heart-healthy lunch ideas I can prepare in under 60 minutes?\\n- Give me a recipe for a healthy lunch that's good for my heart and ready in under an hour.\\n- Looking for lunch, heart-healthy, under 1 hour.\\n- Can you suggest a heart-healthy lunch that I can whip up quickly, within an hour?\"],\n",
       "             ('Dinner',\n",
       "              'Spicy',\n",
       "              '45 minutes'): [\"- I'm looking for a spicy dinner recipe that takes about 45 minutes to prepare.\\n- Can you suggest a hot and flavorful dinner I can make in under 45 minutes?\\n- Spicy dinner, 45 minutes prep time.\\n- I need to cook a spicy dinner, and I only have 45 minutes.\\n- Give me a quick spicy dinner recipe that's done in 45 minutes.\"],\n",
       "             ('Snack',\n",
       "              'without fish or meat',\n",
       "              '10 minutes'): [\"- I'm looking for a quick snack that's vegetarian or vegan and can be made in 10 minutes.\\n- Give me a 10-minute snack recipe without any meat or fish.\\n- Snack, no meat, fast, 10 minutes.\\n- A quick snack recipe, takes less than 10 minutes and contains no fish or meat.\\n- I need a non-meat snack for under 10 minutes.\"],\n",
       "             ('Dessert',\n",
       "              'Meaty',\n",
       "              'Long prep'): [\"- I'm looking for a meaty dessert that requires long preparation time.\\n- Suggest a rich and hearty dessert that takes a long time to prepare.\\n- Give me ideas for a complex, meaty dessert.\\n- Dessert recipe, long prep, and meaty.\\n- What's a substantial dessert I can make that has a long prep time?\"],\n",
       "             ('Brunch',\n",
       "              'Paleo',\n",
       "              'Quick'): [\"- I'm looking for a quick paleo brunch recipe.\\n- Give me fast brunch ideas that are paleo-friendly.\\n- Paleo brunch, quick to prepare.\\n- What's a good quick paleo option for brunch?\\n- I need a speedy paleo brunch recipe.\"],\n",
       "             ('Appetizer',\n",
       "              'Low-salt',\n",
       "              '15 minutes'): [\"- I'm looking for a low-salt appetizer that can be made in 15 minutes.\\n- Give me a quick recipe for a low-salt appetizer, ready in under 15 minutes.\\n- Appetizer, low-salt, 15 minutes.\\n- Need a 15-minute appetizer that's low in salt.\\n- What's a good low-sodium appetizer I can whip up in 15 minutes?\"],\n",
       "             ('Side Dish',\n",
       "              'Diabetic-friendly',\n",
       "              '20 minutes'): [\"- I'm looking for a diabetic-friendly side dish that I can make in about 20 minutes.\\n- Give me a quick recipe for a diabetic side dish that takes 20 minutes.\\n- Diabetic side dish, 20 minutes.\\n- What are some easy side dishes for diabetics that can be prepared in under 20 minutes?\\n- I need a 20-minute side dish recipe suitable for a diabetic diet.\"],\n",
       "             ('Lite breakfast',\n",
       "              'High-fiber',\n",
       "              'Less than 20 minutes'): [\"- I'm looking for a light, high-fiber breakfast that takes less than 20 minutes to make.\\n- Give me a quick, high-fiber breakfast recipe that's light.\\n- High-fiber, light breakfast, under 20 minutes.\\n- What are some quick and light high-fiber breakfast options ready in less than 20 minutes?\\n- I need a light, fiber-rich breakfast that's ready in under 20 minutes.\"],\n",
       "             ('Holiday meal',\n",
       "              'Lactose-free',\n",
       "              'Overnight'): [\"- I need an overnight, lactose-free recipe for a holiday meal.\\n- Give me some holiday meal ideas that are dairy-free and can be prepared overnight.\\n- What are good lactose-free holiday dishes I can make overnight?\\n- Holiday meal, lactose-free, overnight preparation.\\n- I'm looking for an overnight recipe that's perfect for a holiday feast and is also lactose-free.\"],\n",
       "             ('Party food',\n",
       "              'Comfort food',\n",
       "              'About an hour'): [\"- I'm looking for a recipe that is good for a party, is comforting, and takes about an hour to make.\\n- Give me party food ideas that are also comfort food and can be made in roughly an hour.\\n- Comfort food for a party, ready in around an hour.\\n- I need a comforting party dish that takes approximately an hour to prepare.\\n- What can I cook for a party that's comforting and takes about an hour?\"],\n",
       "             ('Quick bite',\n",
       "              'with hummus and carrots',\n",
       "              'Instant'): [\"- I'm looking for a quick bite that's instant and includes hummus and carrots.\\n- Give me an instant quick bite recipe with hummus and carrots.\\n- Quick bite, instant, hummus and carrots.\\n- I need a super fast snack with hummus and carrots.\\n- What can I make instantly for a quick bite using hummus and carrots?\"],\n",
       "             ('Breakfast',\n",
       "              'Gluten-free',\n",
       "              'Quick and easy'): [\"- I'm looking for a healthy Italian dinner recipe.\\n- Healthy Italian dinner ideas, please.\\n- Can you suggest a healthy Italian meal for dinner?\\n- I want to make a healthy Italian dish for dinner tonight.\\n- Give me a recipe for healthy Italian dinner.\"],\n",
       "             ('Lunch',\n",
       "              'Vegan',\n",
       "              '30 minutes'): [\"- I'm looking for a vegan lunch that I can make in 30 minutes.\\n- Give me some quick vegan lunch ideas under 30 minutes.\\n- What's a good vegan lunch recipe I can prepare in half an hour?\\n- I need a 30-minute vegan meal for lunch.\\n- Vegan lunch, 30 min.\"],\n",
       "             ('Dinner',\n",
       "              'High-protein',\n",
       "              'Under 1 hour'): ['- I need a high-protein dinner recipe that I can prepare in less than an hour.\\n- What are some quick, high-protein dinner ideas for tonight?\\n- Give me a high-protein dinner recipe that takes under 60 minutes.\\n- Looking for fast, protein-rich dinner options.\\n- Dinner, high protein, under 1 hour.'],\n",
       "             ('Snack',\n",
       "              'Not spicy',\n",
       "              '10 minutes'): [\"- I need a quick snack that's not spicy and takes about 10 minutes to make.\\n- Give me some ideas for a 10-minute mild snack.\\n- What's a fast, non-spicy snack recipe I can whip up in 10 minutes?\\n- I'm looking for a quick snack, no spice, ready in 10 minutes.\\n- Suggest a 10-minute, non-spicy snack.\"],\n",
       "             ('Dessert',\n",
       "              'Vegetarian',\n",
       "              '45 minutes'): [\"- I'm looking for a vegetarian dessert recipe that takes about 45 minutes to prepare.\\n- Can you suggest a dessert that's vegetarian and can be made in under 45 minutes?\\n- Vegetarian dessert, 45 minutes.\\n- I need a quick vegetarian dessert, maximum 45 minutes.\\n- Show me some vegetarian desserts that take around 45 minutes.\"],\n",
       "             ('Brunch',\n",
       "              'Meaty',\n",
       "              'I have all the time until retirement'): [\"- I'd love a meaty brunch recipe, and I have all the time in the world to make it.\\n- Suggest a hearty, meaty brunch dish where cooking time is not a concern.\\n- Brunch, meaty, and something that takes a while to prepare.\\n- Give me ideas for a luxurious meaty brunch; I'm not in a hurry.\\n- I want a meaty brunch recipe, no time constraints.\"],\n",
       "             ('Appetizer',\n",
       "              'Dairy-free',\n",
       "              '15 minutes'): [\"- I'm looking for a dairy-free appetizer that takes about 15 minutes to make.\\n- Suggest a quick 15-minute appetizer, no dairy please.\\n- Dairy-free appetizer, ready in 15 minutes.\\n- What are some fast dairy-free appetizers I can prepare in under 15 minutes?\\n- I need a recipe for a 15-minute dairy-free appetizer.\"],\n",
       "             ('Side Dish',\n",
       "              'Low-carb',\n",
       "              '20 minutes'): [\"- I need a low-carb side dish that I can make in 20 minutes.\\n- What are some quick, low-carb side dish recipes ready in under 20 minutes?\\n- Show me a fast side dish for dinner, low in carbs and taking only 20 minutes.\\n- Give me ideas for a 20-minute, keto-friendly side.\\n- I'm looking for a quick, low-carb accompaniment that's done in about 20 minutes.\"],\n",
       "             ('Lite breakfast',\n",
       "              'Pescatarian',\n",
       "              'Less than 20 minutes'): [\"- I'm looking for a lite pescatarian breakfast that takes less than 20 minutes to prepare.\\n- Give me a quick recipe for a light, pescatarian breakfast ready in under 20 minutes.\\n- What are some easy, lite pescatarian breakfast ideas that can be made in under 20 minutes?\\n- Pescatarian, light breakfast, quick (under 20 minutes).\\n- I need a fast and light pescatarian breakfast, less than 20 minutes.\"],\n",
       "             ('Holiday meal',\n",
       "              'Kid-friendly',\n",
       "              'All day'): [\"- I'm looking for kid-friendly holiday meal ideas that can be enjoyed all day.\\n- Suggest some easy, all-day recipes for a kid-friendly holiday feast.\\n- What are some good holiday meals that are suitable for kids and can be prepared or eaten over an extended period?\\n- I need a holiday menu that's kid-friendly and good for an all-day celebration.\\n- Give me some ideas for an all-day, kid-approved holiday spread.\"],\n",
       "             ('Party food',\n",
       "              'Nut-free',\n",
       "              'About an hour'): [\"- I need nut-free party food that takes about an hour to make.\\n- What's a good party food recipe that's nut-free and can be ready in about an hour?\\n- Nut-free party snacks, 1 hour.\\n- Looking for party food recipes without nuts, around an hour prep/cook time.\\n- Can you suggest some nut-free dishes suitable for a party that take roughly 60 minutes?\"],\n",
       "             ('Quick bite',\n",
       "              'Sugar-free',\n",
       "              'Instant'): [\"- I need an instant, sugar-free quick bite.\\n- Give me a recipe for a fast, sugar-free snack.\\n- What are some quick, instant, sugar-free bites I can make?\\n- Suggest an instant sugar-free snack.\\n- I'm looking for a sugar-free quick bite that's instant.\"],\n",
       "             ('Breakfast',\n",
       "              'Spicy',\n",
       "              'Quick and easy'): [\"- I'm looking for a quick and easy spicy breakfast recipe.\\n- Can you give me ideas for a fast and spicy breakfast?\\n- Breakfast, spicy, and quick to make.\\n- I need a simple, hot breakfast.\\n- What's a quick and spicy breakfast dish I can prepare easily?\"],\n",
       "             ('Lunch',\n",
       "              'Keto',\n",
       "              '30 minutes'): [\"- I'm looking for a keto lunch recipe that can be made in 30 minutes.\\n- Give me some quick keto lunch ideas, around 30 minutes prep time.\\n- Lunch, keto, 30 minutes.\\n- What's a good keto lunch I can whip up in half an hour?\\n- Need a 30-minute keto lunch recipe.\"],\n",
       "             ('Dinner',\n",
       "              'Heart-healthy',\n",
       "              'Under 1 hour'): [\"- I'm looking for a heart-healthy dinner that takes under an hour to prepare.\\n- Quick heart-healthy dinner recipes needed.\\n- What's a good heart-healthy dinner I can make in less than 60 minutes?\\n- Give me ideas for a healthy dinner, fast, and good for my heart.\\n- Dinner, heart-healthy, under an hour.\"],\n",
       "             ('Snack',\n",
       "              'Low-salt',\n",
       "              '10 minutes'): [\"- I'm looking for a low-salt snack recipe that takes about 10 minutes to make.\\n- Give me some quick 10-minute snack ideas that are low in salt.\\n- Snack, low-salt, fast.\\n- I need a healthy, low-sodium snack that can be prepared in 10 minutes or less.\\n- What's a good 10-minute, low-salt snack?\"],\n",
       "             ('Dessert',\n",
       "              'Diabetic-friendly',\n",
       "              '45 minutes'): [\"- I'm looking for a diabetic-friendly dessert that takes about 45 minutes to prepare.\\n- Can you suggest a dessert recipe for diabetics that can be made in 45 minutes?\\n- Diabetic dessert, 45 minutes max.\\n- Recipe for a dessert, low in sugar, ready in under 45 minutes.\\n- Need a quick diabetic dessert, around 45 minutes prep time.\"],\n",
       "             ('Brunch',\n",
       "              'High-fiber',\n",
       "              'Long prep'): [\"- I need a quick and budget-friendly dinner recipe.\\n- Give me ideas for a fast and cheap dinner.\\n- What's a good dinner that's quick to make and doesn't cost much?\\n- I'm looking for a speedy, affordable dinner.\\n- Dinner, quick and budget-friendly.\"],\n",
       "             ('Appetizer',\n",
       "              'Lactose-free',\n",
       "              '15 minutes'): [\"- I'm looking for a quick appetizer that is lactose-free and can be made in 15 minutes.\\n- Suggest a 15-minute, dairy-free appetizer recipe.\\n- Do you have any ideas for a fast appetizer that's lactose-free and takes under 15 minutes?\\n- Lactose-free appetizer, 15 minutes.\\n- I need a dairy-free appetizer recipe that's quick, around 15 minutes to prepare.\"],\n",
       "             ('Side Dish',\n",
       "              'Comfort food',\n",
       "              '20 minutes'): [\"- I'm looking for a comforting side dish that I can make in 20 minutes.\\n- Give me a quick recipe for a comfort food side dish, something that takes around 20 minutes.\\n- Comfort food side, 20 minutes prep.\\n- I need a fast, comforting side dish recipe, under 20 minutes.\\n- What's a good 20-minute comfort food side dish?\"],\n",
       "             ('Lite breakfast',\n",
       "              'Paleo',\n",
       "              'Less than 20 minutes'): ['- I need a light paleo breakfast recipe that takes less than 20 minutes to prepare.\\n- What are some quick paleo breakfast ideas that are light and can be made in under 20 minutes?\\n- Give me a recipe for a lite paleo breakfast, ready in less than 20 minutes.\\n- Paleo, light breakfast, under 20 minutes.\\n- Fast and light paleo breakfast, quick, under 20 mins.'],\n",
       "             ('Holiday meal',\n",
       "              'with hummus and carrots',\n",
       "              'Overnight'): [\"- I'm looking for an overnight holiday meal recipe that includes hummus and carrots.\\n- Give me an overnight holiday dish with hummus and carrots.\\n- Holiday meal, overnight preparation, featuring hummus and carrots.\\n- I need a holiday meal that can be prepared overnight and has hummus and carrots.\\n- Overnight holiday recipe with hummus and carrots, please.\"],\n",
       "             ('Party food',\n",
       "              'Not spicy',\n",
       "              'About an hour'): [\"- I need a party food recipe that is not spicy and takes about an hour to make.\\n- What are some mild party dishes I can prepare in around 60 minutes?\\n- Give me a non-spicy party food idea that can be ready in about an hour.\\n- Show me recipes for party food, mild taste, takes roughly an hour.\\n- I'm looking for a party dish, non-spicy, that takes an hour to cook.\"],\n",
       "             ('Quick bite',\n",
       "              'Vegetarian',\n",
       "              'Instant'): [\"- I'm looking for a quick, instant, vegetarian bite.\\n- Give me some ideas for an instant vegetarian quick bite.\\n- What are some recipes for quick vegetarian snacks that can be made instantly?\\n- I need a very fast vegetarian bite.\\n- Quick, instant, vegetarian meal ideas, please.\"],\n",
       "             ('Dinner',\n",
       "              'Gluten-free',\n",
       "              '10 minutes'): [\"- I need a gluten-free dinner recipe that can be made in 10 minutes.\\n- Give me ideas for a quick 10-minute gluten-free dinner.\\n- What's a fast, gluten-free dinner I can cook in 10 minutes?\\n- Dinner, gluten-free, 10 minutes.\\n- I'm looking for a speedy gluten-free dinner, ready in under 10 minutes.\"],\n",
       "             ('Lunch',\n",
       "              'Meaty',\n",
       "              'something quick'): [\"- I'm looking for a quick and meaty lunch.\\n- Can you suggest a fast, meat-based recipe for lunch?\\n- Quick meaty lunch ideas.\\n- I need a speedy lunch meal with meat.\\n- What's a good, quick, meaty lunch?\"]})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "tuple_to_queries = defaultdict(list)\n",
    "for dimention in dimention_tuples:\n",
    "    query = call_llm(dimention)\n",
    "    tuple_to_queries[dimention].append(query)\n",
    "\n",
    "tuple_to_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tuple_to_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tuple_to_queries' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m dimention, queries \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtuple_to_queries\u001b[49m.items():\n\u001b[32m      2\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m====\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m + \u001b[39m\u001b[33m\"\u001b[39m.join(dimention)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m====\u001b[39m\u001b[33m\"\"\"\u001b[39m)\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m queries:\n",
      "\u001b[31mNameError\u001b[39m: name 'tuple_to_queries' is not defined"
     ]
    }
   ],
   "source": [
    "for dimention, queries in tuple_to_queries.items():\n",
    "    print(f\"\"\"\\n===={\" + \".join(dimention)}====\"\"\")\n",
    "    for query in queries:\n",
    "        print(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/home/Projects/recipe-chatbot/homeworks/hw2\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "current_dir = Path.cwd()\n",
    "print(current_dir)\n",
    "\n",
    "synthetic_queries = []\n",
    "synthetic_queries_txt = (current_dir / \"synthetic_queries.txt\").read_text()\n",
    "for idx, query in enumerate(synthetic_queries_txt.split(\"\\n\")):\n",
    "    if (line := query.strip()).startswith(\"-\"):\n",
    "        line = line.lstrip(\"-\").removeprefix('\"').removeprefix(\"'\").removesuffix('\"').strip()\n",
    "        synthetic_queries.append({\"id\": idx, \"query\": line})\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(synthetic_queries)\n",
    "df.to_csv(\"synthetic_queries.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
